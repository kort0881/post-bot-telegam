import os
import json
import asyncio
from datetime import datetime

import requests
from aiogram import Bot
from aiogram.client.bot import DefaultBotProperties
from aiogram.enums import ParseMode

from openai import OpenAI

# ===== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ =====
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

print("DEBUG OPENAI KEY LEN:", len(OPENAI_API_KEY) if OPENAI_API_KEY else 0)

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)

openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

# ===== –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ =====
STRONG_KEYWORDS = [
    # VPN / –æ–±—Ö–æ–¥ / —Ü–µ–Ω–∑—É—Ä–∞
    "vpn", "–≤–ø–Ω", "–ø—Ä–æ–∫—Å–∏", "proxy", "tor", "shadowsocks",
    "wireguard", "openvpn", "ikev2",
    "–æ–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫", "–æ–±—Ö–æ–¥ —Ü–µ–Ω–∑—É—Ä—ã", "–∞–Ω–æ–Ω–∏–º–Ω–æ—Å—Ç—å",
    "—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "—Ä–∫–Ω",
    "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ü–µ–Ω–∑—É—Ä–∞", "—Ü–µ–Ω–∑—É—Ä–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ",
    "–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–∞–π—Ç–æ–≤", "–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ä–µ—Å—É—Ä—Å–∞",
    "—Ä–µ–µ—Å—Ç—Ä –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–∞",
    "—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç—Ä–∞—Ñ–∏–∫–∞", "dpi", "deep packet inspection",

    # –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä—ã / —Å–æ—Ü—Å–µ—Ç–∏
    "—Ç–µ–ª–µ–≥—Ä–∞–º", "telegram",
    "whatsapp", "signal", "viber",
    "messenger", "–º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä",

    # —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Å–æ—Ñ—Ç
    "–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "–ø–∞—Ç—á –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
    "–∞–Ω—Ç–∏–≤–∏—Ä—É—Å", "firewall", "—Ñ–∞–µ—Ä–≤–æ–ª",
    "–±—Ä–∞—É–∑–µ—Ä", "–±—Ä–∞—É–∑–µ—Ä tor",
    "–∫–ª–∏–µ–Ω—Ç vpn", "vpn-–∫–ª–∏–µ–Ω—Ç",
]

SOFT_KEYWORDS = [
    "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø–∫", "desktop-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—É—Ç–∏–ª–∏—Ç–∞ –¥–ª—è windows",
    "–ø—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è macos", "open source",
    "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
    "ai", "machine learning",
    "–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
    "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ", "privacy",
    "—Å—É–≤–µ—Ä–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞",
]

# –ò—Å–∫–ª—é—á–∞–µ–º –∏–≥—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
EXCLUDE_KEYWORDS = [
    "–∏–≥—Ä–∞", "–∏–≥—Ä—ã", "game", "games", "–≥–µ–π–º–ø–ª–µ–π", "gameplay",
    "dungeon", "quest", "–±–æ—Å—Å", "boss", "—Ä–µ–π–¥", "raid",
    "–æ–Ω–ª–∞–π–Ω-–∏–≥—Ä–∞", "–∏–≥—Ä–æ–≤–æ–π", "–≥–µ–π–º–∏–Ω–≥", "gaming",
    "playstation", "xbox", "nintendo", "steam",
    "—à—É—Ç–µ—Ä", "rpg", "mmorpg", "moba", "battle royale",
]

POSTED_FILE = "posted_articles.json"

# ===== –†–∞–±–æ—Ç–∞ —Å —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ—Å—Ç–∞–º–∏ =====
if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        posted_articles = set(json.load(f))
else:
    posted_articles = set()


def save_posted(article_id: str) -> None:
    posted_articles.add(article_id)
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(list(posted_articles), f, ensure_ascii=False, indent=2)


# ===== –£—Ç–∏–ª–∏—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ =====
def safe_get(url: str) -> str | None:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            print(f"HTTP {resp.status_code} –¥–ª—è {url}")
            return None
        return resp.text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}:", e)
        return None


def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())


# ===== –ü–∞—Ä—Å–∏–Ω–≥ 3DNews =====
def load_3dnews():
    url = "https://3dnews.ru/"
    html = safe_get(url)
    if not html:
        return []

    articles = []
    parts = html.split('<a href="/')
    for part in parts[1:6]:
        href_end = part.find('"')
        title_start = part.find(">")
        title_end = part.find("</a>")
        if href_end == -1 or title_start == -1 or title_end == -1:
            continue

        href = part[:href_end]
        title = clean_text(part[title_start + 1:title_end])
        if not title:
            continue

        link = "https://3dnews.ru/" + href.lstrip("/")
        summary = ""

        articles.append(
            {
                "id": link,
                "title": title,
                "summary": summary,
                "link": link,
                "published_parsed": datetime.now(),
            }
        )

    print("DEBUG: —Å—Ç–∞—Ç–µ–π –∏–∑ 3DNews:", len(articles))
    return articles


# ===== –ü–∞—Ä—Å–∏–Ω–≥ –•–∞–±—Ä–∞ =====
def load_habr():
    url = "https://habr.com/ru/feed/"
    html = safe_get(url)
    if not html:
        return []

    articles = []
    chunks = html.split("<article")
    for chunk in chunks[1:6]:
        title_marker = 'data-test-id="article-title-link"'
        idx = chunk.find(title_marker)
        if idx == -1:
            continue
        sub = chunk[idx:]
        href_pos = sub.find('href="')
        if href_pos == -1:
            continue
        href_start = href_pos + len('href="')
        href_end = sub.find('"', href_start)
        href = sub[href_start:href_end]

        title_start = sub.find(">", href_end) + 1
        title_end = sub.find("</a>", title_start)
        title = clean_text(sub[title_start:title_end])

        link = "https://habr.com" + href

        p_start = chunk.find("<p")
        if p_start != -1:
            p_start = chunk.find(">", p_start) + 1
            p_end = chunk.find("</p>", p_start)
            summary = clean_text(chunk[p_start:p_end])
        else:
            summary = ""

        articles.append(
            {
                "id": link,
                "title": title,
                "summary": summary,
                "link": link,
                "published_parsed": datetime.now(),
            }
        )

    print("DEBUG: —Å—Ç–∞—Ç–µ–π –∏–∑ –•–∞–±—Ä–∞:", len(articles))
    return articles


# ===== –ü–∞—Ä—Å–∏–Ω–≥ Tproger =====
def load_tproger():
    url = "https://tproger.ru/news"
    html = safe_get(url)
    if not html:
        return []

    articles = []
    parts = html.split('<a ')
    for part in parts[1:6]:
        if "href=" not in part or "news" not in part:
            continue

        href_pos = part.find('href="')
        href_start = href_pos + len('href="')
        href_end = part.find('"', href_start)
        href = part[href_start:href_end]

        title_start = part.find(">", href_end) + 1
        title_end = part.find("</a>", title_start)
        title = clean_text(part[title_start:title_end])
        if not title:
            continue

        if href.startswith("http"):
            link = href
        else:
            link = "https://tproger.ru" + href

        summary = ""

        articles.append(
            {
                "id": link,
                "title": title,
                "summary": summary,
                "link": link,
                "published_parsed": datetime.now(),
            }
        )

    print("DEBUG: —Å—Ç–∞—Ç–µ–π –∏–∑ Tproger:", len(articles))
    return articles


# ===== –°–±–æ—Ä –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π =====
def load_articles_from_sites():
    articles = []
    articles.extend(load_3dnews())
    articles.extend(load_habr())
    articles.extend(load_tproger())
    print("DEBUG: –≤—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π –∏–∑ —Å–∞–π—Ç–æ–≤:", len(articles))
    return articles


# ===== –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≤—ã–±–æ—Ä —Å—Ç–∞—Ç—å–∏ (–ë–ï–ó –ò–ì–†) =====
def filter_article(entry):
    title = entry.get("title", "")
    summary = entry.get("summary", "")
    text = (title + " " + summary).lower()

    # –ò–°–ö–õ–Æ–ß–ê–ï–ú –ò–ì–†–û–í–´–ï –ù–û–í–û–°–¢–ò
    if any(kw.lower() in text for kw in EXCLUDE_KEYWORDS):
        return None

    if any(kw.lower() in text for kw in STRONG_KEYWORDS):
        return "strong"
    if any(kw.lower() in text for kw in SOFT_KEYWORDS):
        return "soft"
    return None


def pick_article(articles):
    scored = []
    for e in articles:
        level = filter_article(e)
        if not level:
            continue
        score = 2 if level == "strong" else 1
        scored.append((score, e))

    if scored:
        scored.sort(
            key=lambda x: (
                x[0],
                x[1].get("published_parsed", datetime.now()),
            ),
            reverse=True,
        )
        return scored[0][1]

    return None


# ===== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (OpenAI) =====
def short_summary(title: str, summary: str) -> str:
    prompt = (
        "–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç—É –ö–û–ù–ö–†–ï–¢–ù–£–Æ –Ω–æ–≤–æ—Å—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç Telegram-–ø–æ—Å—Ç–∞.\n\n"
        f"–ó–ê–ì–û–õ–û–í–û–ö –ù–û–í–û–°–¢–ò: {title}\n"
        f"–û–ü–ò–°–ê–ù–ò–ï –ù–û–í–û–°–¢–ò: {summary or '–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫'}\n\n"
        "–°–¢–†–û–ì–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:\n"
        "1) –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤—ã—à–µ!\n"
        "2) –ù–ï –ø–∏—à–∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ –∏ –æ–±–æ–±—â—ë–Ω–Ω–æ!\n"
        "3) –£–ø–æ–º–∏–Ω–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–∫–æ–º–ø–∞–Ω–∏–∏, –ø—Ä–æ–¥—É–∫—Ç—ã, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –≤–µ—Ä—Å–∏–∏)!\n"
        "4) –ï—Å–ª–∏ –≤ –Ω–æ–≤–æ—Å—Ç–∏ –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã, –∏–º–µ–Ω–∞ ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö!\n"
        "5) –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –°–¢–†–û–ì–û 650-700 —Å–∏–º–≤–æ–ª–æ–≤ (–±–µ–∑ PS-—Å—Ç—Ä–æ–∫–∏). –ü–∏—à–∏ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ!\n\n"
        "–§–û–†–ú–ê–¢ –ü–û–°–¢–ê:\n"
        "- 4 —Å—Ç—Ä–æ–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n"
        "- –í –Ω–∞—á–∞–ª–µ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –æ–¥–Ω–∞ —ç–º–æ–¥–∑–∏ –∏ –ø—Ä–æ–±–µ–ª\n"
        "- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç—Ä–æ–∫:\n"
        "  –°—Ç—Ä–æ–∫–∞ 1: –ß–¢–û –∏–º–µ–Ω–Ω–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è/–ø—Ä–æ–¥—É–∫—Ç/—Å–æ–±—ã—Ç–∏–µ)\n"
        "  –°—Ç—Ä–æ–∫–∞ 2: –ö–∞–∫–∞—è –±—ã–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞ –∏–ª–∏ —á—Ç–æ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏)\n"
        "  –°—Ç—Ä–æ–∫–∞ 3: –ß—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ —É–ª—É—á—à–∏–ª–æ—Å—å/–∏–∑–º–µ–Ω–∏–ª–æ—Å—å (—Ñ–∞–∫—Ç—ã, —Ü–∏—Ñ—Ä—ã, —Ñ—É–Ω–∫—Ü–∏–∏)\n"
        "  –°—Ç—Ä–æ–∫–∞ 4: –ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–ª—å–∑–∞)\n"
        "- –ü–æ—Å–ª–µ 4-–π —Å—Ç—Ä–æ–∫–∏: –æ–¥–Ω–∞ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞\n"
        "- –ó–∞—Ç–µ–º —Å—Ç—Ä–æ–∫–∞: PSüí• –ö—Ç–æ –∑–∞ –∫–ª—é—á–∞–º–∏ üëâ https://t.me/+EdEfIkn83Wg3ZTE6\n\n"
        "–ù–ò–ö–ê–ö–ò–• —Ö–µ—à—Ç–µ–≥–æ–≤, –Ω–∏–∫–∞–∫–∏—Ö –¥—Ä—É–≥–∏—Ö —Å—Å—ã–ª–æ–∫, –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫!\n"
        "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞."
    )
    result = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
    )
    content = result.choices[0].message.content.strip()
    return content[:850].rstrip()


# ===== –ê–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥ =====
async def autopost():
    articles = load_articles_from_sites()

    if not articles:
        print("–í–æ–æ–±—â–µ –Ω–µ—Ç —Å—Ç–∞—Ç–µ–π, –ø–æ—Å—Ç –ø—Ä–æ–ø—É—â–µ–Ω")
        return

    art = pick_article(articles)

    if not art:
        print("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–∞—Ç–∏–∫–µ (VPN/–∏–Ω—Ç–µ—Ä–Ω–µ—Ç/—Å–æ—Ñ—Ç), –ø–æ—Å—Ç –ø—Ä–æ–ø—É—â–µ–Ω")
        return

    article_id = art.get("id", art.get("link", art.get("title")))
    if article_id in posted_articles:
        print("–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —É–∂–µ –±—ã–ª–∞ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞")
        return

    title = art.get("title", "")
    summary = art.get("summary", "")[:400]

    # –û–¢–õ–ê–î–û–ß–ù–´–ô –í–´–í–û–î
    print(f"\n{'='*60}")
    print(f"–í–´–ë–†–ê–ù–ê –ù–û–í–û–°–¢–¨:")
    print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
    print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {summary}")
    print(f"–°—Å—ã–ª–∫–∞: {art.get('link')}")
    print(f"{'='*60}\n")

    news = short_summary(title, summary)

    # –í–´–í–û–î –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–ù–û–ì–û –¢–ï–ö–°–¢–ê
    print(f"–°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–ù–´–ô –¢–ï–ö–°–¢ ({len(news)} —Å–∏–º–≤–æ–ª–æ–≤):")
    print(news)
    print(f"{'='*60}\n")

    all_keywords = STRONG_KEYWORDS + SOFT_KEYWORDS
    text_for_tags = (title + " " + summary).lower()
    hashtags = [
        f"#{kw.replace(' ', '')}"
        for kw in all_keywords
        if kw.lower() in text_for_tags
    ]
    hashtags += ["#–ù–æ–≤–æ—Å—Ç–∏", "#Telegram", "#–ö–∞–Ω–∞–ª"]

    caption = f"{news}\n\n{' '.join(hashtags)}"

    await bot.send_message(CHANNEL_ID, caption)

    save_posted(article_id)
    print("[OK]", datetime.now(), "-", title)


# ===== –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è =====
async def main():
    await autopost()


if __name__ == "__main__":
    asyncio.run(main())



















