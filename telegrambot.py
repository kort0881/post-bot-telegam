import os
import json
import asyncio
import random
import re
import hashlib
import logging
import difflib
import tempfile
import shutil
from datetime import datetime, timezone
from typing import List, Set, Optional
from urllib.parse import urlparse, quote
from dataclasses import dataclass, field

import aiohttp
import feedparser
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq

# –î–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–æ–≤ (Windows —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
try:
    import fcntl
    HAS_FCNTL = True
except ImportError:
    HAS_FCNTL = False

# ====================== –õ–û–ì–ò ======================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("ai_poster.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ====================== CONFIG ======================
class Config:
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.telegram_token = os.getenv("TELEGRAM_BOT_TOKEN")
        self.channel_id = os.getenv("CHANNEL_ID")
        self.retention_days = int(os.getenv("RETENTION_DAYS", "30"))
        self.caption_limit = 1024
        self.posted_file = "posted_articles.json"
        
        # –ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (0.65 = 65% —Å—Ö–æ–¥—Å—Ç–≤–∞)
        self.similarity_threshold = 0.65
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–∞ (–±–µ–∑ —É—á—ë—Ç–∞ —Ö–µ—à—Ç–µ–≥–æ–≤ –∏ —Å—Å—ã–ª–∫–∏)
        self.min_post_length = 500

        missing = []
        for var, name in [(self.groq_api_key, "GROQ_API_KEY"),
                          (self.telegram_token, "TELEGRAM_BOT_TOKEN"),
                          (self.channel_id, "CHANNEL_ID")]:
            if not var:
                missing.append(name)
        if missing:
            raise SystemExit(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {', '.join(missing)}")

config = Config()

bot = Bot(token=config.telegram_token, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=config.groq_api_key)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0 Safari/537.36"
}

# ====================== RSS ======================
RSS_FEEDS = [
    ("https://techcrunch.com/category/artificial-intelligence/feed/", "TechCrunch AI"),
    ("https://venturebeat.com/category/ai/feed/", "VentureBeat AI"),
    ("https://www.technologyreview.com/topic/artificial-intelligence/feed", "MIT Tech Review"),
    ("https://www.theverge.com/rss/index.xml", "The Verge"),
    ("https://arstechnica.com/tag/artificial-intelligence/feed/", "Ars Technica AI"),
    ("https://www.wired.com/feed/tag/ai/latest/rss", "WIRED AI"),
]

# ====================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ======================
AI_KEYWORDS = [
    "ai ", " ai", "artificial intelligence", "machine learning", "deep learning", "neural network",
    "llm", "large language model", "gpt", "chatgpt", "claude", "gemini", "grok", "llama",
    "mistral", "qwen", "deepseek", "midjourney", "dall-e", "stable diffusion", "sora", 
    "groq", "openai", "anthropic", "deepmind", "hugging face", "nvidia", "agi", 
    "inference", "rlhf", "transformer", "generative", "chatbot"
]

EXCLUDE_KEYWORDS = [
    "stock price", "ipo", "earnings call", "quarterly results", "revenue beat", "profit margin", 
    "dividend", "market cap", "wall street",
    "ps5", "xbox", "nintendo switch", "game review", "gameplay", "gaming pc",
    "netflix series", "movie review", "box office", "trailer", "premiere",
    "tesla stock", "ev sales", "model 3", "model y", "cybertruck",
    "bitcoin", "crypto", "blockchain", "nft", "ethereum",
    "election", "trump", "biden", "congress", "senate", "white house"
]

BAD_PHRASES = ["sponsored", "partner content", "advertisement", "black friday", "deal alert", "coupon"]

# ====================== DATACLASSES ======================
@dataclass
class Article:
    title: str
    summary: str
    link: str
    source: str
    published: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

# ====================== TOPIC & HASHTAGS ======================
class Topic:
    LLM = "llm"
    IMAGE_GEN = "image_gen"
    ROBOTICS = "robotics"
    HARDWARE = "hardware"
    GENERAL = "general"
    
    HASHTAGS = {
        LLM: "#ChatGPT #LLM #OpenAI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        IMAGE_GEN: "#Midjourney #DALLE #StableDiffusion #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
        ROBOTICS: "#—Ä–æ–±–æ—Ç—ã #Humanoid #—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
        HARDWARE: "#NVIDIA #GPU #—á–∏–ø—ã #–∂–µ–ª–µ–∑–æ",
        GENERAL: "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
    }

    @staticmethod
    def detect(text: str) -> str:
        t = text.lower()
        if any(x in t for x in ["gpt", "chatgpt", "claude", "gemini", "llama", "grok", "llm", "language model"]):
            return Topic.LLM
        if any(x in t for x in ["midjourney", "dall-e", "dalle", "stable diffusion", "flux", "image gen", "sora"]):
            return Topic.IMAGE_GEN
        if any(x in t for x in ["robot", "humanoid", "boston dynamics", "optimus", "figure ai"]):
            return Topic.ROBOTICS
        if any(x in t for x in ["nvidia", "h100", "h200", "blackwell", "gpu", "tensor core", "cuda"]):
            return Topic.HARDWARE
        return Topic.GENERAL

# ====================== HELPERS ======================
def normalize_url(url: str) -> str:
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL"""
    if not url:
        return ""
    try:
        url = url.strip()
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace("www.", "")
        path = parsed.path.rstrip("/")
        return f"{parsed.scheme}://{domain}{path}"
    except:
        return url.split("?")[0].split("#")[0]

def calculate_similarity(text1: str, text2: str) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–≤—É—Ö —Å—Ç—Ä–æ–∫"""
    return difflib.SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def extract_key_entities(text: str) -> Set[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–º"""
    text_lower = text.lower()
    entities = set()
    
    # –ö–æ–º–ø–∞–Ω–∏–∏ –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
    key_terms = [
        "openai", "google", "meta", "microsoft", "anthropic", "nvidia", "apple",
        "amazon", "deepmind", "hugging face", "stability ai", "midjourney",
        "gpt-4", "gpt-5", "gpt", "chatgpt", "claude", "gemini", "llama", "mistral",
        "copilot", "dall-e", "sora", "stable diffusion", "flux",
        "linux foundation", "agentic", "agent", "agi",
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
        "regulation", "safety", "alignment", "open source", "api"
    ]
    
    for term in key_terms:
        if term in text_lower:
            entities.add(term)
    
    return entities

def clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def ai_relevance(text: str) -> float:
    lower = text.lower()
    matches = sum(1 for kw in AI_KEYWORDS if kw in lower)
    return min(matches / 3.0, 1.0)

def get_content_hash(text: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π —Ö–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    if not text:
        return ""
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    normalized = re.sub(r'\s+', ' ', text.strip().lower())
    return hashlib.md5(normalized.encode()).hexdigest()[:16]

# ====================== POSTED MANAGER ======================
class PostedManager:
    def __init__(self, file="posted_articles.json"):
        self.file = file
        self.lock_file = file + ".lock"
        self.data = []
        self.urls: Set[str] = set()
        self.titles: List[str] = []
        self.content_hashes: Set[str] = set()
        self.topic_entities: List[Set[str]] = []  # –°—É—â–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏
        self._lock_fd = None
        
        self._acquire_lock()
        self._load()

    def _acquire_lock(self):
        """–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
        if not HAS_FCNTL:
            if os.path.exists(self.lock_file):
                try:
                    age = datetime.now().timestamp() - os.path.getmtime(self.lock_file)
                    if age < 600:
                        raise SystemExit("‚ö†Ô∏è –î—Ä—É–≥–æ–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∫—Ä–∏–ø—Ç–∞ —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
                except OSError:
                    pass
            with open(self.lock_file, 'w') as f:
                f.write(str(os.getpid()))
            return
        
        self._lock_fd = open(self.lock_file, 'w')
        try:
            fcntl.flock(self._lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
            self._lock_fd.write(str(os.getpid()))
            self._lock_fd.flush()
        except BlockingIOError:
            raise SystemExit("‚ö†Ô∏è –î—Ä—É–≥–æ–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∫—Ä–∏–ø—Ç–∞ —É–∂–µ –∑–∞–ø—É—â–µ–Ω")

    def _release_lock(self):
        """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏"""
        try:
            if HAS_FCNTL and self._lock_fd:
                fcntl.flock(self._lock_fd, fcntl.LOCK_UN)
                self._lock_fd.close()
            if os.path.exists(self.lock_file):
                os.remove(self.lock_file)
        except Exception:
            pass

    def _load(self):
        if not os.path.exists(self.file):
            self._save()
            return
        try:
            with open(self.file, "r", encoding="utf-8") as f:
                self.data = json.load(f)
            
            for item in self.data:
                url = item.get("url", "")
                if url:
                    self.urls.add(normalize_url(url))
                
                title = item.get("title", "")
                if title:
                    self.titles.append(title)
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    self.topic_entities.append(extract_key_entities(title))
                
                content_hash = item.get("content_hash", "")
                if content_hash:
                    self.content_hashes.add(content_hash)
            
            logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ posted_articles.json: {e}")
            self.data = []

    def _save(self):
        """–ê—Ç–æ–º–∞—Ä–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            dir_name = os.path.dirname(self.file) or '.'
            fd, tmp_path = tempfile.mkstemp(suffix='.json', dir=dir_name)
            
            with os.fdopen(fd, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
            
            shutil.move(tmp_path, self.file)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            try:
                if 'tmp_path' in locals() and os.path.exists(tmp_path):
                    os.remove(tmp_path)
            except:
                pass

    def is_duplicate(self, url: str, title: str, summary: str = "") -> bool:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç"""
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ URL
        norm_url = normalize_url(url)
        if norm_url in self.urls:
            logger.info(f"üö´ –î—É–±–ª–∏–∫–∞—Ç –ø–æ URL: {title[:50]}...")
            return True

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ö–µ—à—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if summary:
            content_hash = get_content_hash(summary)
            if content_hash and content_hash in self.content_hashes:
                logger.info(f"üö´ –î—É–±–ª–∏–∫–∞—Ç –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É: {title[:50]}...")
                return True

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_len = len(title)
        for existing_title in self.titles:
            if abs(len(existing_title) - title_len) > title_len * 0.5:
                continue
            
            similarity = calculate_similarity(title, existing_title)
            if similarity > config.similarity_threshold:
                logger.info(f"üö´ –î—É–±–ª–∏–∫–∞—Ç –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É ({int(similarity*100)}%): '{title[:40]}' ‚âà '{existing_title[:40]}'")
                return True

        # 4. –ù–û–í–û–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (—Ç–µ–º–∞ —Å—Ç–∞—Ç—å–∏)
        new_entities = extract_key_entities(title + " " + summary)
        if len(new_entities) >= 2:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π
            for i, existing_entities in enumerate(self.topic_entities):
                if len(existing_entities) >= 2:
                    # –°—á–∏—Ç–∞–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
                    common = new_entities & existing_entities
                    # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç 70%+ —Å—É—â–Ω–æ—Å—Ç–µ–π ‚Äî —ç—Ç–æ —Ç–∞ –∂–µ —Ç–µ–º–∞
                    if len(common) >= 2 and len(common) / len(new_entities) >= 0.7:
                        logger.info(f"üö´ –î—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–º–µ: –æ–±—â–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ {common}, —Å—Ç–∞—Ç—å—è: '{self.titles[i][:40]}'")
                        return True
        
        return False

    def add(self, url: str, title: str, summary: str = ""):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—å—é –≤ –∏—Å—Ç–æ—Ä–∏—é –ø—É–±–ª–∏–∫–∞—Ü–∏–π"""
        norm_url = normalize_url(url)
        
        if norm_url in self.urls:
            return
        
        content_hash = get_content_hash(summary) if summary else ""
        entities = extract_key_entities(title + " " + summary)
        
        self.urls.add(norm_url)
        self.titles.append(title)
        self.topic_entities.append(entities)
        if content_hash:
            self.content_hashes.add(content_hash)
        
        self.data.append({
            "url": url,
            "norm_url": norm_url,
            "title": title[:200],
            "content_hash": content_hash,
            "entities": list(entities),  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            "ts": datetime.now(timezone.utc).isoformat() + "Z"
        })
        
        self._save()
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {title[:50]}... | –°—É—â–Ω–æ—Å—Ç–∏: {entities}")

    def cleanup(self, days=30):
        """–£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π"""
        cutoff = datetime.now(timezone.utc).timestamp() - days * 86400
        old_count = len(self.data)
        
        self.data = [
            item for item in self.data
            if self._parse_ts(item.get("ts")) > cutoff
        ]
        
        removed = old_count - len(self.data)
        if removed > 0:
            self.urls.clear()
            self.titles.clear()
            self.content_hashes.clear()
            self.topic_entities.clear()
            
            for item in self.data:
                url = item.get("url", "")
                title = item.get("title", "")
                content_hash = item.get("content_hash", "")
                
                if url:
                    self.urls.add(normalize_url(url))
                if title:
                    self.titles.append(title)
                    self.topic_entities.append(extract_key_entities(title))
                if content_hash:
                    self.content_hashes.add(content_hash)
            
            self._save()
            logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–æ {removed} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π")

    def _parse_ts(self, ts_str: Optional[str]) -> float:
        if not ts_str:
            return 0
        try:
            dt = datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
            return dt.timestamp()
        except:
            return 0

    def __del__(self):
        self._release_lock()

# ====================== RSS LOADER ======================
async def fetch_feed(session: aiohttp.ClientSession, url: str, source: str, posted: PostedManager) -> List[Article]:
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as resp:
            if resp.status != 200:
                logger.warning(f"{source}: HTTP {resp.status}")
                return []
            text = await resp.text()
    except Exception as e:
        logger.warning(f"{source} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return []

    try:
        feed = feedparser.parse(text)
    except Exception as e:
        logger.error(f"{source}: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS - {e}")
        return []

    articles = []
    for entry in feed.entries[:25]:
        link = entry.get("link", "").strip()
        title = clean_text(entry.get("title") or "")
        summary = clean_text(entry.get("summary") or entry.get("description") or "")[:1500]

        if not link or not title:
            continue
        
        if len(title) < 15:
            continue
            
        if posted.is_duplicate(link, title, summary):
            continue

        published = datetime.now(timezone.utc)
        for date_field in ["published", "updated", "created"]:
            date_str = entry.get(date_field)
            if date_str:
                try:
                    parsed = feedparser._parse_date(date_str)
                    if parsed:
                        published = datetime(*parsed[:6], tzinfo=timezone.utc)
                        break
                except:
                    pass

        articles.append(Article(
            title=title,
            summary=summary,
            link=link,
            source=source,
            published=published
        ))

    return articles

async def load_all_feeds(posted: PostedManager) -> List[Article]:
    logger.info("üîÑ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    
    connector = aiohttp.TCPConnector(limit_per_host=5, limit=30)
    async with aiohttp.ClientSession(headers=HEADERS, connector=connector) as session:
        tasks = [fetch_feed(session, url, name, posted) for url, name in RSS_FEEDS]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    all_articles = []
    for res, (url, name) in zip(results, RSS_FEEDS):
        if isinstance(res, list) and res:
            all_articles.extend(res)
            logger.info(f"‚úÖ {name}: {len(res)} –Ω–æ–≤—ã—Ö")
        elif isinstance(res, Exception):
            logger.error(f"‚ùå {name}: {res}")

    logger.info(f"üìä –í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_articles)}")
    return all_articles

# ====================== FILTER ======================
def filter_articles(articles: List[Article]) -> List[Article]:
    candidates = []
    
    for a in articles:
        text = f"{a.title} {a.summary}".lower()

        if any(phrase in text for phrase in BAD_PHRASES):
            continue
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue
        if not any(kw in text for kw in AI_KEYWORDS):
            continue
        if ai_relevance(text) < 0.5:
            continue

        candidates.append(a)

    candidates.sort(key=lambda x: x.published, reverse=True)
    logger.info(f"üéØ –ü—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä—ã: {len(candidates)} —Å—Ç–∞—Ç–µ–π")
    return candidates

# ====================== SUMMARY + TRANSLATE ======================
GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "llama3-70b-8192",
    "mixtral-8x7b-32768",
]

async def generate_summary(article: Article) -> Optional[str]:
    logger.info(f"üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–∞: {article.title[:60]}...")
    
    # –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢
    prompt = f"""–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ç–æ–ø–æ–≤–æ–≥–æ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –ò–ò —Å 50K –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º–∏.

–ò–°–•–û–î–ù–ê–Ø –ù–û–í–û–°–¢–¨:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}
–¢–µ–∫—Å—Ç: {article.summary[:2500]}
–ò—Å—Ç–æ—á–Ω–∏–∫: {article.source}

–¢–í–û–Ø –ó–ê–î–ê–ß–ê ‚Äî –Ω–∞–ø–∏—Å–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–°–¢–†–£–ö–¢–£–†–ê –ü–û–°–¢–ê:

1. üî• –ó–ê–ì–û–õ–û–í–û–ö (1 —Å—Ç—Ä–æ–∫–∞)
   - –¶–µ–ø–ª—è—é—â–∏–π, —Å —ç–º–æ–¥–∑–∏
   - –û—Ç—Ä–∞–∂–∞–µ—Ç —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏

2. –ß–¢–û –°–õ–£–ß–ò–õ–û–°–¨ (4-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
   - –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã: –ö–¢–û, –ß–¢–û —Å–¥–µ–ª–∞–ª, –ö–û–ì–î–ê
   - –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –∏–º–µ–Ω–∞, —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã
   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –µ—Å–ª–∏ –µ—Å—Ç—å

3. –ü–û–ß–ï–ú–£ –≠–¢–û –í–ê–ñ–ù–û (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
   - –ö–æ–Ω—Ç–µ–∫—Å—Ç: –∫–∞–∫ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—é
   - –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π/—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤

4. –í–´–í–û–î (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
   - –û—Å—Ç—Ä—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ò–õ–ò
   - –ü—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å –∫ —á–∏—Ç–∞—Ç–µ–ª—è–º

–ñ–Å–°–¢–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
‚Ä¢ –î–ª–∏–Ω–∞: –ú–ò–ù–ò–ú–£–ú 600 —Å–∏–º–≤–æ–ª–æ–≤, –º–∞–∫—Å–∏–º—É–º 850
‚Ä¢ –ü–∏—à–∏ –ö–û–ù–ö–†–ï–¢–ò–ö–£ ‚Äî –Ω–∏–∫–∞–∫–æ–π –≤–æ–¥—ã
‚Ä¢ –ù–ï –ø–∏—à–∏: "–¥—Ä—É–∑—å—è", "–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º—Å—è", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å", "—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å"
‚Ä¢ –ù–ï –ø–∏—à–∏ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞ "—ç—Ç–æ –≤–∞–∂–Ω–æ –ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –≤–∞–∂–Ω–æ"
‚Ä¢ –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Å—Ç—Ä—ã–º–∏, –∞ –Ω–µ —à–∞–±–ª–æ–Ω–Ω—ã–º–∏ "–ß—Ç–æ –≤—ã –¥—É–º–∞–µ—Ç–µ?"

–ü–†–ò–ú–ï–†–´ –•–û–†–û–®–ò–• –í–û–ü–†–û–°–û–í:
‚úÖ "–°–∫–æ–ª—å–∫–æ –µ—â—ë —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ –ø–æ—Ö–æ—Ä–æ–Ω–∏—Ç OpenAI –æ–¥–Ω–∏–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º?"
‚úÖ "Google –æ–ø—è—Ç—å –¥–æ–≥–æ–Ω—è–µ—Ç ‚Äî –∏–ª–∏ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –æ–±–≥–æ–Ω–∏—Ç?"
‚úÖ "–≠—Ç–æ –Ω–∞—á–∞–ª–æ –∫–æ–Ω—Ü–∞ –¥–ª—è —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤-–¥–∏–∑–∞–π–Ω–µ—Ä–æ–≤?"

–ü–†–ò–ú–ï–†–´ –ü–õ–û–•–ò–• –í–û–ü–†–û–°–û–í:
‚ùå "–ß—Ç–æ –≤—ã –¥—É–º–∞–µ—Ç–µ –æ–± —ç—Ç–æ–º?"
‚ùå "–ö–∞–∫ –≤–∞–º —Ç–∞–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏?"
‚ùå "–ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç –¥–ª—è –±—É–¥—É—â–µ–≥–æ –ò–ò?"

–ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å ‚Äî –º—É—Å–æ—Ä, —Ä–µ–∫–ª–∞–º–∞, –Ω–µ –ø—Ä–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ ‚Äî –æ—Ç–≤–µ—Ç—å –û–î–ù–ò–ú —Å–ª–æ–≤–æ–º: SKIP

–¢–ï–ö–°–¢ –ü–û–°–¢–ê (–º–∏–Ω–∏–º—É–º 600 —Å–∏–º–≤–æ–ª–æ–≤):"""

    for attempt in range(3):
        try:
            await asyncio.sleep(1)
            
            resp = await asyncio.to_thread(
                groq_client.chat.completions.create,
                model=random.choice(GROQ_MODELS),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.75,
                max_tokens=1200,
            )
            text = resp.choices[0].message.content.strip()

            if "SKIP" in text.upper()[:10]:
                logger.info("   ‚ö†Ô∏è LLM –æ—Ç–∫–ª–æ–Ω–∏–ª–∞ —Ç–µ–º—É (SKIP)")
                return None

            # –í–ê–õ–ò–î–ê–¶–ò–Ø –î–õ–ò–ù–´
            if len(text) < config.min_post_length:
                logger.warning(f"   ‚ö†Ô∏è –ü–æ—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤), –ø–æ–≤—Ç–æ—Ä...")
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–æ–¥—É
            water_phrases = ["—ç—Ç–æ –≤–∞–∂–Ω–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ", "—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å", 
                           "–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º—Å—è", "–Ω–µ —Å–µ–∫—Ä–µ—Ç, —á—Ç–æ", "–æ—á–µ–≤–∏–¥–Ω–æ, —á—Ç–æ"]
            has_water = any(phrase in text.lower() for phrase in water_phrases)
            if has_water:
                logger.warning("   ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤–æ–¥–∞ –≤ —Ç–µ–∫—Å—Ç–µ, –ø–æ–≤—Ç–æ—Ä...")
                continue

            topic = Topic.detect(f"{article.title} {article.summary}")
            hashtags = Topic.HASHTAGS.get(topic, Topic.HASHTAGS[Topic.GENERAL])

            cta = "\n\nüî• ‚Äî –æ–≥–æ–Ω—å! | üóø ‚Äî –Ω—É —Ç–∞–∫–æ–µ | ‚ö° ‚Äî –ø—Ä–∏–∫–æ–ª—å–Ω–æ"
            source = f'\n\nüîó <a href="{article.link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
            final = text + cta + "\n\n" + hashtags + source

            # –û–±—Ä–µ–∑–∫–∞ –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç
            if len(final) > config.caption_limit:
                overflow = len(final) - config.caption_limit + 30
                text = text[:-overflow]
                for punct in ['.', '!', '?']:
                    last = text.rfind(punct)
                    if last > len(text) // 2:
                        text = text[:last + 1]
                        break
                final = text + cta + "\n\n" + hashtags + source

            logger.info(f"   ‚úÖ –ü–æ—Å—Ç –≥–æ—Ç–æ–≤: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return final
            
        except Exception as e:
            logger.error(f"   ‚ùå Groq –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/3): {e}")
            await asyncio.sleep(3)

    logger.error("   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ—Å—Ç")
    return None

# ====================== IMAGE ======================
async def generate_image(title: str) -> Optional[str]:
    logger.info("   üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
    
    clean_title = re.sub(r'[^\w\s]', '', title)[:60]
    prompt = f"editorial tech illustration, {clean_title}, isometric 3d, artificial intelligence theme, purple and blue neon lights, dark background, 8k"
    url = f"https://image.pollinations.ai/prompt/{quote(prompt)}?width=1024&height=1024&nologo=true&enhance=true&seed={random.randint(1,999999)}"
    
    for attempt in range(3):
        try:
            timeout = aiohttp.ClientTimeout(total=45)
            async with aiohttp.ClientSession(timeout=timeout) as sess:
                async with sess.get(url) as resp:
                    if resp.status != 200:
                        await asyncio.sleep(2)
                        continue
                    
                    content = await resp.read()
                    if len(content) < 5000:
                        continue
                    
                    fname = f"img_{int(datetime.now().timestamp())}_{random.randint(1000,9999)}.jpg"
                    with open(fname, "wb") as f:
                        f.write(content)
                    
                    logger.info(f"   ‚úÖ –ö–∞—Ä—Ç–∏–Ω–∫–∞: {fname}")
                    return fname
                    
        except Exception:
            await asyncio.sleep(3)
    
    logger.warning("   ‚ö†Ô∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞")
    return None

# ====================== POST ======================
async def post_article(article: Article, text: str, posted: PostedManager) -> bool:
    img = await generate_image(article.title)
    
    try:
        if img and os.path.exists(img):
            await bot.send_photo(config.channel_id, FSInputFile(img), caption=text)
            os.remove(img)
        else:
            await bot.send_message(config.channel_id, text, disable_web_page_preview=False)

        posted.add(article.link, article.title, article.summary)
        logger.info(f"‚úÖ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û: {article.title[:60]}...")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ Telegram: {e}")
        if img and os.path.exists(img):
            try:
                os.remove(img)
            except:
                pass
        return False

# ====================== MAIN ======================
async def autopost():
    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –°–ö–†–ò–ü–¢–ê")
    logger.info("=" * 60)

    posted = PostedManager(config.posted_file)
    posted.cleanup(config.retention_days)

    raw = await load_all_feeds(posted)
    candidates = filter_articles(raw)

    if not candidates:
        logger.info("‚ùå –ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
        return

    for article in candidates[:15]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 15 —Å—Ç–∞—Ç–µ–π
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
        if posted.is_duplicate(article.link, article.title, article.summary):
            logger.info(f"   ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ (–¥—É–±–ª–∏–∫–∞—Ç): {article.title[:50]}")
            continue

        summary = await generate_summary(article)
        if not summary:
            continue

        if await post_article(article, summary, posted):
            logger.info("\nüèÅ –ì–æ—Ç–æ–≤–æ!")
            break 
        
        await asyncio.sleep(5)

async def main():
    try:
        await autopost()
    except Exception as e:
        logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    finally:
        await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())





























































































































































































































































