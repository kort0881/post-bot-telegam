import os
import json
import asyncio
import random
import re
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urlparse, parse_qs

import requests
import feedparser
import urllib.parse
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq

# ============ CONFIG ============

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

if not all([GROQ_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    print("‚ö†Ô∏è WARNING: –ù–µ –≤—Å–µ –∫–ª—é—á–∏ –Ω–∞–π–¥–µ–Ω—ã –≤ ENV!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
groq_client = Groq(api_key=GROQ_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

POSTED_FILE = "posted_articles.json"
RETENTION_DAYS = 30
TELEGRAM_CAPTION_LIMIT = 1024

# ============ –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ============

AI_KEYWORDS = [
    "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å", "–∏–∏", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
    "neural network", "artificial intelligence",
    "llm", "gpt", "gpt-4", "gpt-5", "gpt-4o", "chatgpt", "claude", "gemini",
    "copilot", "mistral", "llama", "qwen", "gigachat", "yandexgpt",
    "kandinsky", "—à–µ–¥–µ–≤—Ä—É–º", "deepseek", "grok",
    "openai", "anthropic", "deepmind", "—Å–±–µ—Ä ai", "—è–Ω–¥–µ–∫—Å ai",
    "hugging face", "stability ai", "meta ai", "google ai",
    "stable diffusion", "midjourney", "dall-e", "sora", "runway",
    "–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
    "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ", "text-to-image", "text-to-video",
    "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "transformer",
    "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π",
    "–¥–æ–æ–±—É—á–µ–Ω–∏–µ", "–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", "–¥–∞—Ç–∞—Å–µ—Ç", "fine-tuning",
    "—á–∞—Ç-–±–æ—Ç", "–≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
    "–Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π", "ai-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "—É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫",
    "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ", "–æ–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–∞", "nlp",
    "agi", "—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "–∞–≥–µ–Ω—Ç", "ai-–∞–≥–µ–Ω—Ç", "–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ",
    "—Ç–æ–∫–µ–Ω", "–±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "reasoning",
    "–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "rlhf", "–ø—Ä–æ–º–ø—Ç", "prompt",
    "–∞–ª–≥–æ—Ä–∏—Ç–º –º–∞—à–∏–Ω–Ω–æ–≥–æ", "–æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"
]

EXCLUDE_KEYWORDS = [
    # –§–∏–Ω–∞–Ω—Å—ã –∏ –±–∏–∑–Ω–µ—Å
    "–∞–∫—Ü–∏–∏", "–∞–∫—Ü–∏—è", "–±–∏—Ä–∂–∞", "–∫–æ—Ç–∏—Ä–æ–≤–∫–∏", "–∏–Ω–¥–µ–∫—Å",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–∏–Ω–≤–µ—Å—Ç–æ—Ä", "–∏–Ω–≤–µ—Å—Ç–æ—Ä—ã", "–¥–∏–≤–∏–¥–µ–Ω–¥—ã",
    "ipo", "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è", "—Ä—ã–Ω–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å",
    "–≤—ã—Ä—É—á–∫–∞", "–ø—Ä–∏–±—ã–ª—å", "—É–±—ã—Ç–æ–∫", "–¥–æ—Ö–æ–¥", "–æ–±–æ—Ä–æ—Ç",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á—ë—Ç", "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç", "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç",
    "–º–∏–ª–ª–∏–∞—Ä–¥ –¥–æ–ª–ª–∞—Ä–æ–≤", "–º–∏–ª–ª–∏–æ–Ω –¥–æ–ª–ª–∞—Ä–æ–≤", "–º–ª—Ä–¥", "–º–ª–Ω —Ä—É–±–ª–µ–π",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "–∫—É—Ä—Å –µ–≤—Ä–æ", "–∫—É—Ä—Å —Ä—É–±–ª—è", "–≤–∞–ª—é—Ç–∞",
    "—Ü–±", "—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "—Å—Ç–∞–≤–∫–∞", "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞", "–∏–Ω—Ñ–ª—è—Ü–∏—è",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞", "—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π", "–≤–≤–ø", "—Ä–µ—Ü–µ—Å—Å–∏—è",
    "–±–∞–Ω–∫", "–∫—Ä–µ–¥–∏—Ç", "–∏–ø–æ—Ç–µ–∫–∞", "–≤–∫–ª–∞–¥", "–¥–µ–ø–æ–∑–∏—Ç",
    "—Ñ–æ–Ω–¥", "–≤–µ–Ω—á—É—Ä–Ω—ã–π", "—Ä–∞—É–Ω–¥ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è",
    "—Å–¥–µ–ª–∫–∞", "—Å–ª–∏—è–Ω–∏–µ", "–ø–æ–≥–ª–æ—â–µ–Ω–∏–µ", "m&a",
    "—Ä—ã–Ω–æ–∫", "–¥–æ–ª—è —Ä—ã–Ω–∫–∞", "–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã",
    "—Ü–µ–Ω–∞ –∞–∫—Ü–∏–π", "—Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏", "–æ—Ü–µ–Ω–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏",
    "–≤—ã—Ö–æ–¥ –Ω–∞ –±–∏—Ä–∂—É", "—Ä–∞–∑–º–µ—â–µ–Ω–∏–µ", "–ª–∏—Å—Ç–∏–Ω–≥",
    
    # –ö–∞–¥—Ä—ã
    "–Ω–∞–∑–Ω–∞—á–µ–Ω", "–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ", "–æ—Ç—Å—Ç–∞–≤–∫–∞", "—É–≤–æ–ª–µ–Ω",
    "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "ceo", "–æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å —É—à—ë–ª",
    "—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —à—Ç–∞—Ç–∞", "—É–≤–æ–ª—å–Ω–µ–Ω–∏—è", "—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è",
    "–æ—Ñ–∏—Å", "—à—Ç–∞–±-–∫–≤–∞—Ä—Ç–∏—Ä–∞", "–ø–µ—Ä–µ–µ–∑–¥ –∫–æ–º–ø–∞–Ω–∏–∏",
    
    # –°–ø–æ—Ä—Ç
    "—Ç–µ–Ω–Ω–∏—Å", "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "–±–∞—Å–∫–µ—Ç–±–æ–ª", "—Å–ø–æ—Ä—Ç", "–º–∞—Ç—á",
    "–æ–ª–∏–º–ø–∏–∞–¥–∞", "—á–µ–º–ø–∏–æ–Ω–∞—Ç", "—Ç—É—Ä–Ω–∏—Ä", "—Å–±–æ—Ä–Ω–∞—è",
    
    # –ò–≥—Ä—ã
    "–∏–≥—Ä–∞", "–≥–µ–π–º–ø–ª–µ–π", "playstation", "xbox", "steam", "nintendo",
    "–≤–∏–¥–µ–æ–∏–≥—Ä–∞", "–∫–æ–Ω—Å–æ–ª—å", "gaming",
    
    # –†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è
    "–∫–∏–Ω–æ", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "–º—É–∑—ã–∫–∞", "–∫–æ–Ω—Ü–µ—Ä—Ç", "–∞–∫—Ç—ë—Ä", "–∞–∫—Ç–µ—Ä",
    "–ø—Ä–µ–º—å–µ—Ä–∞", "—Ç—Ä–µ–π–ª–µ—Ä", "netflix", "–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä",
    
    # –ü–æ–ª–∏—Ç–∏–∫–∞
    "–≤—ã–±–æ—Ä—ã", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–ø–∞—Ä–ª–∞–º–µ–Ω—Ç", "–ø–æ–ª–∏—Ç–∏–∫", "–¥–µ–ø—É—Ç–∞—Ç",
    "—Å–∞–Ω–∫—Ü–∏–∏", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–º–∏–Ω–∏—Å—Ç—Ä", "–∑–∞–∫–æ–Ω", "–∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç",
    
    # –ú–µ–¥–∏—Ü–∏–Ω–∞
    "–±–æ–ª–µ–∑–Ω—å", "covid", "–ø–∞–Ω–¥–µ–º–∏—è", "–≥—Ä–∏–ø–ø", "–≤–∞–∫—Ü–∏–Ω–∞",
    
    # –ö—Ä–∏–ø—Ç–∞
    "–∫—Ä–∏–ø—Ç–æ", "bitcoin", "–±–∏—Ç–∫–æ–π–Ω", "–±–∏—Ç–∫–æ–∏–Ω", "ethereum",
    "nft", "–±–ª–æ–∫—á–µ–π–Ω", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "–º–∞–π–Ω–∏–Ω–≥",
    
    # –Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ
    "—Å—É–¥", "—Å—É–¥–µ–±–Ω—ã–π", "–∞—Ä–µ—Å—Ç", "–ø—Ä–∏–≥–æ–≤–æ—Ä", "—Ç—é—Ä—å–º–∞", "—à—Ç—Ä–∞—Ñ",
    "–∏—Å–∫", "–∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω—ã–π",
    
    # –ê—Ä—Ö–µ–æ–ª–æ–≥–∏—è –∏ –∏—Å—Ç–æ—Ä–∏—è
    "–∞—Ä—Ö–µ–æ–ª–æ–≥", "–∞—Ä—Ö–µ–æ–ª–æ–≥–∏—è", "–∞—Ä—Ö–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π", "—Ä–∞—Å–∫–æ–ø–∫–∏",
    "–¥—Ä–µ–≤–Ω", "–∞—Ä—Ç–µ—Ñ–∞–∫—Ç", "–ø–∞–ª–µ–æ–Ω—Ç–æ–ª–æ–≥", "–æ–∫–∞–º–µ–Ω–µ–ª–æ—Å—Ç–∏",
    "–¥–æ–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π", "–ø–∞–ª–µ–æ–ª–∏—Ç", "–Ω–µ–æ–ª–∏—Ç", "–º–µ–∑–æ–ª–∏—Ç",
    "–ø–∞–º—è—Ç–Ω–∏–∫ –∫—É–ª—å—Ç—É—Ä—ã", "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –ø–∞–º—è—Ç–Ω–∏–∫",
    "—Ç—ã—Å—è—á –ª–µ—Ç", "–º–∏–ª–ª–∏–æ–Ω –ª–µ—Ç", "–≤–æ–∑—Ä–∞—Å—Ç —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç",
    "–æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤–æ –≤—Ä–µ–º—è —Ä–∞—Å–∫–æ–ø–æ–∫", "–Ω–∞–π–¥–µ–Ω –ø—Ä–∏ —Ä–∞—Å–∫–æ–ø–∫–∞—Ö",
    "–∞–Ω—Ç–∏—á–Ω", "—Å—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤", "–¥–∏–Ω–∞—Å—Ç–∏—è", "—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏—è",
    "–∑–∞—Ö–æ—Ä–æ–Ω–µ–Ω–∏–µ", "–≥—Ä–æ–±–Ω–∏—Ü–∞", "–º—É–º–∏—è", "—Å–∞—Ä–∫–æ—Ñ–∞–≥",
    
    # –ù–û–í–û–ï: –ê–≤—Ç–æ–º–æ–±–∏–ª–∏ –∏ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç
    "–∞–≤—Ç–æ–º–æ–±–∏–ª—å", "–∞–≤—Ç–æ–º–æ–±–∏–ª", "–º–∞—à–∏–Ω–∞", "–∞–≤—Ç–æ", "–∞–≤—Ç–æ–ø—Ä–æ–º",
    "—ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª—å", "—ç–ª–µ–∫—Ç—Ä–æ–∫–∞—Ä", "—ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª",
    "tesla", "—Ç–µ—Å–ª–∞", "bmw", "mercedes", "audi", "volkswagen",
    "toyota", "honda", "ford", "chevrolet", "nissan",
    "–¥–≤–∏–≥–∞—Ç–µ–ª—å", "–º–æ—Ç–æ—Ä", "–∫–æ—Ä–æ–±–∫–∞ –ø–µ—Ä–µ–¥–∞—á", "—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è",
    "–±–µ–Ω–∑–∏–Ω", "–¥–∏–∑–µ–ª—å", "–∑–∞–ø—Ä–∞–≤–∫–∞", "—Ç–æ–ø–ª–∏–≤–æ",
    "–∫—Ä–æ—Å—Å–æ–≤–µ—Ä", "—Å–µ–¥–∞–Ω", "—Ö—ç—Ç—á–±–µ–∫", "–≤–Ω–µ–¥–æ—Ä–æ–∂–Ω–∏–∫", "—Å—Év",
    "–ø—Ä–æ–±–µ–≥", "—Ä–∞—Å—Ö–æ–¥ —Ç–æ–ø–ª–∏–≤–∞", "—Ä–∞–∑–≥–æ–Ω", "–ª–æ—à–∞–¥–∏–Ω—ã—Ö —Å–∏–ª",
    "–∑–∞–ø–∞—Å —Ö–æ–¥–∞", "–±–∞—Ç–∞—Ä–µ—è", "–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä —ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª—è",
    "–∑–∞—Ä—è–¥–Ω–∞—è —Å—Ç–∞–Ω—Ü–∏—è", "–∑–∞—Ä—è–¥–∫–∞ —ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª—è",
    "–∞–≤—Ç–æ—Å–∞–ª–æ–Ω", "–¥–∏–ª–µ—Ä", "—Ç–µ—Å—Ç-–¥—Ä–∞–π–≤",
    "–ø–¥–¥", "–≥–∏–±–¥–¥", "—à—Ç—Ä–∞—Ñ –∑–∞", "–¥–æ—Ä–æ–∂–Ω",
    "–ø–∞—Ä–∫–æ–≤–∫–∞", "—Å—Ç–æ—è–Ω–∫–∞", "–≥–∞—Ä–∞–∂",
    "—à–∏–Ω—ã", "—Ä–µ–∑–∏–Ω–∞", "–∫–æ–ª—ë—Å–∞", "–¥–∏—Å–∫–∏",
    "–∫—É–∑–æ–≤", "—Å–∞–ª–æ–Ω –∞–≤—Ç–æ–º–æ–±–∏–ª—è", "–±–∞–≥–∞–∂–Ω–∏–∫",
    "—Ä—É–ª—å", "–ø–µ–¥–∞–ª—å", "—Ç–æ—Ä–º–æ–∑",
    "geely", "haval", "chery", "lada", "—É–∞–∑",
    "lamborghini", "ferrari", "porsche", "maserati",
    "electric vehicle", "ev", "hybrid", "–≥–∏–±—Ä–∏–¥"
]

BAD_PHRASES = [
    "–ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ", "–ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ",
    "–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∑–∞—â–∏—Ç—É", "–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–¥—ë–∂–Ω—É—é –∑–∞—â–∏—Ç—É",
    "–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–∞—â–∏—Ç—É", "–ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Å–≤–æ–∏—Ö –∑–∞–¥–∞—á–∞—Ö",
    "–ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ –¥—É–º–∞—Ç—å –æ–± —É–≥—Ä–æ–∑–∞—Ö", "–¥–µ–ª–∞–µ—Ç –±–∏–∑–Ω–µ—Å —É—Å—Ç–æ–π—á–∏–≤–µ–µ",
    "–ø–æ–∑–≤–æ–ª—è–µ—Ç –±–∏–∑–Ω–µ—Å—É —Ä–∞–±–æ—Ç–∞—Ç—å —É—Å—Ç–æ–π—á–∏–≤–µ–µ", "–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ø—Ä–æ—â–∞–µ—Ç",
    "–∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ —É–ø—Ä–æ—â–∞–µ—Ç", "–∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è",
    "–∏–¥–µ–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è", "–ø–æ–º–æ–≥–∞–µ—Ç –±–∏–∑–Ω–µ—Å—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Ä–∞–±–æ—Ç–∞—Ç—å",
]

def is_too_promotional(text: str) -> bool:
    low = text.lower()
    if any(p in low for p in BAD_PHRASES):
        return True
    if ("–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç" in low or "–ø–æ–∑–≤–æ–ª—è–µ—Ç" in low or "–ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ" in low) and \
       not any(k in low for k in ["–∑–∞ —Å—á—ë—Ç", "–∑–∞ —Å—á–µ—Ç", "–∏—Å–ø–æ–ª—å–∑—É—è", "—á–µ—Ä–µ–∑", "–Ω–∞–ø—Ä–∏–º–µ—Ä", 
                                   "–≤ —Ç–æ–º —á–∏—Å–ª–µ", "—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏", "–∞–Ω–∞–ª–∏–∑ —Ç—Ä–∞—Ñ–∏–∫–∞", 
                                   "rate limiting", "–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫"]):
        return True
    return False


# ============ URL NORMALIZATION ============

def normalize_url(url: str) -> str:
    if not url:
        return ""
    
    try:
        parsed = urlparse(url)
        path = parsed.path.rstrip("/")
        domain = parsed.netloc.lower().replace("www.", "")
        normalized = f"{domain}{path}"
        return normalized
    except Exception:
        url = url.replace("https://", "").replace("http://", "")
        url = url.replace("www.", "")
        url = url.split("?")[0].split("#")[0]
        return url.rstrip("/").lower()


def extract_article_id(url: str) -> str:
    normalized = normalize_url(url)
    
    habr_match = re.search(r'habr\.com/.+?/(\d{5,7})', normalized)
    if habr_match:
        return f"habr_{habr_match.group(1)}"
    
    dnews_match = re.search(r'3dnews\.ru/(\d+)', normalized)
    if dnews_match:
        return f"3dnews_{dnews_match.group(1)}"
    
    if 'ixbt.com' in normalized:
        return f"ixbt_{hashlib.md5(normalized.encode()).hexdigest()[:12]}"
    
    return hashlib.md5(normalized.encode()).hexdigest()[:16]


# ============ POSTED ARTICLES MANAGER ============

class PostedManager:
    def __init__(self, filepath: str):
        self.filepath = filepath
        self.posted_ids: set = set()
        self.posted_urls: set = set()
        self.data: list = []
        self._load()
    
    def _load(self):
        print(f"\n{'='*50}")
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏: {self.filepath}")
        
        if not os.path.exists(self.filepath):
            print("   ‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π")
            self._save()
            return
        
        try:
            with open(self.filepath, "r", encoding="utf-8") as f:
                self.data = json.load(f)
            
            if not isinstance(self.data, list):
                print("   ‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º")
                self.data = []
                return
            
            for item in self.data:
                if isinstance(item, dict) and "id" in item:
                    url = item["id"]
                    self.posted_urls.add(normalize_url(url))
                    self.posted_ids.add(extract_article_id(url))
            
            print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.data)} –∑–∞–ø–∏—Å–µ–π")
            print(f"   üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID: {len(self.posted_ids)}")
            
        except json.JSONDecodeError as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ JSON: {e}")
            self.data = []
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
            self.data = []
    
    def _save(self):
        try:
            with open(self.filepath, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(self.data)} –∑–∞–ø–∏—Å–µ–π")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    def is_posted(self, url: str) -> bool:
        article_id = extract_article_id(url)
        if article_id in self.posted_ids:
            return True
        
        normalized = normalize_url(url)
        if normalized in self.posted_urls:
            return True
        
        return False
    
    def add(self, url: str, title: str = ""):
        article_id = extract_article_id(url)
        normalized = normalize_url(url)
        
        self.posted_ids.add(article_id)
        self.posted_urls.add(normalized)
        
        self.data.append({
            "id": url,
            "article_id": article_id,
            "title": title[:100] if title else "",
            "timestamp": datetime.now().timestamp()
        })
        
        self._save()
        print(f"   üìù –î–æ–±–∞–≤–ª–µ–Ω–æ: {article_id}")
    
    def cleanup(self, days: int = 30):
        if not self.data:
            return
        
        cutoff = datetime.now().timestamp() - (days * 86400)
        old_count = len(self.data)
        
        self.data = [
            item for item in self.data
            if item.get("timestamp") is None or item.get("timestamp", 0) > cutoff
        ]
        
        removed = old_count - len(self.data)
        if removed > 0:
            self.posted_ids.clear()
            self.posted_urls.clear()
            for item in self.data:
                if "id" in item:
                    self.posted_urls.add(normalize_url(item["id"]))
                    self.posted_ids.add(extract_article_id(item["id"]))
            
            self._save()
            print(f"üßπ –û—á–∏—â–µ–Ω–æ: {removed} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π")
    
    def count(self) -> int:
        return len(self.data)


posted = PostedManager(POSTED_FILE)


# ============ HELPERS ============

def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())

def detect_topic(title: str, summary: str) -> str:
    text = f"{title} {summary}".lower()
    if any(kw in text for kw in ["gpt", "chatgpt", "claude", "llm", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å"]):
        return "llm"
    elif any(kw in text for kw in ["midjourney", "dall-e", "stable diffusion", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂"]):
        return "image_gen"
    elif any(kw in text for kw in ["—Ä–æ–±–æ—Ç", "robot", "–∞–≤—Ç–æ–Ω–æ–º–Ω"]):
        return "robotics"
    elif any(kw in text for kw in ["nvidia", "gpu", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—á–∏–ø"]):
        return "hardware"
    else:
        return "ai"

def get_hashtags(topic: str) -> str:
    hashtag_map = {
        "llm": "#ChatGPT #LLM #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "image_gen": "#AI #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "robotics": "#—Ä–æ–±–æ—Ç—ã #AI #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "hardware": "#–∂–µ–ª–µ–∑–æ #GPU #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "ai": "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
    }
    return hashtag_map.get(topic, "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏")

def ensure_complete_sentence(text: str) -> str:
    text = text.strip()
    if not text:
        return text
    if text[-1] in '.!?':
        return text
    last_end = max(text.rfind('.'), text.rfind('!'), text.rfind('?'))
    if last_end > 0:
        return text[:last_end + 1]
    return text + '.'

def trim_core_text_to_limit(core_text: str, max_core_length: int) -> str:
    core_text = core_text.strip()
    if len(core_text) <= max_core_length:
        return ensure_complete_sentence(core_text)
    
    sentences = re.split(r'(?<=[.!?])\s+', core_text)
    result = ""
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        candidate = (result + " " + sentence).strip() if result else sentence
        if len(candidate) <= max_core_length:
            result = candidate
        else:
            break
    
    if not result and sentences:
        result = sentences[0][:max_core_length]
        if ' ' in result:
            result = result.rsplit(' ', 1)[0]
    
    return ensure_complete_sentence(result)

def build_final_post(core_text: str, hashtags: str, link: str, max_total: int = 1024) -> str:
    cta_line = "\n\nüî• ‚Äî –æ–≥–æ–Ω—å! | üóø ‚Äî –Ω—É —Ç–∞–∫–æ–µ | ‚ö° ‚Äî –±—É–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è"
    source_line = f'\nüîó <a href="{link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
    hashtag_line = f"\n\n{hashtags}"
    
    service_length = len(cta_line) + len(hashtag_line) + len(source_line)
    max_core_length = max_total - service_length - 10
    
    trimmed_core = trim_core_text_to_limit(core_text, max_core_length)
    return trimmed_core + cta_line + hashtag_line + source_line


# ============ PARSERS ============

def load_rss(url: str, source: str) -> List[Dict]:
    articles = []
    try:
        feed = feedparser.parse(url)
        if feed.bozo and not feed.entries:
            print(f"   ‚ö†Ô∏è RSS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {source}")
            return articles
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ RSS {source}: {e}")
        return articles

    new_count = 0
    skip_count = 0
    
    for entry in feed.entries[:30]:
        link = entry.get("link", "")
        if not link:
            continue
        
        title = clean_text(entry.get("title") or "")
        
        if posted.is_posted(link):
            skip_count += 1
            continue
        
        new_count += 1
        articles.append({
            "id": link,
            "title": title,
            "summary": clean_text(entry.get("summary") or entry.get("description") or "")[:700],
            "link": link,
            "source": source,
            "published_parsed": datetime.now()
        })
    
    print(f"   üì∞ {source}: +{new_count} –Ω–æ–≤—ã—Ö, ‚è≠Ô∏è{skip_count} –ø—Ä–æ–ø—É—â–µ–Ω–æ")
    return articles

def load_articles_from_sites() -> List[Dict]:
    print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ RSS –ª–µ–Ω—Ç...")
    articles: List[Dict] = []
    
    # –¢–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ AI-—Ö–∞–±—ã Habr
    articles.extend(load_rss("https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru", "Habr AI"))
    articles.extend(load_rss("https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru", "Habr ML"))
    articles.extend(load_rss("https://habr.com/ru/rss/hub/neural_networks/all/?fl=ru", "Habr Neural"))
    
    # –û–±—â–∏–µ —Ç–µ—Ö–Ω–æ-–ª–µ–Ω—Ç—ã (—Å –∂—ë—Å—Ç–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π)
    articles.extend(load_rss("https://3dnews.ru/news/rss/", "3DNews"))
    articles.extend(load_rss("https://www.ixbt.com/export/news.rss", "iXBT"))
    
    print(f"\nüìä –í—Å–µ–≥–æ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(articles)}")
    return articles

def filter_articles(articles: List[Dict]) -> List[Dict]:
    """
    –°–¢–†–û–ì–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø:
    1. –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–º—ã (–∞–≤—Ç–æ, –∞—Ä—Ö–µ–æ–ª–æ–≥–∏—è, —Ñ–∏–Ω–∞–Ω—Å—ã –∏ —Ç.–¥.)
    2. –û—Å—Ç–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û —Å—Ç–∞—Ç—å–∏ —Å AI-–∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    """
    valid = []
    filtered_out = {"exclude": 0, "no_ai": 0}
    debug_excluded = []  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
    
    for e in articles:
        text = f"{e['title']} {e['summary']}".lower()
        
        # –®–∞–≥ 1: –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–º—ã
        excluded_kw = next((kw for kw in EXCLUDE_KEYWORDS if kw in text), None)
        if excluded_kw:
            filtered_out["exclude"] += 1
            debug_excluded.append(f"{e['title'][:50]}... (–∏—Å–∫–ª—é—á–µ–Ω–æ: '{excluded_kw}')")
            continue
        
        # –®–∞–≥ 2: –û—Å—Ç–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û AI-—Ç–µ–º–∞—Ç–∏–∫—É
        if not any(kw in text for kw in AI_KEYWORDS):
            filtered_out["no_ai"] += 1
            debug_excluded.append(f"{e['title'][:50]}... (–Ω–µ—Ç AI-—Å–ª–æ–≤)")
            continue
        
        valid.append(e)
    
    # –í—ã–≤–æ–¥ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    if debug_excluded:
        print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π:")
        for ex in debug_excluded[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"   ‚ùå {ex}")
    
    print(f"\n‚ùå –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {filtered_out['exclude']} (–∏—Å–∫–ª—é—á–µ–Ω–∏—è), {filtered_out['no_ai']} (–Ω–µ AI)")
    print(f"üéØ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (AI-—Ç–µ–º–∞—Ç–∏–∫–∞): {len(valid)}")
    
    valid.sort(key=lambda x: x["published_parsed"], reverse=True)
    return valid


# ============ GROQ ============

def build_dynamic_prompt(title: str, summary: str) -> str:
    return f"""
–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∞–≤—Ç–æ—Ä –∫–∞–Ω–∞–ª–∞ –ø—Ä–æ AI-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ù–∞–ø–∏—Å–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ—Å—Ç –ø—Ä–æ –ò–ò.

–ù–û–í–û–°–¢–¨:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}

–¢–µ–∫—Å—Ç: {summary}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –¢–ï–ö–°–¢–£:
1. –ù–ê–ß–ê–õ–û: –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–∞—á–Ω–∏ —Å —Ñ—Ä–∞–∑—ã "–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! üëã" –∏–ª–∏ "–ü—Ä–∏–≤–µ—Ç, –¥—Ä—É–∑—å—è! ‚úåÔ∏è".
2. –°–¢–ò–õ–¨: 
   - –ü–∏—à–∏ –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º, –∫–∞–∫ –±—É–¥—Ç–æ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—à—å –¥—Ä—É–≥—É.
   - –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å—É—Ö–æ–π "–Ω–æ–≤–æ—Å—Ç–Ω–æ–π" —Å—Ç–∏–ª—å. 
   - –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ä–µ–∫–ª–∞–º–Ω—ã–π —Å—Ç–∏–ª—å.
   - –ò–∑–±–µ–≥–∞–π —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏—á–∞—Å—Ç–∏–π, –ø–∏—à–∏ –ø—Ä–æ—Å—Ç–æ.
3. –°–û–î–ï–†–ñ–ê–ù–ò–ï:
   - –û–±—ä—è—Å–Ω–∏ —Å—É—Ç—å: —á—Ç–æ –∏–º–µ–Ω–Ω–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –≤ –º–∏—Ä–µ AI?
   - –ö–∞–∫ —ç—Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç?
   - –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º?
4. –û–ë–™–ï–ú: 1000-1200 –∑–Ω–∞–∫–æ–≤.

–ó–ê–ü–†–ï–¢–´:
- –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–≤–∞: "—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π", "–±–µ—Å–ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–Ω—ã–π", "–ø–æ–∫—É–ø–∞–π—Ç–µ", "–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å".
- –ù–µ —à—É—Ç–∏ –ø—Ä–æ –≤–æ—Å—Å—Ç–∞–Ω–∏–µ –º–∞—à–∏–Ω –∏ Skynet.
- –ù–µ —É–ø–æ–º–∏–Ω–∞–π –∞–≤—Ç–æ–º–æ–±–∏–ª–∏, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø—Ä–æ AI –≤ –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–∞—Ö.
"""

def short_summary(title: str, summary: str, link: str) -> Optional[str]:
    print(f"   üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...")
    
    try:
        res = groq_client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "user", "content": build_dynamic_prompt(title, summary)}],
            temperature=0.7,
            max_tokens=1000,
        )
        core = res.choices[0].message.content.strip()
        
        if core.startswith('"') and core.endswith('"'):
            core = core[1:-1]
        
        if is_too_promotional(core):
            print("   ‚ö†Ô∏è –¢–µ–∫—Å—Ç —Ä–µ–∫–ª–∞–º–Ω—ã–π, –ø—Ä–æ–ø—É—Å–∫")
            return None
        
        topic = detect_topic(title, summary)
        return build_final_post(core, get_hashtags(topic), link, TELEGRAM_CAPTION_LIMIT)
    
    except Exception as e:
        print(f"   ‚ùå Groq –æ—à–∏–±–∫–∞: {e}")
        return None


# ============ IMAGE ============

def generate_image(title: str, max_retries: int = 2) -> Optional[str]:
    styles = [
        "minimalist technology illustration, clean lines, white background, vector art",
        "abstract neural network visualization, connecting dots, blue gradient",
        "isometric 3d icon of AI, glass texture, soft studio lighting",
    ]
    
    for attempt in range(max_retries):
        seed = random.randint(0, 10**7)
        clean_title = re.sub(r'[^a-zA-Z0-9\s]', '', title)[:50]
        prompt = f"{random.choice(styles)}, {clean_title}"
        encoded = urllib.parse.quote(prompt)
        url = f"https://image.pollinations.ai/prompt/{encoded}?seed={seed}&width=1024&height=1024&nologo=true"
        
        try:
            print(f"   üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏ ({attempt+1}/{max_retries})...")
            resp = requests.get(url, timeout=40, headers=HEADERS)
            if resp.status_code == 200 and len(resp.content) > 10000:
                fname = f"img_{seed}.jpg"
                with open(fname, "wb") as f:
                    f.write(resp.content)
                return fname
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
    
    return None

def cleanup_image(filepath: Optional[str]):
    if filepath and os.path.exists(filepath):
        try:
            os.remove(filepath)
        except:
            pass


# ============ MAIN ============

async def autopost():
    print("\n" + "="*60)
    print(f"üöÄ –°–¢–ê–†–¢: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìä –í –∏—Å—Ç–æ—Ä–∏–∏: {posted.count()} —Å—Ç–∞—Ç–µ–π")
    print("="*60)
    
    posted.cleanup(RETENTION_DAYS)
    
    articles = load_articles_from_sites()
    candidates = filter_articles(articles)
    
    if not candidates:
        print("\n‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ AI")
        return
    
    art = candidates[0]
    article_id = extract_article_id(art["link"])
    
    print(f"\nüéØ –í—ã–±—Ä–∞–Ω–∞ —Å—Ç–∞—Ç—å—è:")
    print(f"   ID: {article_id}")
    print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {art['title'][:60]}...")
    print(f"   URL: {art['link']}")
    
    post_text = short_summary(art["title"], art["summary"], art["link"])
    
    if not post_text:
        print("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç")
        return
    
    img = generate_image(art["title"])
    
    try:
        if img:
            await bot.send_photo(CHANNEL_ID, photo=FSInputFile(img), caption=post_text)
        else:
            await bot.send_message(CHANNEL_ID, text=post_text, disable_web_page_preview=False)
        
        posted.add(art["link"], art["title"])
        
        print(f"\n‚úÖ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û!")
        print(f"üìä –¢–µ–ø–µ—Ä—å –≤ –∏—Å—Ç–æ—Ä–∏–∏: {posted.count()} —Å—Ç–∞—Ç–µ–π")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ Telegram: {e}")
    finally:
        cleanup_image(img)


async def main():
    try:
        await autopost()
    finally:
        await bot.session.close()
    print("\n" + "="*60)
    print("‚úÖ –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*60)


if __name__ == "__main__":
    asyncio.run(main())























































































































































































































































