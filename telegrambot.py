import os
import json
import asyncio
import random
import re
import hashlib
import logging
from datetime import datetime, timezone
from typing import List, Dict, Optional
from urllib.parse import urlparse, quote

import aiohttp
import feedparser
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq

# ====================== –õ–û–ì–ò ======================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("ai_poster.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ====================== CONFIG ======================
class Config:
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.telegram_token = os.getenv("TELEGRAM_BOT_TOKEN")
        self.channel_id = os.getenv("CHANNEL_ID")
        self.retention_days = int(os.getenv("RETENTION_DAYS", "30"))
        self.caption_limit = 1024
        self.posted_file = "posted_articles.json"

        missing = []
        for var, name in [(self.groq_api_key, "GROQ_API_KEY"),
                          (self.telegram_token, "TELEGRAM_BOT_TOKEN"),
                          (self.channel_id, "CHANNEL_ID")]:
            if not var:
                missing.append(name)
        if missing:
            raise SystemExit(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {', '.join(missing)}")

config = Config()

bot = Bot(token=config.telegram_token, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=config.groq_api_key)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0 Safari/537.36"
}

# ====================== RSS ======================
RSS_FEEDS = [
    ("https://techcrunch.com/category/artificial-intelligence/feed/", "TechCrunch AI"),
    ("https://venturebeat.com/category/ai/feed/", "VentureBeat AI"),
    ("https://www.technologyreview.com/topic/artificial-intelligence/feed", "MIT Tech Review"),
    ("https://www.theverge.com/rss/index.xml", "The Verge"),
    ("https://arstechnica.com/tag/artificial-intelligence/feed/", "Ars Technica AI"),
    ("https://www.wired.com/feed/tag/ai/latest/rss", "WIRED AI"),
]

# ====================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ======================
AI_KEYWORDS = [
    "ai ", " ai", "artificial intelligence", "machine learning", "deep learning", "neural network",
    "llm", "large language model", "gpt", "chatgpt", "claude", "gemini", "grok", "llama",
    "mistral", "qwen", "deepseek", "midjourney", "dall-e", "stable diffusion", "sora", 
    "groq", "openai", "anthropic", "deepmind", "hugging face", "nvidia", "agi", 
    "inference", "rlhf", "transformer", "generative", "chatbot"
]

EXCLUDE_KEYWORDS = [
    "stock price", "ipo", "earnings call", "quarterly results", "revenue beat", "profit margin", 
    "dividend", "market cap", "wall street",
    "ps5", "xbox", "nintendo switch", "game review", "gameplay", "gaming pc",
    "netflix series", "movie review", "box office", "trailer", "premiere",
    "tesla stock", "ev sales", "model 3", "model y", "cybertruck",
    "bitcoin", "crypto", "blockchain", "nft", "ethereum",
    "election", "trump", "biden", "congress", "senate", "white house"
]

BAD_PHRASES = ["sponsored", "partner content", "advertisement", "black friday", "deal alert", "coupon"]

# ====================== DATACLASSES ======================
from dataclasses import dataclass, field

@dataclass
class Article:
    title: str
    summary: str
    link: str
    source: str
    published: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

# ====================== TOPIC & HASHTAGS ======================
class Topic:
    LLM = "llm"
    IMAGE_GEN = "image_gen"
    ROBOTICS = "robotics"
    HARDWARE = "hardware"
    GENERAL = "general"
    
    HASHTAGS = {
        LLM: "#ChatGPT #LLM #OpenAI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        IMAGE_GEN: "#Midjourney #DALLE #StableDiffusion #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
        ROBOTICS: "#—Ä–æ–±–æ—Ç—ã #Humanoid #—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
        HARDWARE: "#NVIDIA #GPU #—á–∏–ø—ã #–∂–µ–ª–µ–∑–æ",
        GENERAL: "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
    }

    @staticmethod
    def detect(text: str) -> str:
        t = text.lower()
        if any(x in t for x in ["gpt", "chatgpt", "claude", "gemini", "llama", "grok", "llm", "language model"]):
            return Topic.LLM
        if any(x in t for x in ["midjourney", "dall-e", "dalle", "stable diffusion", "flux", "image gen", "sora"]):
            return Topic.IMAGE_GEN
        if any(x in t for x in ["robot", "humanoid", "boston dynamics", "optimus", "figure ai"]):
            return Topic.ROBOTICS
        if any(x in t for x in ["nvidia", "h100", "h200", "blackwell", "gpu", "tensor core", "cuda"]):
            return Topic.HARDWARE
        return Topic.GENERAL

# ====================== HELPERS ======================
def normalize_url(url: str) -> str:
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        path = parsed.path.rstrip("/")
        domain = parsed.netloc.lower().replace("www.", "")
        return f"{domain}{path}".split("?")[0].split("#")[0]
    except:
        return url.split("?")[0].split("#")[0]

def article_id(url: str) -> str:
    return hashlib.md5(normalize_url(url).encode()).hexdigest()[:16]

def clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def ai_relevance(text: str) -> float:
    lower = text.lower()
    matches = sum(1 for kw in AI_KEYWORDS if kw in lower)
    return min(matches / 3.0, 1.0)

# ====================== POSTED MANAGER ======================
class PostedManager:
    def __init__(self, file="posted_articles.json"):
        self.file = file
        self.data = []
        self.ids = set()
        self.urls = set()
        self._load()

    def _load(self):
        if not os.path.exists(self.file):
            self._save()
            return
        try:
            with open(self.file, "r", encoding="utf-8") as f:
                self.data = json.load(f)
            
            for item in self.data:
                url = item.get("url", "")
                if url:
                    self.ids.add(article_id(url))
                    self.urls.add(normalize_url(url))
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ posted_articles.json: {e}")
            self.data = []

    def _save(self):
        try:
            with open(self.file, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

    def is_posted(self, url: str) -> bool:
        return article_id(url) in self.ids or normalize_url(url) in self.urls

    def add(self, url: str, title: str):
        aid = article_id(url)
        nurl = normalize_url(url)
        
        if aid in self.ids or nurl in self.urls:
            return
        
        self.ids.add(aid)
        self.urls.add(nurl)
        self.data.append({
            "url": url,
            "title": title[:100],
            "ts": datetime.now(timezone.utc).isoformat() + "Z"
        })
        self._save()

    def cleanup(self, days=30):
        cutoff = datetime.now(timezone.utc).timestamp() - days * 86400
        old_count = len(self.data)
        
        self.data = [
            item for item in self.data
            if self._parse_ts(item.get("ts")) > cutoff
        ]
        
        removed = old_count - len(self.data)
        if removed > 0:
            self.ids.clear()
            self.urls.clear()
            for item in self.data:
                url = item.get("url", "")
                if url:
                    self.ids.add(article_id(url))
                    self.urls.add(normalize_url(url))
            
            self._save()
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ {removed} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π")
    
    def _parse_ts(self, ts_str: Optional[str]) -> float:
        if not ts_str:
            return 0
        try:
            dt = datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
            return dt.timestamp()
        except:
            return 0

# ====================== RSS LOADER ======================
async def fetch_feed(session: aiohttp.ClientSession, url: str, source: str, posted: PostedManager) -> List[Article]:
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as resp:
            if resp.status != 200:
                logger.warning(f"{source}: HTTP {resp.status}")
                return []
            text = await resp.text()
    except Exception as e:
        logger.warning(f"{source} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return []

    try:
        feed = feedparser.parse(text)
    except Exception as e:
        logger.error(f"{source}: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS - {e}")
        return []

    articles = []
    for entry in feed.entries[:25]:
        link = entry.get("link", "").strip()
        if not link or posted.is_posted(link):
            continue

        title = clean_text(entry.get("title") or "")
        summary = clean_text(entry.get("summary") or entry.get("description") or "")[:1500]

        if not title or len(title) < 15:
            continue

        published = datetime.now(timezone.utc)
        for date_field in ["published", "updated", "created"]:
            date_str = entry.get(date_field)
            if date_str:
                try:
                    parsed = feedparser._parse_date(date_str)
                    if parsed:
                        published = datetime(*parsed[:6], tzinfo=timezone.utc)
                        break
                except:
                    pass

        articles.append(Article(
            title=title,
            summary=summary,
            link=link,
            source=source,
            published=published
        ))

    return articles

async def load_all_feeds(posted: PostedManager) -> List[Article]:
    logger.info("üîÑ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø–∞–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    
    connector = aiohttp.TCPConnector(limit_per_host=5, limit=30)
    async with aiohttp.ClientSession(headers=HEADERS, connector=connector) as session:
        tasks = [fetch_feed(session, url, name, posted) for url, name in RSS_FEEDS]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    all_articles = []
    for res, (url, name) in zip(results, RSS_FEEDS):
        if isinstance(res, list) and res:
            all_articles.extend(res)
            logger.info(f"‚úÖ {name}: {len(res)} —Å—Ç–∞—Ç–µ–π")
        elif isinstance(res, Exception):
            logger.error(f"‚ùå {name}: {res}")

    logger.info(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_articles)}")
    return all_articles

# ====================== FILTER ======================
def filter_articles(articles: List[Article]) -> List[Article]:
    candidates = []
    
    for a in articles:
        text = f"{a.title} {a.summary}".lower()

        if any(phrase in text for phrase in BAD_PHRASES):
            continue
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue
        if not any(kw in text for kw in AI_KEYWORDS):
            continue
        if ai_relevance(text) < 0.5:
            continue

        candidates.append(a)

    candidates.sort(key=lambda x: x.published, reverse=True)
    logger.info(f"üéØ –ü—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä—ã: {len(candidates)} —Å—Ç–∞—Ç–µ–π")
    return candidates

# ====================== SUMMARY + TRANSLATE ======================
GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "llama3-70b-8192",
    "mixtral-8x7b-32768",
]

async def generate_summary(article: Article) -> Optional[str]:
    logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞: {article.title[:60]}...")
    
    prompt = f"""–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ç–æ–ø–æ–≤–æ–≥–æ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (50–∫+ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤).

–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å (English):
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}
–û–ø–∏—Å–∞–Ω–∏–µ: {article.summary[:2000]}

–ù–∞–ø–∏—à–∏ –ø–æ—Å—Ç –Ω–∞ –†–£–°–°–ö–û–ú —è–∑—ã–∫–µ –≤ –∂–∏–≤–æ–º, —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º —Å—Ç–∏–ª–µ:
- –ù–∞—á–Ω–∏ —Å —è—Ä–∫–æ–≥–æ —Ö—É–∫–∞ (–≤–æ–ø—Ä–æ—Å, –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ, —ç–º–æ–¥–∑–∏)
- –û–±—ä—è—Å–Ω–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –ß–¢–û –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏ –ü–û–ß–ï–ú–£ —ç—Ç–æ –≤–∞–∂–Ω–æ
- –î–æ–±–∞–≤—å 1-2 —Å–≤–æ–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –≤ –¥—É—Ö–µ ¬´—ç—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–æ—Ä—ã–≤¬ª, ¬´–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –≤ —à–æ–∫–µ¬ª, ¬´–∂–¥–∞–ª–∏ –≥–æ–¥–∞–º–∏¬ª
- –î–ª–∏–Ω–∞: 600-850 —Å–∏–º–≤–æ–ª–æ–≤
- –ó–∞–∫–æ–Ω—á–∏ –≤–æ–ø—Ä–æ—Å–æ–º –∏–ª–∏ –ø—Ä–∏–∑—ã–≤–æ–º –∫ –æ–±—Å—É–∂–¥–µ–Ω–∏—é

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å –ù–ï –ø—Ä–æ –ò–ò, –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏–ª–∏ ML (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ —Ñ–∏–Ω–∞–Ω—Å—ã, –ø–æ–ª–∏—Ç–∏–∫—É, –∏–≥—Ä—ã) ‚Äî –æ—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û: SKIP

–ü–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏!"""

    for attempt in range(3):
        try:
            await asyncio.sleep(1)
            
            resp = await asyncio.to_thread(
                groq_client.chat.completions.create,
                model=random.choice(GROQ_MODELS),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1100,
            )
            text = resp.choices[0].message.content.strip()

            if "SKIP" in text.upper()[:50]:
                logger.info("   ‚ö†Ô∏è LLM –æ—Ç–∫–ª–æ–Ω–∏–ª–∞ —Ç–µ–º—É (SKIP)")
                return None

            topic = Topic.detect(f"{article.title} {article.summary}")
            hashtags = Topic.HASHTAGS.get(topic, Topic.HASHTAGS[Topic.GENERAL])

            cta = "\n\nüî• ‚Äî –æ–≥–æ–Ω—å! | üóø ‚Äî –Ω—É —Ç–∞–∫–æ–µ | ‚ö° ‚Äî –ø—Ä–∏–∫–æ–ª—å–Ω–æ"
            source = f'\n\nüîó <a href="{article.link}">–û—Ä–∏–≥–∏–Ω–∞–ª</a>'
            final = text + cta + "\n\n" + hashtags + source

            if len(final) > config.caption_limit:
                overflow = len(final) - config.caption_limit + 50
                text = text[:-overflow]
                for punct in ['.', '!', '?']:
                    last = text.rfind(punct)
                    if last > len(text) // 2:
                        text = text[:last + 1]
                        break
                final = text + cta + "\n\n" + hashtags + source

            return final
            
        except Exception as e:
            logger.error(f"   ‚ùå Groq –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/3): {e}")
            await asyncio.sleep(3)

    return None

# ====================== IMAGE (–° –õ–û–ì–ê–ú–ò!) ======================
async def generate_image(title: str) -> Optional[str]:
    logger.info("   üé® –ù–∞—á–∏–Ω–∞—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
    
    clean_title = re.sub(r'[^\w\s]', '', title)[:60]
    prompt = f"minimalist futuristic AI technology illustration, {clean_title}, dark background, neon glow, cyberpunk aesthetic, 4k quality"
    url = f"https://image.pollinations.ai/prompt/{quote(prompt)}?width=1024&height=1024&nologo=true&enhance=true&seed={random.randint(1,999999)}"
    
    logger.info(f"   üì° URL: {url[:100]}...")

    for attempt in range(3):
        try:
            logger.info(f"   üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3...")
            
            timeout = aiohttp.ClientTimeout(total=45)
            async with aiohttp.ClientSession(timeout=timeout) as sess:
                async with sess.get(url) as resp:
                    logger.info(f"   üìä HTTP Status: {resp.status}")
                    
                    if resp.status != 200:
                        logger.warning(f"   ‚ö†Ô∏è –ü–ª–æ—Ö–æ–π —Å—Ç–∞—Ç—É—Å: {resp.status}")
                        await asyncio.sleep(3)
                        continue
                    
                    content = await resp.read()
                    size = len(content)
                    logger.info(f"   üíæ –†–∞–∑–º–µ—Ä: {size} –±–∞–π—Ç")
                    
                    if size < 10000:
                        logger.warning(f"   ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª: {size} –±–∞–π—Ç")
                        await asyncio.sleep(3)
                        continue
                    
                    fname = f"img_{int(datetime.now().timestamp())}_{random.randint(1000,9999)}.jpg"
                    with open(fname, "wb") as f:
                        f.write(content)
                    
                    logger.info(f"   ‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {fname}")
                    return fname
                    
        except asyncio.TimeoutError:
            logger.warning(f"   ‚è±Ô∏è Timeout –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}")
            await asyncio.sleep(5)
        except Exception as e:
            logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {type(e).__name__}: {e}")
            await asyncio.sleep(3)
    
    logger.warning("   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫")
    return None

# ====================== POST ======================
async def post_article(article: Article, text: str, posted: PostedManager) -> bool:
    img = await generate_image(article.title)
    
    try:
        if img and os.path.exists(img):
            logger.info(f"   üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º...")
            await bot.send_photo(config.channel_id, FSInputFile(img), caption=text)
            os.remove(img)
            logger.info(f"   üóëÔ∏è –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª—ë–Ω")
        else:
            logger.info(f"   üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –ë–ï–ó –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ç–µ–∫—Å—Ç only)")
            await bot.send_message(config.channel_id, text, disable_web_page_preview=False)

        posted.add(article.link, article.title)
        logger.info(f"‚úÖ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û: {article.title[:60]}...")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
        if img and os.path.exists(img):
            try:
                os.remove(img)
            except:
                pass
        return False

# ====================== MAIN ======================
async def autopost():
    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –ê–í–¢–û–ü–û–°–¢–ï–†–ê (Western AI News ‚Üí RU)")
    logger.info("=" * 60)

    posted = PostedManager(config.posted_file)
    posted.cleanup(config.retention_days)

    raw = await load_all_feeds(posted)
    if not raw:
        logger.info("‚ùå –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return

    candidates = filter_articles(raw)
    if not candidates:
        logger.info("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        return

    for i, article in enumerate(candidates[:10], 1):
        logger.info(f"\n[{i}/{min(10, len(candidates))}] –ü–æ–ø—ã—Ç–∫–∞: {article.source}")
        
        summary = await generate_summary(article)
        if not summary:
            logger.info("   ‚è© –ü—Ä–æ–ø—É—Å–∫, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é...")
            continue

        if await post_article(article, summary, posted):
            logger.info("\n‚ú® –ü–æ—Å—Ç —É—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω! –ó–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É.")
            break
        
        await asyncio.sleep(3)
    else:
        logger.warning("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ç–æ–ø-10")

    logger.info("=" * 60)
    logger.info("üèÅ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    logger.info("=" * 60)

async def main():
    try:
        await autopost()
    except KeyboardInterrupt:
        logger.info("\n‚õî –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    finally:
        await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())





























































































































































































































































