import os
import json
import asyncio
import random
import re
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional

import requests
import feedparser
import urllib.parse
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from openai import OpenAI

# ============ CONFIG ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

if not all([OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    raise ValueError("‚ùå –ù–µ –≤—Å–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

POSTED_FILE = "posted_articles.json"
FAILED_FILE = "failed_attempts.json"
RETENTION_DAYS = 30  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 7 –¥–æ 30 –¥–Ω–µ–π
LAST_CATEGORY_FILE = "last_category.json"
LAST_SECURITY_FILE = "last_security_post.json"

MAX_ARTICLE_AGE_DAYS = 3
TELEGRAM_CAPTION_LIMIT = 1024

# ============ –ö–ê–¢–ï–ì–û–†–ò–ò –ò–°–¢–û–ß–ù–ò–ö–û–í ============

SOURCE_CATEGORIES = {
    "ai": ["Habr AI", "Habr ML", "Habr Neural", "Habr NLP", "Reuters AI", "Futurism AI"],
    "tech_ru": ["CNews", "ComNews", "3DNews", "iXBT", "Habr News"],
    "robotics": ["Habr Robotics"],
    "security": ["SecurityNews", "CyberAlerts"],
}

CATEGORY_ROTATION = ["ai", "tech_ru", "ai", "robotics", "ai", "tech_ru", "security"]

# ============ –°–¢–ò–õ–ò –ü–û–°–¢–û–í ============

POST_STYLES = [
    {
        "name": "–≤–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–π_–≥–∏–∫",
        "intro": "–¢—ã –≤–µ–¥—ë—à—å –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∫–∞–Ω–∞–ª –ø—Ä–æ –ò–ò –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –†–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—à—å –æ –Ω–æ–≤–∏–Ω–∫–∞—Ö —Å —ç–Ω—Ç—É–∑–∏–∞–∑–º–æ–º, –Ω–æ –ø–æ –¥–µ–ª—É.",
        "tone": "–≠–Ω–µ—Ä–≥–∏—á–Ω—ã–π, –∂–∏–≤–æ–π. –§–∞–∫—Ç—ã –ø–æ–¥–∞—ë—à—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –Ω–æ –±–µ–∑ –ø—Ä–µ—É–≤–µ–ª–∏—á–µ–Ω–∏–π.",
        "emojis": "üî•üöÄüí°ü§ñ‚ú®"
    },
    {
        "name": "–∞–Ω–∞–ª–∏—Ç–∏–∫",
        "intro": "–¢—ã ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ–±–æ–∑—Ä–µ–≤–∞—Ç–µ–ª—å. –†–∞–∑–±–∏—Ä–∞–µ—à—å –Ω–æ–≤–æ—Å—Ç–∏, –æ–±—ä—è—Å–Ω—è–µ—à—å —Å—É—Ç—å –∏ –ø–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ.",
        "tone": "–°–ø–æ–∫–æ–π–Ω—ã–π, –≤–¥—É–º—á–∏–≤—ã–π. –î–∞—ë—à—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤—ã–≤–æ–¥—ã.",
        "emojis": "üß†üìäüî¨üíª‚ö°"
    },
    {
        "name": "–∏—Ä–æ–Ω–∏—á–Ω—ã–π_–æ–±–æ–∑—Ä–µ–≤–∞—Ç–µ–ª—å",
        "intro": "–¢—ã ‚Äî –æ–±–æ–∑—Ä–µ–≤–∞—Ç–µ–ª—å —Å —á—É–≤—Å—Ç–≤–æ–º —é–º–æ—Ä–∞. –ü–æ–¥–º–µ—á–∞–µ—à—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ, –∏–Ω–æ–≥–¥–∞ —Å –ª—ë–≥–∫–æ–π –∏—Ä–æ–Ω–∏–µ–π.",
        "tone": "–ñ–∏–≤–æ–π, –º–µ—Å—Ç–∞–º–∏ –∏—Ä–æ–Ω–∏—á–Ω—ã–π, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π.",
        "emojis": "üëÄüéØüòèüõ†Ô∏èüí´"
    },
    {
        "name": "–ø—Ä–∞–∫—Ç–∏–∫",
        "intro": "–¢—ã ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç. –û–±—ä—è—Å–Ω—è–µ—à—å, —á—Ç–æ —Å–¥–µ–ª–∞–ª–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –∫–æ–º—É –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è.",
        "tone": "–î–µ–ª–æ–≤–æ–π, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã.",
        "emojis": "‚öôÔ∏è‚úÖüì±üîßüí™"
    },
    {
        "name": "—Ñ—É—Ç—É—Ä–∏—Å—Ç",
        "intro": "–¢—ã ‚Äî —ç–Ω—Ç—É–∑–∏–∞—Å—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π. –ü–æ–∫–∞–∑—ã–≤–∞–µ—à—å, –∫–∞–∫ –Ω–æ–≤–∏–Ω–∫–∏ –º–µ–Ω—è—é—Ç –º–∏—Ä.",
        "tone": "–í–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–π, –Ω–æ –æ–ø–∏—Ä–∞—é—â–∏–π—Å—è –Ω–∞ —Ñ–∞–∫—Ç—ã.",
        "emojis": "üåüüîÆüöÄüåç‚ú®"
    }
]

POST_STRUCTURES = ["hook_details_conclusion", "problem_solution_impact", "news_analysis"]

# ============ –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ============

AI_KEYWORDS = [
    "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å", "–∏–∏", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
    "neural network", "artificial intelligence",
    "llm", "gpt", "gpt-4", "gpt-5", "gpt-4o", "chatgpt", "claude", "gemini",
    "copilot", "mistral", "llama", "qwen", "gigachat", "yandexgpt",
    "kandinsky", "—à–µ–¥–µ–≤—Ä—É–º", "deepseek", "grok",
    "openai", "anthropic", "deepmind", "hugging face", "stability ai",
    "stable diffusion", "midjourney", "dall-e", "sora", "runway",
    "–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
    "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "transformer", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä",
    "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π", "–¥–æ–æ–±—É—á–µ–Ω–∏–µ", "fine-tuning",
    "—á–∞—Ç-–±–æ—Ç", "–≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ",
    "nlp", "agi", "ai-–∞–≥–µ–Ω—Ç", "–±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "reasoning", "rlhf"
]

TECH_KEYWORDS = [
    "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª", "–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª", "–≤—ã–ø—É—Å—Ç–∏–ª", "—Ä–µ–ª–∏–∑", "–∑–∞–ø—É—Å—Ç–∏–ª",
    "—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "–Ω–æ—É—Ç–±—É–∫", "–≥–∞–¥–∂–µ—Ç", "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "—É–º–Ω—ã–µ —á–∞—Å—ã",
    "—Ä–æ–±–æ—Ç", "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞", "–¥—Ä–æ–Ω", "–±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç",
    "–∫–≤–∞–Ω—Ç–æ–≤—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—á–∏–ø", "gpu", "–≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞",
    "nvidia", "amd", "intel", "apple", "spacex", "starship", "–∫–æ—Å–º–æ—Å",
    "vr", "ar", "meta quest", "apple vision", "—ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª—å", "tesla",
    "–≥–æ—Å–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è", "–º–∏–∫—Ä–æ—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞", "–ø–æ–ª—É–ø—Ä–æ–≤–æ–¥–Ω–∏–∫–∏", "–∏–º–ø–æ—Ä—Ç–æ–∑–∞–º–µ—â–µ–Ω–∏–µ",
    "–±–∞–π–∫–∞–ª", "—ç–ª—å–±—Ä—É—Å", "–º—Ç—Å", "–±–∏–ª–∞–π–Ω", "–º–µ–≥–∞—Ñ–æ–Ω", "—Ä–æ—Å—Ç–µ–ª–µ–∫–æ–º",
    "5g", "lte", "—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "vpn", "—è–Ω–¥–µ–∫—Å", "—Å–±–µ—Ä", "vk",
]

SENSATIONAL_KEYWORDS = [
    "–≤–∑–ª–æ–º", "–≤–∑–ª–æ–º–∞–ª–∏", "—É—Ç–µ—á–∫–∞", "—É—Ç–µ–∫–ª–∏ –¥–∞–Ω–Ω—ã–µ", "data leak",
    "ransomware", "–≤—ã–∫—É–ø", "—à–∏—Ñ—Ä–æ–≤–∞–ª—å—â–∏–∫", "–∞—Ç–∞–∫–∞", "–∫–∏–±–µ—Ä–∞—Ç–∞–∫–∞",
    "ddos", "—Ñ–∏—à–∏–Ω–≥", "—ç–∫—Å–ø–ª–æ–π—Ç", "—É—è–∑–≤–∏–º–æ—Å—Ç—å", "0-day", "–Ω—É–ª–µ–≤–æ–≥–æ –¥–Ω—è",
    "breach", "leak", "data breach", "hack", "hacked", "vulnerability",
]

# === –§–ò–õ–¨–¢–†–´ –î–õ–Ø –ò–°–•–û–î–ù–´–• –ù–û–í–û–°–¢–ï–ô ===

EXCLUDE_KEYWORDS = [
    "–∞–∫—Ü–∏–∏", "–±–∏—Ä–∂–∞", "–∫–æ—Ç–∏—Ä–æ–≤–∫–∏", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–¥–∏–≤–∏–¥–µ–Ω–¥—ã", "ipo",
    "–≤—ã—Ä—É—á–∫–∞", "–ø—Ä–∏–±—ã–ª—å", "—É–±—ã—Ç–æ–∫", "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á—ë—Ç",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "–∫—É—Ä—Å –µ–≤—Ä–æ", "—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞",
    "–≤–µ–Ω—á—É—Ä–Ω—ã–π", "—Å–ª–∏—è–Ω–∏–µ", "–ø–æ–≥–ª–æ—â–µ–Ω–∏–µ", "–ª–∏—Å—Ç–∏–Ω–≥",
    "–Ω–∞–∑–Ω–∞—á–µ–Ω", "–æ—Ç—Å—Ç–∞–≤–∫–∞", "—É–≤–æ–ª–µ–Ω", "ceo", "—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —à—Ç–∞—Ç–∞",
    "—Ç–µ–Ω–Ω–∏—Å", "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "–±–∞—Å–∫–µ—Ç–±–æ–ª", "—Å–ø–æ—Ä—Ç", "–º–∞—Ç—á",
    "–æ–ª–∏–º–ø–∏–∞–¥–∞", "—á–µ–º–ø–∏–æ–Ω–∞—Ç", "—Ç—É—Ä–Ω–∏—Ä",
    "playstation", "xbox", "steam", "nintendo", "–≤–∏–¥–µ–æ–∏–≥—Ä–∞",
    "–∫–∏–Ω–æ", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "–º—É–∑—ã–∫–∞", "–∫–æ–Ω—Ü–µ—Ä—Ç", "–∞–∫—Ç—ë—Ä",
    "netflix", "–≤—ã–±–æ—Ä—ã", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–ø–∞—Ä–ª–∞–º–µ–Ω—Ç", "–ø–æ–ª–∏—Ç–∏–∫",
    "—Å–∞–Ω–∫—Ü–∏–∏", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–º–∏–Ω–∏—Å—Ç—Ä", "–∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç",
    "covid", "–ø–∞–Ω–¥–µ–º–∏—è", "–≤–∞–∫—Ü–∏–Ω–∞",
    "bitcoin", "–±–∏—Ç–∫–æ–∏–Ω", "ethereum", "nft", "–±–ª–æ–∫—á–µ–π–Ω", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞",
    "—Å—É–¥", "—Å—É–¥–µ–±–Ω—ã–π", "–∞—Ä–µ—Å—Ç", "–ø—Ä–∏–≥–æ–≤–æ—Ä", "—à—Ç—Ä–∞—Ñ", "–∏—Å–∫"
]

# –†–µ–∫–ª–∞–º–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –ò–°–•–û–î–ù–û–ô –Ω–æ–≤–æ—Å—Ç–∏ (—Ñ–∏–ª—å—Ç—Ä—É–µ–º –î–û –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
SOURCE_PROMO_PATTERNS = [
    r"–∫—É–ø–∏(—Ç–µ)?[\s\.,!]", r"–∑–∞–∫–∞–∂–∏(—Ç–µ)?[\s\.,!]", r"–æ—Ñ–æ—Ä–º–∏(—Ç–µ)?[\s\.,!]",
    r"—Å–∫–∞—á–∞–π(—Ç–µ)?[\s\.,!]", r"–ø–æ–ø—Ä–æ–±—É–π(—Ç–µ)?[\s\.,!]",
    r"—Å–∫–∏–¥–∫[–∞–∏]", r"–ø—Ä–æ–º–æ–∫–æ–¥", r"–∞–∫—Ü–∏—è\b", r"—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞",
    r"–±–µ—Å–ø–ª–∞—Ç–Ω(–æ|—ã–π|–∞—è)", r"–≤ –ø–æ–¥–∞—Ä–æ–∫", r"–≤—ã–≥–æ–¥(–∞|–Ω–æ)",
    r"\d+%\s*(off|—Å–∫–∏–¥–∫)", r"—Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è", r"—Ç–æ–ª—å–∫–æ —Å–µ–π—á–∞—Å",
    r"–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω(–æ–µ|–∞—è)\s+(–≤—Ä–µ–º—è|–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)",
    r"—É—Å–ø–µ–π(—Ç–µ)?[\s\.,!]", r"–Ω–µ —É–ø—É—Å—Ç–∏(—Ç–µ)?", r"–Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏(—Ç–µ)?",
    r"–ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å", r"–ª—É—á—à(–∞—è|–µ–µ)\s+—Ü–µ–Ω–∞",
    r"–ø—Ä–µ–¥–∑–∞–∫–∞–∑", r"pre-?order", r"—Å—Ç–∞—Ä—Ç –ø—Ä–æ–¥–∞–∂",
    r"–≥–¥–µ –∫—É–ø–∏—Ç—å", r"—Ü–µ–Ω–∞ –æ—Ç", r"—Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—Ç",
    r"—Ä—É–±–ª(–µ–π|—å)", r"\$\d+", r"‚Ç¨\d+", r"‚ÇΩ\d+",
]

def is_source_promotional(title: str, summary: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ò–°–•–û–î–ù–£–Æ –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ —Ä–µ–∫–ª–∞–º—É –î–û –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    text = f"{title} {summary}".lower()
    
    for pattern in SOURCE_PROMO_PATTERNS:
        if re.search(pattern, text):
            return True
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
    promo_indicators = [
        "–æ–±—ä—è–≤–∏–ª–∞ —Ü–µ–Ω—É", "–Ω–∞–∑–≤–∞–ª–∞ —Ü–µ–Ω—É", "—Å—Ç–∞—Ä—Ç—É—é—Ç –ø—Ä–æ–¥–∞–∂–∏",
        "–æ—Ç–∫—Ä—ã—Ç –ø—Ä–µ–¥–∑–∞–∫–∞–∑", "–º–æ–∂–Ω–æ –∫—É–ø–∏—Ç—å", "–ø–æ—Å—Ç—É–ø–∏–ª –≤ –ø—Ä–æ–¥–∞–∂—É",
        "–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∑–∞–∫–∞–∑–∞", "–ø–æ—è–≤–∏–ª—Å—è –≤ –ø—Ä–æ–¥–∞–∂–µ",
    ]
    
    for indicator in promo_indicators:
        if indicator in text:
            return True
    
    return False

# ============ STATE ============

posted_articles: Dict[str, Dict] = {}
failed_attempts: Dict[str, int] = {}

# ============ –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ò ============

def get_content_hash(title: str, summary: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ, –∞ –Ω–µ URL"""
    content = f"{title.lower().strip()} {summary.lower().strip()}"
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    content = re.sub(r'[^\w\s]', '', content)
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def is_already_posted(link: str, title: str, summary: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL –ò —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ URL
    if link in posted_articles:
        return True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
    content_hash = get_content_hash(title, summary)
    for info in posted_articles.values():
        if info.get("content_hash") == content_hash:
            return True
    
    return False

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        try:
            posted_data = json.load(f)
            posted_articles = {
                item["id"]: {
                    "timestamp": item.get("timestamp"),
                    "content_hash": item.get("content_hash", "")
                } 
                for item in posted_data
            }
        except Exception:
            posted_articles = {}

if os.path.exists(FAILED_FILE):
    with open(FAILED_FILE, "r", encoding="utf-8") as f:
        try:
            failed_attempts = json.load(f)
        except Exception:
            failed_attempts = {}

def save_posted_articles() -> None:
    data = [
        {
            "id": id_str, 
            "timestamp": info["timestamp"],
            "content_hash": info["content_hash"]
        } 
        for id_str, info in posted_articles.items()
    ]
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def save_failed_attempts() -> None:
    with open(FAILED_FILE, "w", encoding="utf-8") as f:
        json.dump(failed_attempts, f, ensure_ascii=False, indent=2)

def clean_old_posts() -> None:
    global posted_articles
    now = datetime.now().timestamp()
    cutoff = now - (RETENTION_DAYS * 86400)
    posted_articles = {
        id_str: info for id_str, info in posted_articles.items()
        if info.get("timestamp") is None or info.get("timestamp") > cutoff
    }
    save_posted_articles()

def save_posted(article_id: str, title: str, summary: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç—å—é —Å —Ö–µ—à–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
    posted_articles[article_id] = {
        "timestamp": datetime.now().timestamp(),
        "content_hash": get_content_hash(title, summary)
    }
    save_posted_articles()

def mark_as_failed(article_id: str) -> None:
    """–ü–æ–º–µ—á–∞–µ—Ç —Å—Ç–∞—Ç—å—é –∫–∞–∫ –Ω–µ—É–¥–∞—á–Ω—É—é, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–Ω–æ–≤–∞"""
    failed_attempts[article_id] = failed_attempts.get(article_id, 0) + 1
    save_failed_attempts()

# ============ CATEGORY ROTATION ============

def load_last_category() -> Dict:
    if not os.path.exists(LAST_CATEGORY_FILE):
        return {"category": None, "index": 0}
    try:
        with open(LAST_CATEGORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {"category": None, "index": 0}

def save_last_category(category: str, index: int) -> None:
    try:
        with open(LAST_CATEGORY_FILE, "w", encoding="utf-8") as f:
            json.dump({"category": category, "index": index}, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def get_next_category() -> tuple:
    data = load_last_category()
    last_index = data.get("index", 0)
    next_index = (last_index + 1) % len(CATEGORY_ROTATION)
    return CATEGORY_ROTATION[next_index], next_index

def load_last_security_ts() -> Optional[float]:
    if not os.path.exists(LAST_SECURITY_FILE):
        return None
    try:
        with open(LAST_SECURITY_FILE, "r", encoding="utf-8") as f:
            return json.load(f).get("ts")
    except Exception:
        return None

def save_last_security_ts() -> None:
    try:
        with open(LAST_SECURITY_FILE, "w", encoding="utf-8") as f:
            json.dump({"ts": datetime.now().timestamp()}, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

# ============ HELPERS ============

def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())

def get_article_category(source: str) -> str:
    for category, sources in SOURCE_CATEGORIES.items():
        if source in sources:
            return category
    return "tech_ru"

def detect_topic(title: str, summary: str) -> str:
    text = f"{title} {summary}".lower()

    if any(kw in text for kw in ["gpt", "chatgpt", "claude", "llm", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å"]):
        return "llm"
    elif any(kw in text for kw in ["midjourney", "dall-e", "stable diffusion"]):
        return "image_gen"
    elif any(kw in text for kw in ["—Ä–æ–±–æ—Ç", "robot", "–∞–≤—Ç–æ–Ω–æ–º–Ω"]):
        return "robotics"
    elif any(kw in text for kw in ["spacex", "–∫–æ—Å–º–æ—Å", "—Ä–∞–∫–µ—Ç–∞", "—Å–ø—É—Ç–Ω–∏–∫"]):
        return "space"
    elif any(kw in text for kw in ["nvidia", "gpu", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—á–∏–ø"]):
        return "hardware"
    elif any(kw in text for kw in ["–Ω–µ–π—Ä–æ—Å–µ—Ç", "neural", "ai", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]):
        return "ai"
    elif any(kw in text for kw in ["–æ–ø–µ—Ä–∞—Ç–æ—Ä", "—Ç–∞—Ä–∏—Ñ", "—Ç–µ–ª–µ–∫–æ–º", "–º—Ç—Å", "–±–∏–ª–∞–π–Ω"]):
        return "telecom"
    elif any(kw in text for kw in ["–≥–æ—Å–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è", "–∏–º–ø–æ—Ä—Ç–æ–∑–∞–º–µ—â–µ–Ω–∏–µ"]):
        return "ru_tech"
    else:
        return "tech"

def get_hashtags(topic: str) -> str:
    hashtag_map = {
        "llm": "#ChatGPT #LLM #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "image_gen": "#AI #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "robotics": "#—Ä–æ–±–æ—Ç—ã #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–±—É–¥—É—â–µ–µ",
        "space": "#–∫–æ—Å–º–æ—Å #SpaceX #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "hardware": "#–∂–µ–ª–µ–∑–æ #GPU #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "ai": "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "tech": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–Ω–æ–≤–∏–Ω–∫–∏ #–≥–∞–¥–∂–µ—Ç—ã",
        "telecom": "#—Ç–µ–ª–µ–∫–æ–º #—Å–≤—è–∑—å #–æ–ø–µ—Ä–∞—Ç–æ—Ä—ã",
        "ru_tech": "#–∏–º–ø–æ—Ä—Ç–æ–∑–∞–º–µ—â–µ–Ω–∏–µ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–†–æ—Å—Å–∏—è",
        "sensational": "#–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å #–≤–∑–ª–æ–º #—É—Ç–µ—á–∫–∞"
    }
    return hashtag_map.get(topic, "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–Ω–æ–≤–æ—Å—Ç–∏")

def force_complete_sentence(text: str) -> str:
    text = text.strip()
    if not text:
        return text
    
    incomplete = [
        r'\s+–∏$', r'\s+–∞$', r'\s+–Ω–æ$', r'\s+–∏–ª–∏$', r'\s+—á—Ç–æ$', r'\s+–∫–∞–∫$',
        r'\s+–¥–ª—è$', r'\s+–Ω–∞$', r'\s+–≤$', r'\s+—Å$', r'\s+–∫$', r'\s+–ø–æ$',
        r'\s+–∫–æ—Ç–æ—Ä—ã–π$', r'\s+–∫–æ—Ç–æ—Ä–∞—è$', r'\s+–∫–æ—Ç–æ—Ä–æ–µ$', r'\s+—ç—Ç–æ$',
        r'\s+‚Äî$', r'\s+-$', r':$', r';$', r',$',
    ]
    
    for pattern in incomplete:
        text = re.sub(pattern, '', text)
    
    text = text.strip()
    
    if text and text[-1] in '.!?':
        return text
    
    last_end = max(text.rfind('.'), text.rfind('!'), text.rfind('?'))
    
    if last_end > len(text) * 0.6:
        return text[:last_end + 1]
    
    return text + '.'

def trim_to_limit(text: str, max_length: int) -> str:
    text = text.strip()
    if len(text) <= max_length:
        return force_complete_sentence(text)
    
    sentences = re.split(r'(?<=[.!?])\s+', text)
    result = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        candidate = (result + " " + sentence).strip() if result else sentence
        if len(candidate) <= max_length:
            result = candidate
        else:
            break
    
    if not result and sentences:
        result = sentences[0][:max_length]
        if ' ' in result:
            result = result.rsplit(' ', 1)[0]
    
    return force_complete_sentence(result)

def build_final_post(core_text: str, hashtags: str, link: str) -> str:
    cta = "\n\nüëç ‚Äî –ø–æ–ª–µ–∑–Ω–æ | üëé ‚Äî –º–∏–º–æ | üî• ‚Äî –æ–≥–æ–Ω—å"
    source = f'\n\nüîó <a href="{link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
    tags = f"\n\n{hashtags}"
    
    service_len = len(cta) + len(source) + len(tags)
    max_core = TELEGRAM_CAPTION_LIMIT - service_len - 5
    
    trimmed = trim_to_limit(core_text, max_core)
    
    return trimmed + cta + tags + source

# ============ PARSERS ============

def load_rss(url: str, source: str) -> List[Dict]:
    articles = []
    try:
        feed = feedparser.parse(url)
        if feed.bozo and not feed.entries:
            print(f"‚ö†Ô∏è RSS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {source}")
            return articles
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ RSS {source}: {e}")
        return articles

    now = datetime.now()
    max_age = timedelta(days=MAX_ARTICLE_AGE_DAYS)

    for entry in feed.entries[:50]:
        link = entry.get("link", "")
        title = clean_text(entry.get("title") or "")
        summary = clean_text(entry.get("summary") or entry.get("description") or "")[:1000]
        
        if not link:
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        if is_already_posted(link, title, summary):
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ—É–¥–∞—á–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
        if link in failed_attempts and failed_attempts[link] >= 3:
            continue

        pub_dt = now
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            pub_dt = datetime(*entry.published_parsed[:6])
        elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
            pub_dt = datetime(*entry.updated_parsed[:6])

        if now - pub_dt > max_age:
            continue

        articles.append({
            "id": link,
            "title": title,
            "summary": summary,
            "link": link,
            "source": source,
            "published_parsed": pub_dt,
            "category": get_article_category(source)
        })

    if articles:
        print(f"‚úÖ {source}: {len(articles)} —Å—Ç–∞—Ç–µ–π")

    return articles

def load_articles_from_sites() -> List[Dict]:
    articles: List[Dict] = []

    articles.extend(load_rss("https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru", "Habr AI"))
    articles.extend(load_rss("https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru", "Habr ML"))
    articles.extend(load_rss("https://habr.com/ru/rss/hub/neural_networks/all/?fl=ru", "Habr Neural"))
    articles.extend(load_rss("https://habr.com/ru/rss/hub/natural_language_processing/all/?fl=ru", "Habr NLP"))
    articles.extend(load_rss("https://habr.com/ru/rss/hub/robotics/all/?fl=ru", "Habr Robotics"))
    articles.extend(load_rss("https://habr.com/ru/rss/news/?fl=ru", "Habr News"))

    articles.extend(load_rss("https://www.cnews.ru/inc/rss/news.xml", "CNews"))
    articles.extend(load_rss("https://3dnews.ru/news/rss/", "3DNews"))
    articles.extend(load_rss("https://www.ixbt.com/export/news.rss", "iXBT"))
    articles.extend(load_rss("https://www.comnews.ru/rss", "ComNews"))

    articles.extend(load_rss("https://secnews.ru/rss/", "SecurityNews"))
    articles.extend(load_rss("https://cyberalerts.io/rss/latest-public", "CyberAlerts"))

    articles.extend(load_rss("https://www.reuters.com/technology/artificial-intelligence/rss", "Reuters AI"))
    articles.extend(load_rss("https://futurism.com/categories/ai-artificial-intelligence/feed", "Futurism AI"))

    return articles

def filter_articles(articles: List[Dict]) -> Dict[str, List[Dict]]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏: —É–±–∏—Ä–∞–µ—Ç —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –î–û –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    
    categorized = {
        "ai": [],
        "tech_ru": [],
        "robotics": [],
        "security": [],
        "sensational": []
    }
    
    skipped_promo = 0
    skipped_excluded = 0

    for e in articles:
        title = e['title']
        summary = e['summary']
        text = f"{title} {summary}".lower()

        # 1. –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–ø–æ–ª–∏—Ç–∏–∫–∞, —Ñ–∏–Ω–∞–Ω—Å—ã –∏ —Ç.–¥.)
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            skipped_excluded += 1
            continue
        
        # 2. –ò—Å–∫–ª—é—á–∞–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        if is_source_promotional(title, summary):
            skipped_promo += 1
            print(f"  üö´ –†–µ–∫–ª–∞–º–∞: {title[:50]}...")
            continue

        # 3. –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
        if any(kw in text for kw in SENSATIONAL_KEYWORDS):
            categorized["sensational"].append(e)
            continue

        category = e.get("category", "tech_ru")
        
        if any(kw in text for kw in AI_KEYWORDS):
            category = "ai"
        
        if category in categorized:
            categorized[category].append(e)
        else:
            categorized["tech_ru"].append(e)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
    for cat in categorized:
        categorized[cat].sort(key=lambda x: x["published_parsed"], reverse=True)
        print(f"üìÇ {cat}: {len(categorized[cat])} —Å—Ç–∞—Ç–µ–π")
    
    print(f"üö´ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {skipped_promo} —Ä–µ–∫–ª–∞–º–Ω—ã—Ö, {skipped_excluded} –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö")

    return categorized

# ============ –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê ============

def build_prompt(title: str, summary: str, style: dict, structure: str) -> str:
    
    structures = {
        "hook_details_conclusion": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. –ó–ê–•–í–ê–¢ ‚Äî –∏–Ω—Ç—Ä–∏–≥—É—é—â–µ–µ –Ω–∞—á–∞–ª–æ, —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏
2. –ü–û–î–†–û–ë–ù–û–°–¢–ò ‚Äî –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã, —Ü–∏—Ñ—Ä—ã, –¥–µ—Ç–∞–ª–∏  
3. –í–´–í–û–î ‚Äî –ø–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ, —á—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç
""",
        "problem_solution_impact": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. –ö–û–ù–¢–ï–ö–°–¢ ‚Äî —á—Ç–æ –±—ã–ª–æ –¥–æ —ç—Ç–æ–≥–æ, –∫–∞–∫—É—é –∑–∞–¥–∞—á—É —Ä–µ—à–∞–ª–∏
2. –†–ï–®–ï–ù–ò–ï ‚Äî —á—Ç–æ —Å–¥–µ–ª–∞–ª–∏, –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
3. –ó–ù–ê–ß–ï–ù–ò–ï ‚Äî –∫–∞–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç, —á—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—Å—è
""",
        "news_analysis": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. –ù–û–í–û–°–¢–¨ ‚Äî —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ
2. –î–ï–¢–ê–õ–ò ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏
3. –ü–ï–†–°–ü–ï–ö–¢–ò–í–ê ‚Äî —á—Ç–æ –¥–∞–ª—å—à–µ
"""
    }

    return f"""
{style['intro']}
–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {style['tone']}

–ù–û–í–û–°–¢–¨:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {summary}

{structures.get(structure, structures['news_analysis'])}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
‚Ä¢ –ù–∞–ø–∏—à–∏ –ø–æ—Å—Ç –Ω–∞ 600-800 —Å–∏–º–≤–æ–ª–æ–≤
‚Ä¢ –ú–∞–∫—Å–∏–º—É–º 2-3 —ç–º–æ–¥–∑–∏ –∏–∑ —ç—Ç–∏—Ö: {style['emojis']}
‚Ä¢ –ü–∏—à–∏ –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º, –∫–∞–∫ –¥–ª—è –¥—Ä—É–∑–µ–π-—Ç–µ—Ö–Ω–∞—Ä–µ–π
‚Ä¢ –î–æ–±–∞–≤—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
‚Ä¢ –ó–∞–∫–æ–Ω—á–∏ –≤—ã–≤–æ–¥–æ–º –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–æ–º
‚Ä¢ –¢–µ–∫—Å—Ç –î–û–õ–ñ–ï–ù –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è –Ω–∞ . –∏–ª–∏ ! –∏–ª–∏ ?
‚Ä¢ –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –≤ –∏–∑–ª–æ–∂–µ–Ω–∏–∏

–ó–ê–ü–†–ï–©–ï–ù–û:
‚Ä¢ –õ—é–±—ã–µ –ø—Ä–∏–∑—ã–≤—ã –∫ –ø–æ–∫—É–ø–∫–µ, –∑–∞–∫–∞–∑—É, —Å–∫–∞—á–∏–≤–∞–Ω–∏—é
‚Ä¢ –°–ª–æ–≤–∞: —Å–∫–∏–¥–∫–∞, –∞–∫—Ü–∏—è, –±–µ—Å–ø–ª–∞—Ç–Ω–æ, –∫—É–ø–∏—Ç—å, –∑–∞–∫–∞–∑–∞—Ç—å
‚Ä¢ –¶–µ–Ω—ã –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å
‚Ä¢ –†–µ–∫–ª–∞–º–Ω—ã–π —Ç–æ–Ω
‚Ä¢ –°—Å—ã–ª–∫–∏ –∏ —Ö–µ—à—Ç–µ–≥–∏
‚Ä¢ –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Ñ—Ä–∞–∑ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä

–ù–∞–ø–∏—à–∏ –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞:
"""


def generate_post_text(title: str, summary: str, link: str) -> Optional[str]:
    style = random.choice(POST_STYLES)
    structure = random.choice(POST_STRUCTURES)
    
    print(f"  üìù –°—Ç–∏–ª—å: {style['name']}")
    
    prompt = build_prompt(title, summary, style, structure)
    
    for attempt in range(3):
        try:
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "–¢—ã ‚Äî –∞–≤—Ç–æ—Ä Telegram-–∫–∞–Ω–∞–ª–∞ –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö. "
                            "–ü–∏—à–µ—à—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ, –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ—Å—Ç—ã –±–µ–∑ —Ä–µ–∫–ª–∞–º—ã. "
                            "–í—Å–µ–≥–¥–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—à—å —Ç–µ–∫—Å—Ç –ø–æ–ª–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º. "
                            "–ö–∞–∂–¥—ã–π –ø–æ—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∏ –Ω–µ –ø–æ—Ö–æ–∂–∏–º –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ."
                        )
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.9,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                max_tokens=700,
                frequency_penalty=0.3,  # –®—Ç—Ä–∞—Ñ –∑–∞ —á–∞—Å—Ç—ã–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                presence_penalty=0.6,   # –®—Ç—Ä–∞—Ñ –∑–∞ –ª—é–±—ã–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
            )
            
            text = response.choices[0].message.content.strip()
            
            if (text.startswith('"') and text.endswith('"')) or \
               (text.startswith('¬´') and text.endswith('¬ª')):
                text = text[1:-1].strip()
            
            text = force_complete_sentence(text)
            
            if len(text) < 150:
                print(f"  ‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç")
                continue
            
            topic = detect_topic(title, summary)
            if any(kw in (title + summary).lower() for kw in SENSATIONAL_KEYWORDS):
                topic = "sensational"
            
            hashtags = get_hashtags(topic)
            final_post = build_final_post(text, hashtags, link)
            
            print(f"  ‚úÖ –ì–æ—Ç–æ–≤–æ: {len(final_post)} —Å–∏–º–≤–æ–ª–æ–≤")
            return final_post
            
        except Exception as e:
            print(f"  ‚ùå OpenAI –æ—à–∏–±–∫–∞: {e}")
            time.sleep(2)
    
    return None

# ============ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–ê–†–¢–ò–ù–û–ö ============

def generate_image(title: str, max_retries: int = 3) -> Optional[str]:
    styles = [
        "futuristic minimalist illustration, soft gradients",
        "abstract tech visualization, geometric shapes",
        "modern digital art, clean lines, neon accents",
        "sci-fi concept art, atmospheric lighting",
    ]
    
    style = random.choice(styles)
    
    for attempt in range(max_retries):
        seed = random.randint(0, 10**7)
        clean_title = re.sub(r'["\'\n]', ' ', title[:50])
        
        prompt = f"{style}, {clean_title}, technology, 4k, no text, no letters"
        
        try:
            encoded = urllib.parse.quote(prompt)
            url = f"https://image.pollinations.ai/prompt/{encoded}?seed={seed}&width=1024&height=1024&nologo=true"
            
            print(f"  üé® –ö–∞—Ä—Ç–∏–Ω–∫–∞ ({attempt + 1}/{max_retries})...")
            
            resp = requests.get(url, timeout=90, headers=HEADERS)
            
            if resp.status_code == 200 and 'image' in resp.headers.get('content-type', ''):
                if len(resp.content) > 10000:
                    fname = f"img_{seed}.jpg"
                    with open(fname, "wb") as f:
                        f.write(resp.content)
                    print(f"  ‚úÖ –ö–∞—Ä—Ç–∏–Ω–∫–∞ –≥–æ—Ç–æ–≤–∞")
                    return fname
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
        
        time.sleep(2)
    
    return None

def cleanup_image(filepath: Optional[str]) -> None:
    if filepath and os.path.exists(filepath):
        try:
            os.remove(filepath)
        except:
            pass

# ============ –ê–í–¢–û–ü–û–°–¢ ============

async def autopost():
    clean_old_posts()
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π...\n")
    
    articles = load_articles_from_sites()
    
    print(f"\nüìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(articles)} —Å—Ç–∞—Ç–µ–π")
    print("üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è...\n")
    
    categorized = filter_articles(articles)
    
    total = sum(len(v) for v in categorized.values())
    if total == 0:
        print("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return
    
    print(f"\n‚úÖ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {total} —Å—Ç–∞—Ç–µ–π")
    
    last_security_ts = load_last_security_ts()
    now_ts = datetime.now().timestamp()
    security_cooldown = 7 * 86400
    
    posted = False

    # 1. –°–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    for art in categorized["sensational"][:10]:
        is_security = art.get("source") in ["SecurityNews", "CyberAlerts"]
        
        if is_security and last_security_ts and (now_ts - last_security_ts) < security_cooldown:
            continue
        
        print(f"\nüö® –°–ï–ù–°–ê–¶–ò–Ø: {art['title'][:60]}...")
        
        post_text = generate_post_text(art["title"], art["summary"], art["link"])
        if not post_text:
            print("  ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é...")
            mark_as_failed(art["id"])
            continue
        
        img = generate_image(art["title"])
        
        try:
            if img:
                await bot.send_photo(CHANNEL_ID, photo=FSInputFile(img), caption=post_text)
            else:
                await bot.send_message(CHANNEL_ID, text=post_text)
            
            save_posted(art["id"], art["title"], art["summary"])
            if is_security:
                save_last_security_ts()
            
            print(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ!")
            posted = True
            break
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            mark_as_failed(art["id"])
        finally:
            cleanup_image(img)

    # 2. –û–±—ã—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ä–æ—Ç–∞—Ü–∏–∏
    if not posted:
        next_cat, next_idx = get_next_category()
        print(f"\nüîÑ –†–æ—Ç–∞—Ü–∏—è: {next_cat}")
        
        candidates = categorized.get(next_cat, [])
        
        if not candidates:
            for fallback in ["ai", "tech_ru"]:
                if categorized.get(fallback):
                    candidates = categorized[fallback]
                    next_cat = fallback
                    print(f"  ‚Ü™Ô∏è Fallback: {fallback}")
                    break
        
        # –ü—Ä–æ–±—É–µ–º –¥–æ 10 —Å—Ç–∞—Ç–µ–π –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        for art in candidates[:10]:
            print(f"\nüì∞ {art['title'][:60]}...")
            
            post_text = generate_post_text(art["title"], art["summary"], art["link"])
            if not post_text:
                print("  ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é...")
                mark_as_failed(art["id"])
                continue
            
            img = generate_image(art["title"])
            
            try:
                if img:
                    await bot.send_photo(CHANNEL_ID, photo=FSInputFile(img), caption=post_text)
                else:
                    await bot.send_message(CHANNEL_ID, text=post_text)
                
                save_posted(art["id"], art["title"], art["summary"])
                save_last_category(next_cat, next_idx)
                
                print(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ!")
                posted = True
                break
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                mark_as_failed(art["id"])
            finally:
                cleanup_image(img)

    if not posted:
        print("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –ø–æ—Å—Ç")
    else:
        print("\nüéâ –ì–æ—Ç–æ–≤–æ!")

async def main():
    try:
        await autopost()
    finally:
        await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())













































































































































































































































