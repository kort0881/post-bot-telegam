import os
import json
import asyncio
import random
import re
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urlparse
from dataclasses import dataclass, field

import aiohttp
import requests
import feedparser
import urllib.parse
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq

# ============ CONFIG ============

@dataclass
class Config:
    groq_api_key: str
    telegram_token: str
    channel_id: str
    retention_days: int = 30
    caption_limit: int = 1024
    posted_file: str = "posted_articles.json"
    
    @classmethod
    def from_env(cls) -> "Config":
        groq_key = os.getenv("GROQ_API_KEY")
        tg_token = os.getenv("TELEGRAM_BOT_TOKEN")
        channel = os.getenv("CHANNEL_ID")
        
        missing = []
        if not groq_key:
            missing.append("GROQ_API_KEY")
        if not tg_token:
            missing.append("TELEGRAM_BOT_TOKEN")
        if not channel:
            missing.append("CHANNEL_ID")
        
        if missing:
            raise SystemExit(f"‚ùå CRITICAL: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {', '.join(missing)}")
        
        return cls(
            groq_api_key=groq_key,
            telegram_token=tg_token,
            channel_id=channel,
        )

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
config = Config.from_env()

bot = Bot(
    token=config.telegram_token,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
groq_client = Groq(api_key=config.groq_api_key)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

# ============ RSS –ò–°–¢–û–ß–ù–ò–ö–ò ============

RSS_FEEDS = [
    ("https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru", "Habr AI"),
    ("https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru", "Habr ML"),
    ("https://habr.com/ru/rss/hub/neural_networks/all/?fl=ru", "Habr Neural"),
    ("https://3dnews.ru/news/rss/", "3DNews"),
    ("https://www.ixbt.com/export/news.rss", "iXBT"),
]

# ============ –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ============

AI_KEYWORDS = [
    # –û–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π",
    "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–∏–∏",
    "neural network", "artificial intelligence",
    
    # –ú–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
    "llm", "gpt", "chatgpt", "claude", "gemini",
    "copilot", "mistral", "llama", "qwen", "gigachat", "yandexgpt",
    "kandinsky", "—à–µ–¥–µ–≤—Ä—É–º", "deepseek", "grok",
    
    # –ö–æ–º–ø–∞–Ω–∏–∏
    "openai", "anthropic", "deepmind", "—Å–±–µ—Ä ai", "—è–Ω–¥–µ–∫—Å ai",
    "hugging face", "stability ai", "meta ai", "google ai",
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    "stable diffusion", "midjourney", "dall-e", "sora", "runway",
    "–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
    "text-to-image", "text-to-video",
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "transformer",
    "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π",
    "–¥–æ–æ–±—É—á–µ–Ω–∏–µ", "–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", "–¥–∞—Ç–∞—Å–µ—Ç", "fine-tuning",
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
    "—á–∞—Ç-–±–æ—Ç", "–≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
    "ai-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "—É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫",
    "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ", "–æ–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–∞", "nlp",
    
    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
    "agi", "—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "–∞–≥–µ–Ω—Ç", "ai-–∞–≥–µ–Ω—Ç", "–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ",
    "—Ç–æ–∫–µ–Ω", "–±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "reasoning",
    "–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "rlhf", "–ø—Ä–æ–º–ø—Ç", "prompt",
    "–∞–ª–≥–æ—Ä–∏—Ç–º –º–∞—à–∏–Ω–Ω–æ–≥–æ", "–æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
]

EXCLUDE_KEYWORDS = [
    # –§–∏–Ω–∞–Ω—Å—ã
    "–∞–∫—Ü–∏–∏", "–±–∏—Ä–∂–∞", "–∫–æ—Ç–∏—Ä–æ–≤–∫–∏", "–∏–Ω–¥–µ–∫—Å", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–∏–Ω–≤–µ—Å—Ç–æ—Ä",
    "–¥–∏–≤–∏–¥–µ–Ω–¥—ã", "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è", "–≤—ã—Ä—É—á–∫–∞", "–ø—Ä–∏–±—ã–ª—å", "—É–±—ã—Ç–æ–∫",
    "–¥–æ—Ö–æ–¥", "–æ–±–æ—Ä–æ—Ç", "–æ—Ç—á—ë—Ç–Ω–æ—Å—Ç—å",
    "—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "—Å—Ç–∞–≤–∫–∞", "–∏–Ω—Ñ–ª—è—Ü–∏—è", "—Ä–µ—Ü–µ—Å—Å–∏—è",
    "–±–∞–Ω–∫", "–∫—Ä–µ–¥–∏—Ç", "–∏–ø–æ—Ç–µ–∫–∞", "–≤–∫–ª–∞–¥", "–¥–µ–ø–æ–∑–∏—Ç", "—Å–¥–µ–ª–∫–∞", "—Å–ª–∏—è–Ω–∏–µ",
    "–ø–æ–≥–ª–æ—â–µ–Ω–∏–µ", "–ª–∏—Å—Ç–∏–Ω–≥",
    
    # –ö–∞–¥—Ä—ã
    "–Ω–∞–∑–Ω–∞—á–µ–Ω", "–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ", "–æ—Ç—Å—Ç–∞–≤–∫–∞", "—É–≤–æ–ª–µ–Ω",
    "—à—Ç–∞—Ç", "—É–≤–æ–ª—å–Ω–µ–Ω–∏—è", "—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è", "—à—Ç–∞–±-–∫–≤–∞—Ä—Ç–∏—Ä–∞",
    
    # –°–ø–æ—Ä—Ç
    "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "—Å–ø–æ—Ä—Ç", "–º–∞—Ç—á", "—Ç—É—Ä–Ω–∏—Ä",
    "—á–µ–º–ø–∏–æ–Ω–∞—Ç", "–æ–ª–∏–º–ø–∏–∞–¥–∞", "—Å–±–æ—Ä–Ω–∞—è",
    
    # –ü–æ–ª–∏—Ç–∏–∫–∞
    "–≤—ã–±–æ—Ä—ã", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–¥–µ–ø—É—Ç–∞—Ç", "—Å–∞–Ω–∫—Ü–∏–∏",
    "—Ç—é—Ä—å–º–∞", "—à—Ç—Ä–∞—Ñ", "–ø—Ä–∏–≥–æ–≤–æ—Ä", "–∞—Ä–µ—Å—Ç",
    "–º–∏–Ω–∏—Å—Ç—Ä", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–≥–æ—Å–¥—É–º–∞",
    
    # –†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è
    "–∫–∏–Ω–æ", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "–∫–æ–Ω—Ü–µ—Ä—Ç", "–∞–∫—Ç–µ—Ä", "–∞–∫—Ç—ë—Ä",
    "—Ä–µ–∂–∏—Å—Å–µ—Ä", "–ø—Ä–µ–º—å–µ—Ä–∞", "–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä",
    
    # –ê–≤—Ç–æ
    "–∞–≤—Ç–æ–º–æ–±–∏–ª—å", "–º–∞—à–∏–Ω–∞", "—Ç–µ—Å–ª–∞", "tesla",
    "—ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª—å", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç", "–¥–≤–∏–≥–∞—Ç–µ–ª—å",
    "–±–µ–Ω–∑–∏–Ω", "–¥–∏–∑–µ–ª—å", "–≤–æ–¥–∏—Ç–µ–ª—å", "–∞–≤—Ç–æ–ø—Ä–æ–º",
    
    # –ê—Ä—Ö–µ–æ–ª–æ–≥–∏—è/–ò—Å—Ç–æ—Ä–∏—è
    "–∞—Ä—Ö–µ–æ–ª–æ–≥", "—Ä–∞—Å–∫–æ–ø–∫–∏", "–¥—Ä–µ–≤–Ω–∏–π", "–∞—Ä—Ç–µ—Ñ–∞–∫—Ç", "–º—É–º–∏—è",
    "–≥—Ä–æ–±–Ω–∏—Ü–∞", "–¥–∏–Ω–æ–∑–∞–≤—Ä", "–∏—Å–∫–æ–ø–∞–µ–º–æ–µ",
]

BAD_PHRASES = [
    "–ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ", "—É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ",
    "–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–∞—â–∏—Ç—É", "–ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è",
    "–¥–µ–ª–∞–µ—Ç –±–∏–∑–Ω–µ—Å —É—Å—Ç–æ–π—á–∏–≤–µ–µ", "–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ø—Ä–æ—â–∞–µ—Ç",
    "–∏–¥–µ–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è", "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Ä–∞–±–æ—Ç–∞—Ç—å",
    "–Ω–∞ –ø—Ä–∞–≤–∞—Ö —Ä–µ–∫–ª–∞–º—ã", "–ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª",
    "–ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ", "—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç",
    "–Ω–µ –∏–º–µ–µ—Ç –∞–Ω–∞–ª–æ–≥–æ–≤", "–ª–∏–¥–µ—Ä —Ä—ã–Ω–∫–∞",
]


# ============ ARTICLE DATACLASS ============

@dataclass
class Article:
    id: str
    title: str
    summary: str
    link: str
    source: str
    published: datetime = field(default_factory=datetime.now)
    
    def get_full_text(self) -> str:
        return f"{self.title} {self.summary}"


# ============ TOPIC ENUM ============

class Topic:
    LLM = "llm"
    IMAGE_GEN = "image_gen"
    ROBOTICS = "robotics"
    HARDWARE = "hardware"
    AI = "ai"
    
    HASHTAGS = {
        "llm": "#ChatGPT #LLM #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "image_gen": "#AI #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "robotics": "#—Ä–æ–±–æ—Ç—ã #AI #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "hardware": "#–∂–µ–ª–µ–∑–æ #GPU #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "ai": "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
    }
    
    @classmethod
    def detect(cls, title: str, summary: str) -> str:
        text = f"{title} {summary}".lower()
        
        if any(kw in text for kw in ["gpt", "chatgpt", "claude", "llm", "gemini"]):
            return cls.LLM
        if any(kw in text for kw in ["midjourney", "dall-e", "stable diffusion", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂"]):
            return cls.IMAGE_GEN
        if any(kw in text for kw in ["—Ä–æ–±–æ—Ç", "robot", "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫"]):
            return cls.ROBOTICS
        if any(kw in text for kw in ["nvidia", "gpu", "—á–∏–ø", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "–≤–∏–¥–µ–æ–∫–∞—Ä—Ç"]):
            return cls.HARDWARE
        
        return cls.AI
    
    @classmethod
    def get_hashtags(cls, topic: str) -> str:
        return cls.HASHTAGS.get(topic, cls.HASHTAGS[cls.AI])


# ============ URL NORMALIZATION ============

def normalize_url(url: str) -> str:
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        path = parsed.path.rstrip("/")
        domain = parsed.netloc.lower().replace("www.", "")
        return f"{domain}{path}"
    except Exception:
        return url.split("?")[0].rstrip("/")


def extract_article_id(url: str) -> str:
    normalized = normalize_url(url)
    
    # Habr
    habr_match = re.search(r'habr\.com/.+?/(\d{5,7})', normalized)
    if habr_match:
        return f"habr_{habr_match.group(1)}"
    
    # 3DNews
    dnews_match = re.search(r'3dnews\.ru/(\d+)', normalized)
    if dnews_match:
        return f"3dnews_{dnews_match.group(1)}"
    
    # iXBT
    if 'ixbt.com' in normalized:
        return f"ixbt_{hashlib.md5(normalized.encode()).hexdigest()[:12]}"
    
    return hashlib.md5(normalized.encode()).hexdigest()[:16]


# ============ POSTED ARTICLES MANAGER ============

class PostedManager:
    def __init__(self, filepath: str):
        self.filepath = filepath
        self.posted_ids: set = set()
        self.posted_urls: set = set()
        self.data: List[Dict] = []
        self._load()
    
    def _load(self) -> None:
        if not os.path.exists(self.filepath):
            self._save()
            return
        
        try:
            with open(self.filepath, "r", encoding="utf-8") as f:
                self.data = json.load(f)
            
            for item in self.data:
                if "id" in item:
                    url = item["id"]
                    self.posted_urls.add(normalize_url(url))
                    self.posted_ids.add(extract_article_id(url))
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {self.filepath}: {e}")
            self.data = []
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            self.data = []
    
    def _save(self) -> None:
        try:
            with open(self.filepath, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {self.filepath}: {e}")
    
    def is_posted(self, url: str) -> bool:
        return (
            extract_article_id(url) in self.posted_ids or 
            normalize_url(url) in self.posted_urls
        )
    
    def add(self, url: str, title: str = "") -> None:
        self.posted_ids.add(extract_article_id(url))
        self.posted_urls.add(normalize_url(url))
        self.data.append({
            "id": url,
            "title": title[:100],
            "timestamp": datetime.now().timestamp()
        })
        self._save()
    
    def cleanup(self, days: int = 30) -> int:
        cutoff = datetime.now().timestamp() - (days * 86400)
        old_count = len(self.data)
        self.data = [i for i in self.data if i.get("timestamp", 0) > cutoff]
        removed = old_count - len(self.data)
        
        if removed > 0:
            # –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
            self.posted_ids.clear()
            self.posted_urls.clear()
            for item in self.data:
                if "id" in item:
                    url = item["id"]
                    self.posted_urls.add(normalize_url(url))
                    self.posted_ids.add(extract_article_id(url))
            self._save()
            print(f"üßπ –£–¥–∞–ª–µ–Ω–æ {removed} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π")
        
        return removed
    
    def count(self) -> int:
        return len(self.data)


# ============ KEYWORD MATCHING ============

def has_exact_keyword(text: str, keywords: List[str]) -> Optional[str]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–æ–≥–æ —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞ —Å –¥–µ—Ñ–∏—Å–∞–º–∏ (ai-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, text-to-image).
    """
    text_lower = text.lower()
    # –ó–∞—Ö–≤–∞—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ —Å –¥–µ—Ñ–∏—Å–∞–º–∏
    words = set(re.findall(r'\b[\w-]+\b', text_lower))
    
    for kw in keywords:
        kw_lower = kw.lower()
        # –§—Ä–∞–∑–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤
        if " " in kw_lower:
            if kw_lower in text_lower:
                return kw
        # –û–¥–Ω–æ —Å–ª–æ–≤–æ (–≤–æ–∑–º–æ–∂–Ω–æ —Å –¥–µ—Ñ–∏—Å–æ–º)
        elif kw_lower in words:
            return kw
    
    return None


def has_ai_keyword(text: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ AI-—Ç–µ–º–∞—Ç–∏–∫–∏.
    –î–ª—è AI –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∏—Å–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ (–Ω–µ–π—Ä–æ—Å–µ—Ç -> –Ω–µ–π—Ä–æ—Å–µ—Ç–∏).
    """
    text_lower = text.lower()
    
    for kw in AI_KEYWORDS:
        if kw.lower() in text_lower:
            return True
    
    return False


def is_too_promotional(text: str) -> bool:
    text_lower = text.lower()
    return any(phrase in text_lower for phrase in BAD_PHRASES)


# ============ HELPERS ============

def clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤."""
    if not text:
        return ""
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())


def build_final_post(
    core_text: str, 
    hashtags: str, 
    link: str, 
    max_total: int = 1024
) -> str:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç —Å CTA, —Ö–µ—à—Ç–µ–≥–∞–º–∏ –∏ —Å—Å—ã–ª–∫–æ–π."""
    
    # ========== –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨ ==========
    cta_line = "\n\nüî• ‚Äî –æ–≥–æ–Ω—å! | üóø ‚Äî –Ω—É —Ç–∞–∫–æ–µ | ‚ö° ‚Äî –ø—Ä–∏–∫–æ–ª—å–Ω–æ"
    # =====================================
    
    source_line = f'\nüîó <a href="{link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
    hashtag_line = f"\n\n{hashtags}"
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    reserved = len(cta_line) + len(hashtag_line) + len(source_line) + 20
    max_core = max_total - reserved
    
    if len(core_text) > max_core:
        core_text = core_text[:max_core]
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        last_punct = max(
            core_text.rfind('.'),
            core_text.rfind('!'),
            core_text.rfind('?')
        )
        if last_punct > max_core // 2:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –Ω–µ —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ
            core_text = core_text[:last_punct + 1]
    
    return core_text + cta_line + hashtag_line + source_line


# ============ RSS LOADING ============

async def load_rss_async(
    session: aiohttp.ClientSession, 
    url: str, 
    source: str,
    posted_manager: PostedManager
) -> List[Article]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç RSS-–ª–µ–Ω—Ç—É."""
    articles = []
    
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as resp:
            if resp.status != 200:
                print(f"‚ö†Ô∏è {source}: HTTP {resp.status}")
                return []
            
            content = await resp.text()
            feed = feedparser.parse(content)
            
            if feed.bozo:
                print(f"‚ö†Ô∏è {source}: RSS parsing issue - {feed.bozo_exception}")
                if not feed.entries:
                    return []
    
    except asyncio.TimeoutError:
        print(f"‚ö†Ô∏è {source}: Timeout")
        return []
    except aiohttp.ClientError as e:
        print(f"‚ùå {source}: Connection error - {e}")
        return []
    except Exception as e:
        print(f"‚ùå {source}: Unexpected error - {e}")
        return []
    
    for entry in feed.entries[:30]:
        link = entry.get("link", "")
        if not link or posted_manager.is_posted(link):
            continue
        
        title = clean_text(entry.get("title") or "")
        summary = clean_text(
            entry.get("summary") or entry.get("description") or ""
        )[:700]
        
        if not title:
            continue
        
        articles.append(Article(
            id=link,
            title=title,
            summary=summary,
            link=link,
            source=source,
            published=datetime.now()
        ))
    
    return articles


async def load_all_feeds(posted_manager: PostedManager) -> List[Article]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ RSS-–ª–µ–Ω—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ."""
    print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ RSS...")
    
    async with aiohttp.ClientSession(headers=HEADERS) as session:
        tasks = [
            load_rss_async(session, url, name, posted_manager)
            for url, name in RSS_FEEDS
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
    
    articles = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"‚ùå {RSS_FEEDS[i][1]}: {result}")
        elif isinstance(result, list):
            articles.extend(result)
            if result:
                print(f"‚úÖ {RSS_FEEDS[i][1]}: {len(result)} —Å—Ç–∞—Ç–µ–π")
    
    print(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
    return articles


# ============ FILTERING ============

def filter_articles(articles: List[Article]) -> List[Article]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º."""
    valid = []
    excluded_log = []
    
    for article in articles:
        text = article.get_full_text()
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤)
        bad_word = has_exact_keyword(text, EXCLUDE_KEYWORDS)
        if bad_word:
            excluded_log.append(f"‚ùå {article.title[:40]}... (—Å–ª–æ–≤–æ: '{bad_word}')")
            continue
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–º–∞—Ç–∏–∫–∏ AI
        if not has_ai_keyword(text):
            continue
        
        valid.append(article)
    
    excluded_count = len(articles) - len(valid)
    print(f"\nüóë –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {excluded_count} —Å—Ç–∞—Ç–µ–π")
    
    if excluded_log:
        print("üîç –ü—Ä–∏–º–µ—Ä—ã –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö:")
        for log in excluded_log[:5]:
            print(f"   {log}")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
    valid.sort(key=lambda x: x.published, reverse=True)
    
    return valid


# ============ GROQ GENERATION ============

async def generate_summary(
    article: Article,
    rate_limit_delay: float = 1.0
) -> Optional[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ—Å—Ç —á–µ—Ä–µ–∑ Groq API."""
    print(f"   üìù –û–±—Ä–∞–±–æ—Ç–∫–∞: {article.title[:50]}...")
    
    prompt = f"""
–†–æ–ª—å: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä–µ–¥–∞–∫—Ç–æ—Ä Telegram-–∫–∞–Ω–∞–ª–∞ –æ–± –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ.

–ó–∞–¥–∞—á–∞: –ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å –Ω–æ–≤–æ—Å—Ç—å –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∏ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ—Å—Ç –¥–ª—è –∞—É–¥–∏—Ç–æ—Ä–∏–∏, –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–µ–π—Å—è AI.

–ò—Å—Ö–æ–¥–Ω–∏–∫:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {article.summary}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
1. –ù–∞—á–Ω–∏ —Å –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è: "–ü—Ä–∏–≤–µ—Ç! üëã" –∏–ª–∏ "AI-–Ω–æ–≤–æ—Å—Ç–∏ ‚ö°" –∏–ª–∏ "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –∏–∑ –º–∏—Ä–∞ AI ü§ñ"
2. –û–±—ä—è—Å–Ω–∏ –ß–¢–û –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏ –ü–û–ß–ï–ú–£ —ç—Ç–æ –≤–∞–∂–Ω–æ/–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç–æ–π —è–∑—ã–∫, –ø–æ–Ω—è—Ç–Ω—ã–π —à–∏—Ä–æ–∫–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
4. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ –∫–ª–∏—à–µ ("—É–Ω–∏–∫–∞–ª—å–Ω—ã–π", "—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π", "–ª—É—á—à–∏–π")
5. –ù–ï –¥–æ–±–∞–≤–ª—è–π –ø—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é ("–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å", "—Å—Ç–∞–≤—å—Ç–µ –ª–∞–π–∫")
6. –û–±—ä–µ–º: 600-800 —Å–∏–º–≤–æ–ª–æ–≤

–í–∞–∂–Ω–æ: –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å –ù–ï —Å–≤—è–∑–∞–Ω–∞ —Å AI/ML/–Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º–∏ ‚Äî –æ—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û —Å–ª–æ–≤–æ–º: SKIP
"""
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è rate limiting
        await asyncio.sleep(rate_limit_delay)
        
        response = await asyncio.to_thread(
            lambda: groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.6,
                max_tokens=900,
            )
        )
        
        content = response.choices[0].message.content.strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ SKIP
        if content.upper().startswith("SKIP") or content.upper() == "SKIP":
            print("   ‚ö†Ô∏è Groq: —Ç–µ–º–∞ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç (SKIP)")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–∫–ª–∞–º–Ω—ã–π —Ç–µ–∫—Å—Ç
        if is_too_promotional(content):
            print("   ‚ö†Ô∏è –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º —Ä–µ–∫–ª–∞–º–Ω—ã–π")
            return None
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç
        topic = Topic.detect(article.title, article.summary)
        hashtags = Topic.get_hashtags(topic)
        
        return build_final_post(
            content, 
            hashtags, 
            article.link, 
            config.caption_limit
        )
    
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return None


# ============ IMAGE GENERATION ============

async def generate_image(title: str) -> Optional[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Pollinations.ai."""
    try:
        # –û—á–∏—â–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        clean_title = re.sub(r'[^\w\s]', '', title)[:50]
        prompt = (
            f"futuristic AI technology illustration, {clean_title}, "
            "minimalist design, 4k quality, blue and purple neon lighting, "
            "dark background, tech aesthetic"
        )
        
        seed = random.randint(0, 10000)
        url = (
            f"https://image.pollinations.ai/prompt/{urllib.parse.quote(prompt)}"
            f"?width=1024&height=1024&nologo=true&seed={seed}"
        )
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                if resp.status == 200:
                    fname = f"temp_img_{seed}.jpg"
                    content = await resp.read()
                    with open(fname, "wb") as f:
                        f.write(content)
                    print(f"   üñº –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ")
                    return fname
                else:
                    print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: HTTP {resp.status}")
    
    except asyncio.TimeoutError:
        print("   ‚ö†Ô∏è Timeout –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    except Exception as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
    
    return None


# ============ POSTING ============

async def post_to_channel(
    article: Article,
    text: str,
    posted_manager: PostedManager
) -> bool:
    """–ü—É–±–ª–∏–∫—É–µ—Ç –ø–æ—Å—Ç –≤ Telegram-–∫–∞–Ω–∞–ª."""
    img_path = None
    
    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_path = await generate_image(article.title)
        
        if img_path and os.path.exists(img_path):
            await bot.send_photo(
                config.channel_id,
                photo=FSInputFile(img_path),
                caption=text
            )
        else:
            await bot.send_message(
                config.channel_id,
                text=text,
                disable_web_page_preview=False
            )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ
        posted_manager.add(article.link, article.title)
        print(f"‚úÖ –£–°–ü–ï–®–ù–û –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û: {article.title[:50]}...")
        return True
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
        return False
    
    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if img_path and os.path.exists(img_path):
            try:
                os.remove(img_path)
            except Exception:
                pass


# ============ MAIN ============

async def autopost():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥–∞."""
    print(f"\n{'='*50}")
    print(f"üöÄ –ó–ê–ü–£–°–ö: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*50}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø–æ—Å—Ç–æ–≤
    posted_manager = PostedManager(config.posted_file)
    print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {posted_manager.count()} –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
    
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
    posted_manager.cleanup(config.retention_days)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π
    raw_articles = await load_all_feeds(posted_manager)
    
    if not raw_articles:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç—å–∏ –∏–∑ RSS")
        return
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
    candidates = filter_articles(raw_articles)
    
    if not candidates:
        print("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        return
    
    print(f"\nüéØ –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates)}")
    
    # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏
    for i, article in enumerate(candidates, 1):
        print(f"\n[{i}/{len(candidates)}] –ü—Ä–æ–±—É–µ–º: {article.title[:60]}...")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        text = await generate_summary(article)
        
        if not text:
            print("   ‚è© –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é...")
            continue
        
        # –ü—É–±–ª–∏–∫–∞—Ü–∏—è
        success = await post_to_channel(article, text, posted_manager)
        
        if success:
            break  # –í—ã—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
        await asyncio.sleep(2)
    
    else:
        print("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏")
    
    print(f"\n{'='*50}")
    print(f"üèÅ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {datetime.now().strftime('%H:%M:%S')}")
    print(f"{'='*50}")


async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞."""
    try:
        await autopost()
    except KeyboardInterrupt:
        print("\n\n‚õî –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise
    finally:
        await bot.session.close()


if __name__ == "__main__":
    asyncio.run(main())
























































































































































































































































