import os
import json
import asyncio
import random
import re
import time
from datetime import datetime
from typing import List, Dict, Optional

import requests
import feedparser
import urllib.parse
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from openai import OpenAI

# ============ CONFIG ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

if not all([OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    raise ValueError("‚ùå –ù–µ –≤—Å–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

POSTED_FILE = "posted_articles.json"
RETENTION_DAYS = 7
LAST_TYPE_FILE = "last_post_type.json"

REACTIONS_TEXT = (
    "–ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –∑–∞—à–µ–ª ‚Äî –∂–º–∏ üëç\n"
    "–ù–µ —Å–æ–≥–ª–∞—Å–µ–Ω ‚Äî –≤—ã–±–µ—Ä–∏ üò°\n"
    "–•–æ—á–µ—à—å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ ‚Äî –ø–æ—Å—Ç–∞–≤—å üî•\n"
    "–ö–æ–Ω—Ñ–∏–≥ —Ä–∞–±–æ—á–∏–π? –∂–º–∏ üü¢, –ª–∞–≥–∞–µ—Ç ‚Äî —Ç—ã–∫–∞–π üî¥\n"
    "–ü—Ä–æ—Ç–æ–∫–æ–ª —Ç–æ–ø? —Å—Ç–∞–≤—å üöÄ, –µ—Å–ª–∏ —Ñ–µ–π–ª ‚Äî –∂–º–∏ üí•\n"
    "–Æ–∑–∞–µ—à—å? –æ—Ç–º–µ—á–∞–π üòé, –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –≤—ã–±–∏—Ä–∞–π ü§î"
)

# ============ –°–¢–ò–õ–ò –ü–û–°–¢–û–í ============

POST_STYLES = [
    {
        "name": "–≤–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–π_–≥–∏–∫",
        "intro": "–¢—ã –≤–µ–¥—ë—à—å –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∫–∞–Ω–∞–ª –ø—Ä–æ –ò–ò –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –î–µ–ª–∏—à—å—Å—è –Ω–∞—Ö–æ–¥–∫–∞–º–∏ –∏ –Ω–æ–≤—ã–º–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞–º–∏, –±–µ–∑ —Ä–µ–∫–ª–∞–º–Ω–æ–≥–æ –ø–∞—Ñ–æ—Å–∞.",
        "tone": "–≠–Ω–µ—Ä–≥–∏—á–Ω—ã–π, –Ω–æ –æ–ø–∏—Ä–∞—é—â–∏–π—Å—è –Ω–∞ —Ñ–∞–∫—Ç—ã. –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∞–∫—Ü–µ–Ω—Ç –Ω–∞ —Å—É—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏.",
        "emojis": "üî•üöÄüí°ü§ñ‚ú®"
    },
    {
        "name": "–∞–Ω–∞–ª–∏—Ç–∏–∫",
        "intro": "–¢—ã ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ–±–æ–∑—Ä–µ–≤–∞—Ç–µ–ª—å. –û–±—ä—è—Å–Ω—è–µ—à—å –Ω–æ–≤—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤ –ò–ò –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –≤—ã–¥–µ–ª—è–µ—à—å –≥–ª–∞–≤–Ω–æ–µ.",
        "tone": "–°–ø–æ–∫–æ–π–Ω—ã–π, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π. –§–∞–∫—Ç—ã + –∫–æ—Ä–æ—Ç–∫–∏–π –≤—ã–≤–æ–¥, —á–µ–º –ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–∞ —ç—Ç–∞ —Ä–∞–±–æ—Ç–∞ –∏–ª–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è.",
        "emojis": "üß†üìäüî¨üíª‚ö°"
    },
    {
        "name": "–∏—Ä–æ–Ω–∏—á–Ω—ã–π_–æ–±–æ–∑—Ä–µ–≤–∞—Ç–µ–ª—å",
        "intro": "–¢—ã ‚Äî –æ–±–æ–∑—Ä–µ–≤–∞—Ç–µ–ª—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π. –ü–æ–∫–∞–∑—ã–≤–∞–µ—à—å, —á—Ç–æ –∏–º–µ–Ω–Ω–æ —Å–¥–µ–ª–∞–ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–ª–∏ –∏–Ω–∂–µ–Ω–µ—Ä—ã, –∏–Ω–æ–≥–¥–∞ —Å –ª—ë–≥–∫–æ–π –∏—Ä–æ–Ω–∏–µ–π.",
        "tone": "–ñ–∏–≤–æ–π, —Å –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–º —é–º–æ—Ä–æ–º, –Ω–æ –±–µ–∑ –ø—Ä–µ—É–≤–µ–ª–∏—á–µ–Ω–∏–π –∏ –±–µ–∑ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –ª–æ–∑—É–Ω–≥–æ–≤.",
        "emojis": "üëÄüéØüòèüõ†Ô∏èüí´"
    },
    {
        "name": "–ø—Ä–∞–∫—Ç–∏–∫",
        "intro": "–¢—ã ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –ò–ò. –ü–æ—è—Å–Ω—è–µ—à—å –ø–æ —Å—É—Ç–∏: –∫–∞–∫–∞—è –∑–∞–¥–∞—á–∞ —Ä–µ—à–∞–µ—Ç—Å—è, –∫–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–æ —Ä–µ—à–µ–Ω–∏–µ –∏ –∫–æ–º—É —ç—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è.",
        "tone": "–î–µ–ª–æ–≤–æ–π –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π. –ë–µ–∑ –ø–∞—Ñ–æ—Å–∞, –º–∏–Ω–∏–º—É–º –æ—Ü–µ–Ω–æ–∫.",
        "emojis": "‚öôÔ∏è‚úÖüì±üîßüí™"
    },
    {
        "name": "—Ñ—É—Ç—É—Ä–∏—Å—Ç",
        "intro": "–¢—ã ‚Äî —ç–Ω—Ç—É–∑–∏–∞—Å—Ç –±—É–¥—É—â–µ–≥–æ –ò–ò. –ü–æ–∫–∞–∑—ã–≤–∞–µ—à—å, –∫–∞–∫ –Ω–æ–≤–∞—è —Ä–∞–±–æ—Ç–∞, –º–æ–¥–µ–ª—å –∏–ª–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≤–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –≤ –∫–∞—Ä—Ç–∏–Ω—É —Ä–∞–∑–≤–∏—Ç–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π.",
        "tone": "–°–¥–µ—Ä–∂–∞–Ω–Ω–æ –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–π. –û—Å–Ω–æ–≤–Ω–æ–π —É–ø–æ—Ä –Ω–∞ —Ñ–∞–∫—Ç—ã –∏ –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π –≤–∑–≥–ª—è–¥ –≤–ø–µ—Ä—ë–¥.",
        "emojis": "üåüüîÆüöÄüåç‚ú®"
    }
]

POST_STRUCTURES = [
    "hook_features_conclusion",
    "problem_solution",
    "straight_news"
]

# ============ –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ============

AI_KEYWORDS = [
    "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å", "–∏–∏", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
    "neural network", "artificial intelligence",
    "llm", "gpt", "gpt-4", "gpt-5", "gpt-4o", "chatgpt", "claude", "gemini",
    "copilot", "mistral", "llama", "qwen", "gigachat", "yandexgpt",
    "kandinsky", "—à–µ–¥–µ–≤—Ä—É–º", "deepseek", "grok",
    "openai", "anthropic", "deepmind", "—Å–±–µ—Ä ai", "—è–Ω–¥–µ–∫—Å ai",
    "hugging face", "stability ai", "meta ai", "google ai",
    "stable diffusion", "midjourney", "dall-e", "sora", "runway",
    "–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
    "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ", "text-to-image", "text-to-video",
    "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "transformer",
    "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π",
    "–¥–æ–æ–±—É—á–µ–Ω–∏–µ", "–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", "–¥–∞—Ç–∞—Å–µ—Ç", "fine-tuning",
    "—á–∞—Ç-–±–æ—Ç", "–≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
    "–Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π", "ai-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "—É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫",
    "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ", "–æ–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–∞", "nlp",
    "agi", "—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "–∞–≥–µ–Ω—Ç", "ai-–∞–≥–µ–Ω—Ç", "–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ",
    "—Ç–æ–∫–µ–Ω", "–±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "reasoning",
    "–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "rlhf", "–ø—Ä–æ–º–ø—Ç", "prompt"
]

TECH_KEYWORDS = [
    "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª", "–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª", "–≤—ã–ø—É—Å—Ç–∏–ª", "—Ä–µ–ª–∏–∑", "–∑–∞–ø—É—Å—Ç–∏–ª",
    "–Ω–æ–≤–∏–Ω–∫–∞", "–¥–µ–±—é—Ç", "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è", "–ø–æ–∫–∞–∑–∞–ª", "unveiled",
    "—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "–Ω–æ—É—Ç–±—É–∫", "–≥–∞–¥–∂–µ—Ç", "–¥–µ–≤–∞–π—Å", "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ",
    "–Ω–æ—Å–∏–º–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞", "—É–º–Ω—ã–µ —á–∞—Å—ã", "–Ω–∞—É—à–Ω–∏–∫–∏",
    "—Ä–æ–±–æ—Ç", "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞", "–¥—Ä–æ–Ω", "–±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç",
    "–∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π", "boston dynamics", "tesla bot",
    "–∫–≤–∞–Ω—Ç–æ–≤—ã–π", "–∫–≤–∞–Ω—Ç–æ–≤—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—á–∏–ø",
    "gpu", "–≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞", "nvidia", "amd", "intel", "apple m",
    "spacex", "starship", "–∫–æ—Å–º–æ—Å", "—Ä–∞–∫–µ—Ç–∞", "—Å–ø—É—Ç–Ω–∏–∫",
    "starlink", "nasa", "—Ä–æ—Å–∫–æ—Å–º–æ—Å",
    "–≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å", "–¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å",
    "vr", "ar", "meta quest", "apple vision",
    "—ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª—å", "tesla", "—ç–ª–µ–∫—Ç—Ä–æ–∫–∞—Ä", "–±–∞—Ç–∞—Ä–µ—è",
    "–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä",
    "–ø—Ä–æ—Ä—ã–≤", "–∏–Ω–Ω–æ–≤–∞—Ü–∏—è", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è"
]

EXCLUDE_KEYWORDS = [
    "–∞–∫—Ü–∏–∏", "–∞–∫—Ü–∏—è", "–±–∏—Ä–∂–∞", "–∫–æ—Ç–∏—Ä–æ–≤–∫–∏", "–∏–Ω–¥–µ–∫—Å",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–∏–Ω–≤–µ—Å—Ç–æ—Ä", "–∏–Ω–≤–µ—Å—Ç–æ—Ä—ã", "–¥–∏–≤–∏–¥–µ–Ω–¥—ã",
    "ipo", "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è", "—Ä—ã–Ω–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å",
    "–≤—ã—Ä—É—á–∫–∞", "–ø—Ä–∏–±—ã–ª—å", "—É–±—ã—Ç–æ–∫", "–¥–æ—Ö–æ–¥", "–æ–±–æ—Ä–æ—Ç",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á—ë—Ç", "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç", "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç",
    "–º–∏–ª–ª–∏–∞—Ä–¥ –¥–æ–ª–ª–∞—Ä–æ–≤", "–º–∏–ª–ª–∏–æ–Ω –¥–æ–ª–ª–∞—Ä–æ–≤", "–º–ª—Ä–¥", "–º–ª–Ω —Ä—É–±–ª–µ–π",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "–∫—É—Ä—Å –µ–≤—Ä–æ", "–∫—É—Ä—Å —Ä—É–±–ª—è", "–≤–∞–ª—é—Ç–∞",
    "—Ü–±", "—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "—Å—Ç–∞–≤–∫–∞", "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞", "–∏–Ω—Ñ–ª—è—Ü–∏—è",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞", "—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π", "–≤–≤–ø", "—Ä–µ—Ü–µ—Å—Å–∏—è",
    "–±–∞–Ω–∫", "–∫—Ä–µ–¥–∏—Ç", "–∏–ø–æ—Ç–µ–∫–∞", "–≤–∫–ª–∞–¥", "–¥–µ–ø–æ–∑–∏—Ç",
    "—Ñ–æ–Ω–¥", "–≤–µ–Ω—á—É—Ä–Ω—ã–π", "—Ä–∞—É–Ω–¥ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è",
    "—Å–¥–µ–ª–∫–∞", "—Å–ª–∏—è–Ω–∏–µ", "–ø–æ–≥–ª–æ—â–µ–Ω–∏–µ", "m&a",
    "—Ä—ã–Ω–æ–∫", "–¥–æ–ª—è —Ä—ã–Ω–∫–∞", "–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã",
    "—Ü–µ–Ω–∞ –∞–∫—Ü–∏–π", "—Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏", "–æ—Ü–µ–Ω–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏",
    "–≤—ã—Ö–æ–¥ –Ω–∞ –±–∏—Ä–∂—É", "—Ä–∞–∑–º–µ—â–µ–Ω–∏–µ", "–ª–∏—Å—Ç–∏–Ω–≥",
    "–Ω–∞–∑–Ω–∞—á–µ–Ω", "–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ", "–æ—Ç—Å—Ç–∞–≤–∫–∞", "—É–≤–æ–ª–µ–Ω",
    "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "ceo", "–æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å —É—à—ë–ª",
    "—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —à—Ç–∞—Ç–∞", "—É–≤–æ–ª—å–Ω–µ–Ω–∏—è", "—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è",
    "–æ—Ñ–∏—Å", "—à—Ç–∞–±-–∫–≤–∞—Ä—Ç–∏—Ä–∞", "–ø–µ—Ä–µ–µ–∑–¥ –∫–æ–º–ø–∞–Ω–∏–∏",
    "—Ç–µ–Ω–Ω–∏—Å", "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "–±–∞—Å–∫–µ—Ç–±–æ–ª", "—Å–ø–æ—Ä—Ç", "–º–∞—Ç—á",
    "–æ–ª–∏–º–ø–∏–∞–¥–∞", "—á–µ–º–ø–∏–æ–Ω–∞—Ç", "—Ç—É—Ä–Ω–∏—Ä", "—Å–±–æ—Ä–Ω–∞—è",
    "–∏–≥—Ä–∞", "–≥–µ–π–º–ø–ª–µ–π", "playstation", "xbox", "steam", "nintendo",
    "–≤–∏–¥–µ–æ–∏–≥—Ä–∞", "–∫–æ–Ω—Å–æ–ª—å", "gaming",
    "–∫–∏–Ω–æ", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "–º—É–∑—ã–∫–∞", "–∫–æ–Ω—Ü–µ—Ä—Ç", "–∞–∫—Ç—ë—Ä", "–∞–∫—Ç–µ—Ä",
    "–ø—Ä–µ–º—å–µ—Ä–∞", "—Ç—Ä–µ–π–ª–µ—Ä", "netflix", "–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä",
    "–≤—ã–±–æ—Ä—ã", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–ø–∞—Ä–ª–∞–º–µ–Ω—Ç", "–ø–æ–ª–∏—Ç–∏–∫", "–¥–µ–ø—É—Ç–∞—Ç",
    "—Å–∞–Ω–∫—Ü–∏–∏", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–º–∏–Ω–∏—Å—Ç—Ä", "–∑–∞–∫–æ–Ω", "–∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç",
    "–±–æ–ª–µ–∑–Ω—å", "covid", "–ø–∞–Ω–¥–µ–º–∏—è", "–≥—Ä–∏–ø–ø", "–≤–∞–∫—Ü–∏–Ω–∞",
    "–∫—Ä–∏–ø—Ç–æ", "bitcoin", "–±–∏—Ç–∫–æ–π–Ω", "–±–∏—Ç–∫–æ–∏–Ω", "ethereum",
    "nft", "–±–ª–æ–∫—á–µ–π–Ω", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "–º–∞–π–Ω–∏–Ω–≥",
    "—Å—É–¥", "—Å—É–¥–µ–±–Ω—ã–π", "–∞—Ä–µ—Å—Ç", "–ø—Ä–∏–≥–æ–≤–æ—Ä", "—Ç—é—Ä—å–º–∞", "—à—Ç—Ä–∞—Ñ",
    "–∏—Å–∫", "–∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω—ã–π"
]

# ============ –ê–ù–¢–ò–†–ï–ö–õ–ê–ú–ù–´–ô –§–ò–õ–¨–¢–† ============

BAD_PHRASES = [
    "–ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ",
    "–ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ",
    "–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∑–∞—â–∏—Ç—É",
    "–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–¥—ë–∂–Ω—É—é –∑–∞—â–∏—Ç—É",
    "–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–∞—â–∏—Ç—É",
    "–ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Å–≤–æ–∏—Ö –∑–∞–¥–∞—á–∞—Ö",
    "–ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ –¥—É–º–∞—Ç—å –æ–± —É–≥—Ä–æ–∑–∞—Ö",
    "–¥–µ–ª–∞–µ—Ç –±–∏–∑–Ω–µ—Å —É—Å—Ç–æ–π—á–∏–≤–µ–µ",
    "–ø–æ–∑–≤–æ–ª—è–µ—Ç –±–∏–∑–Ω–µ—Å—É —Ä–∞–±–æ—Ç–∞—Ç—å —É—Å—Ç–æ–π—á–∏–≤–µ–µ",
    "–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ø—Ä–æ—â–∞–µ—Ç",
    "–∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ —É–ø—Ä–æ—â–∞–µ—Ç",
    "–∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è",
    "–∏–¥–µ–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è",
    "–ø–æ–º–æ–≥–∞–µ—Ç –±–∏–∑–Ω–µ—Å—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Ä–∞–±–æ—Ç–∞—Ç—å",
]

def is_too_promotional(text: str) -> bool:
    low = text.lower()
    if any(p in low for p in BAD_PHRASES):
        return True
    if ("–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç" in low or "–ø–æ–∑–≤–æ–ª—è–µ—Ç" in low or "–ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ" in low) and \
       not any(k in low for k in ["–∑–∞ —Å—á—ë—Ç", "–∑–∞ —Å—á–µ—Ç", "–∏—Å–ø–æ–ª—å–∑—É—è", "—á–µ—Ä–µ–∑", "–Ω–∞–ø—Ä–∏–º–µ—Ä", "–≤ —Ç–æ–º —á–∏—Å–ª–µ", "—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏", "–∞–Ω–∞–ª–∏–∑ —Ç—Ä–∞—Ñ–∏–∫–∞", "rate limiting", "–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫"]):
        return True
    return False

# ============ STATE ============

posted_articles: Dict[str, Optional[float]] = {}

if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        try:
            posted_data = json.load(f)
            posted_articles = {item["id"]: item.get("timestamp") for item in posted_data}
        except Exception:
            posted_articles = {}

def save_posted_articles() -> None:
    data = [{"id": id_str, "timestamp": ts} for id_str, ts in posted_articles.items()]
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def clean_old_posts() -> None:
    global posted_articles
    now = datetime.now().timestamp()
    cutoff = now - (RETENTION_DAYS * 86400)
    posted_articles = {
        id_str: ts for id_str, ts in posted_articles.items()
        if ts is None or ts > cutoff
    }
    save_posted_articles()

def save_posted(article_id: str) -> None:
    posted_articles[article_id] = datetime.now().timestamp()
    save_posted_articles()

def load_last_post_type() -> Optional[str]:
    if not os.path.exists(LAST_TYPE_FILE):
        return None
    try:
        with open(LAST_TYPE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("type")
    except Exception:
        return None

def save_last_post_type(post_type: str) -> None:
    try:
        with open(LAST_TYPE_FILE, "w", encoding="utf-8") as f:
            json.dump({"type": post_type}, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

# ============ HELPERS ============

def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())

def detect_topic(title: str, summary: str) -> str:
    text = f"{title} {summary}".lower()

    if any(kw in text for kw in ["gpt", "chatgpt", "claude", "llm", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å"]):
        return "llm"
    elif any(kw in text for kw in ["midjourney", "dall-e", "stable diffusion", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂"]):
        return "image_gen"
    elif any(kw in text for kw in ["—Ä–æ–±–æ—Ç", "robot", "–∞–≤—Ç–æ–Ω–æ–º–Ω"]):
        return "robotics"
    elif any(kw in text for kw in ["spacex", "–∫–æ—Å–º–æ—Å", "—Ä–∞–∫–µ—Ç–∞", "—Å–ø—É—Ç–Ω–∏–∫"]):
        return "space"
    elif any(kw in text for kw in ["nvidia", "gpu", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—á–∏–ø"]):
        return "hardware"
    elif any(kw in text for kw in ["–Ω–µ–π—Ä–æ—Å–µ—Ç", "neural", "–∏–∏", "ai", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]):
        return "ai"
    else:
        return "tech"

def get_hashtags(topic: str) -> str:
    hashtag_map = {
        "llm": "#ChatGPT #LLM #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "image_gen": "#AI #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "robotics": "#—Ä–æ–±–æ—Ç—ã #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
        "space": "#–∫–æ—Å–º–æ—Å #SpaceX #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "hardware": "#–∂–µ–ª–µ–∑–æ #GPU #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "ai": "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "tech": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–Ω–æ–≤–∏–Ω–∫–∏ #–≥–∞–¥–∂–µ—Ç—ã"
    }
    return hashtag_map.get(topic, "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–Ω–æ–≤–æ—Å—Ç–∏")

# ===== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –æ–±—Ä–µ–∑–∫–∞ ‚Äî —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç, –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º =====

def ensure_complete_sentence(text: str) -> str:
    """–£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–µ–∫—Å—Ç –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ."""
    text = text.strip()
    if not text:
        return text
    
    # –ï—Å–ª–∏ —É–∂–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è ‚Äî –û–ö
    if text[-1] in '.!?':
        return text
    
    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–Ω–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    last_period = text.rfind('.')
    last_exclaim = text.rfind('!')
    last_question = text.rfind('?')
    
    last_end = max(last_period, last_exclaim, last_question)
    
    if last_end > 0:
        return text[:last_end + 1]
    
    # –ï—Å–ª–∏ –∑–Ω–∞–∫–æ–≤ –Ω–µ—Ç ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫—É
    return text + '.'

def trim_core_text_to_limit(core_text: str, max_core_length: int) -> str:
    """
    –û–±—Ä–µ–∑–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, —á—Ç–æ–±—ã —É–ª–æ–∂–∏—Ç—å—Å—è –≤ –ª–∏–º–∏—Ç.
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º.
    """
    core_text = core_text.strip()
    
    if len(core_text) <= max_core_length:
        return ensure_complete_sentence(core_text)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å–æ—Ö—Ä–∞–Ω—è—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
    # –ü–∞—Ç—Ç–µ—Ä–Ω: —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ—Å–ª–µ .!? –Ω–æ –Ω–µ –≤–Ω—É—Ç—Ä–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π —Ç–∏–ø–∞ "—Ç.–µ.", "–∏ —Ç.–¥."
    sentence_pattern = r'(?<=[.!?])\s+'
    sentences = re.split(sentence_pattern, core_text)
    
    result = ""
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        candidate = (result + " " + sentence).strip() if result else sentence
        
        if len(candidate) <= max_core_length:
            result = candidate
        else:
            # –ù–µ –≤–ª–µ–∑–∞–µ—Ç ‚Äî –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
            break
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –≤–ª–µ–∑–ª–æ ‚Äî –±–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –æ–±—Ä–µ–∑–∞–µ–º –∂—ë—Å—Ç–∫–æ
    if not result and sentences:
        result = sentences[0][:max_core_length]
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–æ–±–µ–ª–∞, —á—Ç–æ–±—ã –Ω–µ —Ä–µ–∑–∞—Ç—å —Å–ª–æ–≤–æ
        if len(result) == max_core_length and ' ' in result:
            result = result.rsplit(' ', 1)[0]
    
    return ensure_complete_sentence(result)

def build_final_post(core_text: str, hashtags: str, link: str, max_total: int = 1024) -> str:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è —á—Ç–æ:
    1. –û–±—â–∞—è –¥–ª–∏–Ω–∞ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç
    2. –•–µ—à—Ç–µ–≥–∏, —Ä–µ–∞–∫—Ü–∏–∏ –∏ —Å—Å—ã–ª–∫–∞ –≤—Å–µ–≥–¥–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
    3. –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º
    """
    source_line = f'\n\nüîó <a href="{link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
    hashtag_line = f"\n\n{hashtags}"
    reactions_line = f"\n\n{REACTIONS_TEXT}"
    
    # –°—á–∏—Ç–∞–µ–º –º–µ—Å—Ç–æ –¥–ª—è —Å–ª—É–∂–µ–±–Ω—ã—Ö —á–∞—Å—Ç–µ–π
    service_length = len(hashtag_line) + len(reactions_line) + len(source_line)
    max_core_length = max_total - service_length - 10  # –∑–∞–ø–∞—Å 10 —Å–∏–º–≤–æ–ª–æ–≤
    
    # –û–±—Ä–µ–∑–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    trimmed_core = trim_core_text_to_limit(core_text, max_core_length)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç
    final = trimmed_core + hashtag_line + reactions_line + source_line
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    if len(final) > max_total:
        # –ê–≤–∞—Ä–∏–π–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ ‚Äî —É–º–µ–Ω—å—à–∞–µ–º core –µ—â—ë
        overflow = len(final) - max_total
        trimmed_core = trim_core_text_to_limit(core_text, max_core_length - overflow - 20)
        final = trimmed_core + hashtag_line + reactions_line + source_line
    
    return final

# ============ PARSERS ============

def load_rss(url: str, source: str) -> List[Dict]:
    articles = []
    try:
        feed = feedparser.parse(url)
        if feed.bozo and not feed.entries:
            print(f"‚ö†Ô∏è RSS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {source}")
            return articles
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RSS {source}: {e}")
        return articles

    for entry in feed.entries[:30]:
        link = entry.get("link", "")
        if not link or link in posted_articles:
            continue
        articles.append({
            "id": link,
            "title": clean_text(entry.get("title") or ""),
            "summary": clean_text(
                entry.get("summary") or entry.get("description") or ""
            )[:700],
            "link": link,
            "source": source,
            "published_parsed": datetime.now()
        })

    if articles:
        print(f"‚úÖ {source}: {len(articles)} —Å—Ç–∞—Ç–µ–π")

    return articles

def load_articles_from_sites() -> List[Dict]:
    articles: List[Dict] = []

    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru",
        "Habr AI"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru",
        "Habr ML"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/neural_networks/all/?fl=ru",
        "Habr Neural"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/data_science/all/?fl=ru",
        "Habr DS"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/natural_language_processing/all/?fl=ru",
        "Habr NLP"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/robotics/all/?fl=ru",
        "Habr Robotics"
    ))

    articles.extend(load_rss(
        "https://all-rss.ru/export/55.xml",
        "Overclockers Hardware"
    ))
    articles.extend(load_rss(
        "https://all-rss.ru/export/57.xml",
        "Overclockers IT"
    ))
    articles.extend(load_rss("https://hightech.fm/feed", "–•–∞–π—Ç–µ–∫"))
    articles.extend(load_rss("https://nplus1.ru/rss", "N+1"))

    articles.extend(load_rss("https://3dnews.ru/news/rss/", "3DNews"))
    articles.extend(load_rss("https://www.ixbt.com/export/news.rss", "iXBT"))
    articles.extend(load_rss("https://servernews.ru/rss", "ServerNews"))

    return articles

def filter_articles(articles: List[Dict]) -> List[Dict]:
    ai_articles = []
    tech_articles = []

    for e in articles:
        text = f"{e['title']} {e['summary']}".lower()

        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue

        source = e.get("source", "")
        if source in ["Overclockers Hardware", "Overclockers IT", "3DNews", "iXBT", "ServerNews"]:
            e["post_type"] = "hardware"
        else:
            e["post_type"] = "it"

        if any(kw in text for kw in AI_KEYWORDS):
            ai_articles.append(e)
        elif any(kw in text for kw in TECH_KEYWORDS):
            tech_articles.append(e)

    ai_articles.sort(key=lambda x: x["published_parsed"], reverse=True)
    tech_articles.sort(key=lambda x: x["published_parsed"], reverse=True)

    return ai_articles + tech_articles

# ============ –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê ============

def build_dynamic_prompt(title: str, summary: str, style: dict, structure: str) -> str:
    news_text = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n\n–¢–µ–∫—Å—Ç: {summary}"

    base_instructions = f"""
{style['intro']}

–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {style['tone']}
–≠–º–æ–¥–∑–∏: {style['emojis']}
"""

    structure_instructions = {
        "hook_features_conclusion": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. –ö–†–ê–¢–ö–û–ï –°–£–¢–¨ ‚Äî —á—Ç–æ –∑–∞ —Å–∏—Å—Ç–µ–º–∞/–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ –≤ —á—ë–º –Ω–æ–≤–∏–∑–Ω–∞.
2. –ö–ê–ö –†–ê–ë–û–¢–ê–ï–¢ ‚Äî 2‚Äì3 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞ –∏–ª–∏ –ø—Ä–∏—ë–º–∞, –∑–∞ —Å—á—ë—Ç —á–µ–≥–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
3. –í–´–í–û–î ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–º –ø–æ—Å–ª–µ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º: —á–µ–º —ç—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –∏ —á—Ç–æ —ç—Ç–æ –º–µ–Ω—è–µ—Ç.
""",
        "problem_solution": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. –ü–†–û–ë–õ–ï–ú–ê ‚Äî –∫–∞–∫—É—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–¥–∞—á—É —Ä–µ—à–∞—é—Ç (—É–∑–∫–∏–µ –º–µ—Å—Ç–∞, –Ω–∞–≥—Ä—É–∑–∫–∞, –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –∏ —Ç.–ø.).
2. –†–ï–®–ï–ù–ò–ï ‚Äî –∫–∞–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, —Ñ–æ—Ä–º–∞—Ç—ã —á–∏—Å–µ–ª, —Ä–∞–±–æ—Ç–∞ —Å –ø–∞–º—è—Ç—å—é, –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Ç.–ø.).
3. –≠–§–§–ï–ö–¢ ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–º –ø–æ—Å–ª–µ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º: —á—Ç–æ —ç—Ç–æ –¥–∞—ë—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º/—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º/–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ.
""",
        "straight_news": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. –§–ê–ö–¢ ‚Äî —á—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏/–∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∏ –±–µ–∑ —Ä–µ–∫–ª–∞–º—ã.
2. –¢–ï–•–î–ï–¢–ê–õ–ò ‚Äî 2‚Äì3 –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏–ª–∏ –ø—Ä–∏—ë–º–∞.
3. –ö–û–ù–¢–ï–ö–°–¢ ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–º –ø–æ—Å–ª–µ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º: –∑–∞—á–µ–º —ç—Ç–æ –∏ –≤ –∫–∞–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ.
"""
    }

    prompt = f"""
{base_instructions}

–ù–û–í–û–°–¢–¨:
{news_text}

{structure_instructions.get(structure, structure_instructions['straight_news'])}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
‚Ä¢ –ù–∞–ø–∏—à–∏ –æ–¥–∏–Ω —Å–≤—è–∑–Ω—ã–π –∞–±–∑–∞—Ü –¥–ª–∏–Ω–æ–π 400‚Äì600 —Å–∏–º–≤–æ–ª–æ–≤.
‚Ä¢ –Ø–∑—ã–∫: —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π.
‚Ä¢ –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–ø–æ–º—è–Ω–∏ 2‚Äì3 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏—ë–º–∞ –∏–ª–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞.
‚Ä¢ –ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —è–≤–Ω—ã–º –≤—ã–≤–æ–¥–æ–º.
‚Ä¢ –¢–µ–∫—Å—Ç –û–ë–Ø–ó–ê–ù –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è —Ç–æ—á–∫–æ–π, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–æ–º.
‚Ä¢ 0‚Äì2 —ç–º–æ–¥–∑–∏, —Ç–æ–ª—å–∫–æ –ø–æ –¥–µ–ª—É.
‚Ä¢ –ù–µ–ª—å–∑—è –ø–∏—Å–∞—Ç—å –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏—è ¬´–∫–∞–∫ –∏–º–µ–Ω–Ω–æ¬ª.
‚Ä¢ –ü–∏—à–∏ –ø–æ —Ñ–∞–∫—Ç–∞–º –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏.

–ó–ê–ü–†–ï–©–ï–ù–û:
‚Ä¢ –†–µ–∫–ª–∞–º–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –±–µ–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è.
‚Ä¢ –ö–ª–∏—à–µ: ¬´–ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Å–≤–æ–∏—Ö –∑–∞–¥–∞—á–∞—Ö¬ª, ¬´–¥–µ–ª–∞–µ—Ç –±–∏–∑–Ω–µ—Å —É—Å—Ç–æ–π—á–∏–≤–µ–µ¬ª.
‚Ä¢ –ü—Ä–æ–¥–∞–∂–Ω—ã–π —Ç–æ–Ω, –ø—Ä–∏–∑—ã–≤—ã –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏–ª–∏ –∫—É–ø–∏—Ç—å.
‚Ä¢ –û–±—Ä—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

–í–´–î–ê–ô –¢–û–õ–¨–ö–û –¢–ï–ö–°–¢ –ü–û–°–¢–ê, –±–µ–∑ —Ö–µ—à—Ç–µ–≥–æ–≤ –∏ —Å—Å—ã–ª–æ–∫.
"""
    return prompt

def validate_generated_text(text: str) -> tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (is_valid, reason).
    """
    text = text.strip()
    
    if not text:
        return False, "–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç"
    
    if len(text) < 100:
        return False, f"–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ—Å—Ç—å
    if text[-1] not in '.!?':
        return False, "–¢–µ–∫—Å—Ç –Ω–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –∑–Ω–∞–∫–æ–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ–±—Ä—ã–≤ (–Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Å–∫–æ–±–∫–∏, –∫–∞–≤—ã—á–∫–∏)
    if text.count('(') != text.count(')'):
        return False, "–ù–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Å–∫–æ–±–∫–∏"
    
    if text.count('¬´') != text.count('¬ª'):
        return False, "–ù–µ–∑–∞–∫—Ä—ã—Ç—ã–µ –∫–∞–≤—ã—á–∫–∏"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ (–ø—Ä–∏–∑–Ω–∞–∫ –æ–±—Ä—ã–≤–∞)
    sentences = re.split(r'[.!?]', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    if sentences and len(sentences[-1]) < 10:
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ –æ–±—Ä—ã–≤
        # –ù–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º –∫–æ—Ä–æ—Ç–∫–∏–º –≤—ã–≤–æ–¥–æ–º, —Ç–∞–∫ —á—Ç–æ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º
        pass
    
    return True, "OK"

def short_summary(title: str, summary: str, link: str) -> Optional[str]:
    style = random.choice(POST_STYLES)
    structure = random.choice(POST_STRUCTURES)

    print(f"  üìù –°—Ç–∏–ª—å: {style['name']}, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {structure}")

    prompt = build_dynamic_prompt(title, summary, style, structure)

    max_attempts = 2
    
    for attempt in range(max_attempts):
        try:
            res = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "–¢—ã ‚Äî –∞–≤—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –ò–ò –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. "
                            "–ü–∏—à–µ—à—å –ø–æ —Ñ–∞–∫—Ç–∞–º, —Å —É–ø–æ—Ä–æ–º –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º—ã –∏ –ø–æ–¥—Ö–æ–¥—ã, –±–µ–∑ —Ä–µ–∫–ª–∞–º–Ω–æ–≥–æ —Ç–æ–Ω–∞. "
                            "–í–ê–ñ–ù–û: –≤—Å–µ–≥–¥–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞–π —Ç–µ–∫—Å—Ç –ø–æ–ª–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º —Å —Ç–æ—á–∫–æ–π, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–æ–º."
                        )
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=600,
            )
            core = res.choices[0].message.content.strip()

            # –£–±–∏—Ä–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ –∫–∞–≤—ã—á–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
            if core.startswith('"') and core.endswith('"'):
                core = core[1:-1]
            if core.startswith('¬´') and core.endswith('¬ª'):
                core = core[1:-1]
            
            core = core.strip()

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            is_valid, reason = validate_generated_text(core)
            if not is_valid:
                print(f"  ‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: {reason}")
                if attempt < max_attempts - 1:
                    continue
                # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å
                core = ensure_complete_sentence(core)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–∫–ª–∞–º–Ω–æ—Å—Ç—å
            if is_too_promotional(core):
                print("  ‚ö†Ô∏è –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º —Ä–µ–∫–ª–∞–º–Ω—ã–π –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return None

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É –∏ —Ö–µ—à—Ç–µ–≥–∏
            topic = detect_topic(title, summary)
            hashtags = get_hashtags(topic)

            # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ—Å—Ç–∏
            final = build_final_post(core, hashtags, link, max_total=1024)

            print(f"  ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –ø–æ—Å—Ç: {len(final)} —Å–∏–º–≤–æ–ª–æ–≤")
            return final

        except Exception as e:
            print(f"‚ùå OpenAI –æ—à–∏–±–∫–∞: {e}")
            if attempt < max_attempts - 1:
                time.sleep(2)
                continue
            return None
    
    return None

# ============ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–ê–†–¢–ò–ù–û–ö ============

def generate_image(title: str, max_retries: int = 3) -> Optional[str]:
    image_styles = [
        "futuristic minimalist illustration, soft gradients, ",
        "abstract tech visualization, geometric shapes, ",
        "modern digital art, clean lines, ",
        "sci-fi concept art, atmospheric lighting, ",
        "sleek technology render, professional, "
    ]

    style = random.choice(image_styles)

    for attempt in range(max_retries):
        seed = random.randint(0, 10**7)
        clean_title = title[:60].replace('"', '').replace("'", "").replace('\n', ' ')

        prompt = (
            f"{style}{clean_title}, "
            "neural networks, innovation, technology, "
            "4k quality, no text, no letters, no words, "
            "clean composition, professional"
        )

        try:
            encoded = urllib.parse.quote(prompt)
            url = f"https://image.pollinations.ai/prompt/{encoded}?seed={seed}&width=1024&height=1024&nologo=true"

            print(f"  üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})...")

            resp = requests.get(url, timeout=90, headers=HEADERS)

            if resp.status_code == 200:
                content_type = resp.headers.get('content-type', '')
                if 'image' in content_type and len(resp.content) > 10000:
                    fname = f"img_{seed}.jpg"
                    with open(fname, "wb") as f:
                        f.write(resp.content)
                    print(f"  ‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {fname}")
                    return fname
                else:
                    print(f"  ‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω –Ω–µ–≤–µ—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (size: {len(resp.content)})")
            else:
                print(f"  ‚ö†Ô∏è HTTP {resp.status_code}")

        except requests.Timeout:
            print("  ‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        except requests.RequestException as e:
            print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}")
        except Exception as e:
            print(f"  ‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

        if attempt < max_retries - 1:
            await_time = (attempt + 1) * 2
            print(f"  ‚è≥ –ñ–¥—ë–º {await_time}—Å –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
            time.sleep(await_time)

    print("  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫")
    return None

def cleanup_image(filepath: Optional[str]) -> None:
    if filepath and os.path.exists(filepath):
        try:
            os.remove(filepath)
        except Exception as e:
            print(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {filepath}: {e}")

# ============ –ê–í–¢–û–ü–û–°–¢ ============

async def autopost():
    clean_old_posts()
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π...")
    articles = load_articles_from_sites()
    candidates = filter_articles(articles)

    if not candidates:
        print("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ –ò–ò/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.")
        return

    ai_count = sum(1 for a in candidates if any(
        kw in f"{a['title']} {a['summary']}".lower()
        for kw in AI_KEYWORDS
    ))
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(candidates)} —Å—Ç–∞—Ç–µ–π ({ai_count} –ø—Ä–æ –ò–ò)")

    last_type = load_last_post_type()
    posted_count = 0
    max_posts = 1

    hardware_candidates = [c for c in candidates if c.get("post_type") == "hardware"]
    it_candidates = [c for c in candidates if c.get("post_type") == "it"]

    def pick_next_article() -> Optional[Dict]:
        nonlocal last_type
        if last_type == "hardware":
            if it_candidates:
                return it_candidates.pop(0)
            elif hardware_candidates:
                return hardware_candidates.pop(0)
        else:
            if hardware_candidates:
                return hardware_candidates.pop(0)
            elif it_candidates:
                return it_candidates.pop(0)
        return None

    while posted_count < max_posts:
        art = pick_next_article()
        if not art:
            break

        print(f"\nüîç –û–±—Ä–∞–±–æ—Ç–∫–∞: {art['title'][:60]}... [{art['source']}] (type={art.get('post_type')})")

        post_text = short_summary(art["title"], art["summary"], art["link"])

        if not post_text:
            print("  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é")
            continue

        img = generate_image(art["title"])

        try:
            if img:
                await bot.send_photo(
                    CHANNEL_ID,
                    photo=FSInputFile(img),
                    caption=post_text
                )
            else:
                await bot.send_message(CHANNEL_ID, text=post_text)

            save_posted(art["id"])
            posted_count += 1
            last_type = art.get("post_type")
            save_last_post_type(last_type)
            print(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {art['source']} (type={last_type})")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
        finally:
            cleanup_image(img)

    if posted_count == 0:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞")
    else:
        print(f"\nüéâ –£—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {posted_count}")

async def main():
    try:
        await autopost()
    finally:
        await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())



















































































































