import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional

import requests
import feedparser  # pip install feedparser
from aiogram import Bot
from aiogram.client.bot import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile

from openai import OpenAI

# ===== ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ =====
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

print("DEBUG OPENAI KEY LEN:", len(OPENAI_API_KEY) if OPENAI_API_KEY else 0)

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)

openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

# ===== ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° =====
STRONG_KEYWORDS = [
    "vpn", "Ð²Ð¿Ð½", "Ð¿Ñ€Ð¾ÐºÑÐ¸", "proxy", "tor", "shadowsocks",
    "wireguard", "openvpn", "ikev2",
    "Ð¾Ð±Ñ…Ð¾Ð´ Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð¾Ðº", "Ð¾Ð±Ñ…Ð¾Ð´ Ñ†ÐµÐ½Ð·ÑƒÑ€Ñ‹", "Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð½Ð¾ÑÑ‚ÑŒ",
    "Ñ€Ð¾ÑÐºÐ¾Ð¼Ð½Ð°Ð´Ð·Ð¾Ñ€", "Ñ€ÐºÐ½",
    "Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚-Ñ†ÐµÐ½Ð·ÑƒÑ€Ð°", "Ñ†ÐµÐ½Ð·ÑƒÑ€Ð° Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ðµ",
    "Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²ÐºÐ° ÑÐ°Ð¹Ñ‚Ð¾Ð²", "Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²ÐºÐ° Ñ€ÐµÑÑƒÑ€ÑÐ°",
    "Ñ€ÐµÐµÑÑ‚Ñ€ Ð·Ð°Ð¿Ñ€ÐµÑ‰ÐµÐ½Ð½Ñ‹Ñ… ÑÐ°Ð¹Ñ‚Ð¾Ð²", "Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð°",
    "Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ñ‚Ñ€Ð°Ñ„Ð¸ÐºÐ°", "dpi", "deep packet inspection",
    "Ñ‚ÐµÐ»ÐµÐ³Ñ€Ð°Ð¼", "telegram",
    "whatsapp", "signal", "viber",
    "messenger", "Ð¼ÐµÑÑÐµÐ½Ð´Ð¶ÐµÑ€",
    "Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸", "Ð¿Ð°Ñ‚Ñ‡ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸",
    "Ð°Ð½Ñ‚Ð¸Ð²Ð¸Ñ€ÑƒÑ", "firewall", "Ñ„Ð°ÐµÑ€Ð²Ð¾Ð»",
    "Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€", "Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€ tor",
    "ÐºÐ»Ð¸ÐµÐ½Ñ‚ vpn", "vpn-ÐºÐ»Ð¸ÐµÐ½Ñ‚",
    "Ð¼Ð¸Ð½Ñ†Ð¸Ñ„Ñ€Ñ‹", "Ð¼Ð¸Ð½Ñ†Ð¸Ñ„Ñ€Ñ‹ Ñ€Ñ„",
    "Ð±ÐµÐ»Ñ‹Ðµ ÑÐ¿Ð¸ÑÐºÐ¸", "Ð±ÐµÐ»Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº",
]

SOFT_KEYWORDS = [
    "Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¿Ðº", "desktop-Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ", "ÑƒÑ‚Ð¸Ð»Ð¸Ñ‚Ð° Ð´Ð»Ñ windows",
    "Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð° Ð´Ð»Ñ macos", "open source",
    "Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚", "Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÑŒ", "Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚Ð¸",
    "ai", "machine learning",
    "ÐºÐ¸Ð±ÐµÑ€Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ", "Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ",
    "ÐºÐ¾Ð½Ñ„Ð¸Ð´ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ðµ", "privacy",
    "ÑÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚", "Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ð°",
]

EXCLUDE_KEYWORDS = [
    "Ð¸Ð³Ñ€Ð°", "Ð¸Ð³Ñ€Ñ‹", "game", "games", "Ð³ÐµÐ¹Ð¼Ð¿Ð»ÐµÐ¹", "gameplay",
    "dungeon", "quest", "Ð±Ð¾ÑÑ", "boss", "Ñ€ÐµÐ¹Ð´", "raid",
    "Ð¾Ð½Ð»Ð°Ð¹Ð½-Ð¸Ð³Ñ€Ð°", "Ð¸Ð³Ñ€Ð¾Ð²Ð¾Ð¹", "Ð³ÐµÐ¹Ð¼Ð¸Ð½Ð³", "gaming",
    "playstation", "xbox", "nintendo", "steam",
    "ÑˆÑƒÑ‚ÐµÑ€", "rpg", "mmorpg", "moba", "battle royale",
]

POSTED_FILE = "posted_articles.json"
RETENTION_DAYS = 7  # Ð¥Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¿Ð¾ÑÑ‚Ñ‹ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 7 Ð´Ð½ÐµÐ¹

if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        try:
            posted_data = json.load(f)
            # posted_data â€” ÑÐ¿Ð¸ÑÐ¾Ðº {"id": ..., "timestamp": ...}
            if isinstance(posted_data, list) and posted_data and isinstance(posted_data[0], dict):
                posted_articles = {item["id"]: item.get("timestamp") for item in posted_data}
            else:
                # ÑÑ‚Ð°Ñ€Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ â€” Ð¿Ñ€Ð¾ÑÑ‚Ð¾ ÑÐ¿Ð¸ÑÐ¾Ðº id
                posted_articles = {id_str: None for id_str in posted_data}
        except Exception:
            posted_articles = {}
else:
    posted_articles = {}


def clean_old_posts() -> None:
    """Ð£Ð´Ð°Ð»ÑÐµÑ‚ Ð¿Ð¾ÑÑ‚Ñ‹ ÑÑ‚Ð°Ñ€ÑˆÐµ RETENTION_DAYS"""
    global posted_articles
    now = datetime.now().timestamp()
    cutoff = now - (RETENTION_DAYS * 86400)
    
    old_count = len(posted_articles)
    posted_articles = {
        id_str: ts for id_str, ts in posted_articles.items()
        if ts is None or ts > cutoff
    }
    removed = old_count - len(posted_articles)
    if removed > 0:
        print(f"ðŸ—‘ï¸ Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ð¿Ð¾ÑÑ‚Ð¾Ð²: {removed}")
    
    save_posted_articles()


def save_posted_articles() -> None:
    """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¿Ð¾ÑÑ‚Ñ‹ Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚ÐºÐ°Ð¼Ð¸"""
    data = [
        {"id": id_str, "timestamp": ts}
        for id_str, ts in posted_articles.items()
    ]
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def save_posted(article_id: str) -> None:
    posted_articles[article_id] = datetime.now().timestamp()
    save_posted_articles()


def safe_get(url: str) -> Optional[str]:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            print(f"HTTP {resp.status_code} Ð´Ð»Ñ {url}")
            return None
        return resp.text
    except Exception as e:
        print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ {url}:", e)
        return None


def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())


# ===== ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ 3DNews (HTML, Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 3) =====
def load_3dnews() -> List[Dict]:
    url = "https://3dnews.ru/"
    html = safe_get(url)
    if not html:
        return []

    articles: List[Dict] = []
    parts = html.split('<a href="/')

    # Ð‘ÐµÑ€Ñ‘Ð¼ 3 ÑÑ‚Ð°Ñ‚ÑŒÐ¸
    for part in parts[1:4]:
        href_end = part.find('"')
        title_start = part.find(">")
        title_end = part.find("</a>")
        if href_end == -1 or title_start == -1 or title_end == -1:
            continue

        href = part[:href_end]
        title = clean_text(part[title_start + 1:title_end])
        if not title:
            continue

        link = "https://3dnews.ru/" + href.lstrip("/")

        summary = ""
        desc_start = part.find('class="')
        if desc_start != -1:
            desc_chunk = part[desc_start:desc_start + 500]
            p_start = desc_chunk.find(">")
            if p_start != -1:
                p_end = desc_chunk.find("</", p_start)
                if p_end != -1:
                    summary = clean_text(desc_chunk[p_start + 1:p_end])[:300]

        articles.append({
            "id": link,
            "title": title,
            "summary": summary,
            "link": link,
            "source": "3DNews",
            "published_parsed": datetime.now(),
        })

    print(f"DEBUG: 3DNews - {len(articles)} ÑÑ‚Ð°Ñ‚ÐµÐ¹")
    return articles


# ===== VC.ru: Ð¾Ð´Ð½Ð° Ð¾Ð±Ñ‰Ð°Ñ RSS-Ð»ÐµÐ½Ñ‚Ð° =====
VC_RU_FEED = "https://vc.ru/rss"


def load_vcru_from_rss() -> List[Dict]:
    articles: List[Dict] = []

    print(f"Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ RSS VC.ru: {VC_RU_FEED}")
    try:
        feed = feedparser.parse(VC_RU_FEED)
    except Exception as e:
        print(f"ÐžÑˆÐ¸Ð±ÐºÐ° RSS {VC_RU_FEED}: {e}")
        return articles

    # Ð‘ÐµÑ€Ñ‘Ð¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 30 Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ (Ð²Ð¼ÐµÑÑ‚Ð¾ 10)
    for entry in feed.entries[:30]:
        link = entry.get("link", "")
        title = clean_text(entry.get("title", "") or "")
        summary = clean_text(
            entry.get("summary", "") or entry.get("description", "") or ""
        )[:400]

        if not link or not title:
            continue

        published_parsed = datetime.now()
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            try:
                published_parsed = datetime(*entry.published_parsed[:6])
            except Exception:
                pass

        articles.append({
            "id": link,
            "title": title,
            "summary": summary,
            "link": link,
            "source": "VC.ru/rss",
            "published_parsed": published_parsed,
        })

    print(f"DEBUG: VC.ru RSS - {len(articles)} ÑÑ‚Ð°Ñ‚ÐµÐ¹")
    return articles


def load_articles_from_sites() -> List[Dict]:
    articles: List[Dict] = []
    articles.extend(load_3dnews())
    articles.extend(load_vcru_from_rss())

    print(f"\n{'=' * 60}")
    print(f"Ð’Ð¡Ð•Ð“Ðž Ð¡ÐŸÐÐ Ð¡Ð•ÐÐž: {len(articles)} ÑÑ‚Ð°Ñ‚ÐµÐ¹")
    print(f"Ð’ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¾Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ¾Ð²Ð°Ð½Ð¾: {len(posted_articles)} ÑÑ‚Ð°Ñ‚ÐµÐ¹")
    print(f"{'=' * 60}")
    for i, art in enumerate(articles, 1):
        print(f"{i}. [{art['source']}] {art['title'][:80]}")
    print(f"{'=' * 60}\n")

    return articles


def filter_article(entry: Dict) -> Optional[str]:
    title = entry.get("title", "")
    summary = entry.get("summary", "")
    text = (title + " " + summary).lower()

    if any(kw.lower() in text for kw in EXCLUDE_KEYWORDS):
        return None

    if any(kw.lower() in text for kw in STRONG_KEYWORDS):
        return "strong"
    if any(kw.lower() in text for kw in SOFT_KEYWORDS):
        return "soft"
    return None


def pick_article(articles: List[Dict]) -> Optional[Dict]:
    scored = []
    skipped_count = 0
    
    for e in articles:
        # Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑƒÐ¶Ðµ Ð¾Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÑ‚Ð°Ñ‚ÑŒÐ¸
        article_id = e.get("id", e.get("link"))
        if article_id in posted_articles:
            skipped_count += 1
            continue

        level = filter_article(e)
        if not level:
            continue

        score = 2 if level == "strong" else 1
        scored.append((score, e))

    print(f"ÐŸÐ ÐžÐŸÐ£Ð©Ð•ÐÐž ÐžÐŸÐ£Ð‘Ð›Ð˜ÐšÐžÐ’ÐÐÐÐ«Ð¥: {skipped_count}")
    print(f"ÐŸÐžÐ”Ð¥ÐžÐ”Ð¯Ð©Ð˜Ð¥ ÐÐžÐ’Ð«Ð¥ Ð¡Ð¢ÐÐ¢Ð•Ð™: {len(scored)}")
    for i, (score, art) in enumerate(scored[:5], 1):
        level = "STRONG" if score == 2 else "SOFT"
        print(f"{i}. [{level}] [{art['source']}] {art['title'][:80]}")
    print(f"{'=' * 60}\n")

    if scored:
        scored.sort(
            key=lambda x: (
                x[0],
                x[1].get("published_parsed", datetime.now())
            ),
            reverse=True,
        )
        return scored[0][1]

    return None


# ===== Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° =====
def short_summary(title: str, summary: str) -> str:
    news_text = f"{title}. {summary}" if summary else title

    prompt = (
        f"ÐŸÐµÑ€ÐµÐ¿Ð¸ÑˆÐ¸ ÑÑ‚Ñƒ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð½Ð¾Ð²Ð¾ÑÑ‚ÑŒ Ð² ÑÑ‚Ð¸Ð»Ðµ Telegram-ÐºÐ°Ð½Ð°Ð»Ð°:\n\n"
        f"{news_text}\n\n"
        f"ÐŸÐ ÐÐ’Ð˜Ð›Ð:\n"
        f"1. ÐŸÐ¸ÑˆÐ¸ ÐšÐžÐÐšÐ Ð•Ð¢ÐÐž â€” ÐµÑÐ»Ð¸ ÑƒÐ¿Ð¾Ð¼ÑÐ½ÑƒÑ‚Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸/Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐµÐ³Ð¾! ÐÐ• Ð¿Ð¸ÑˆÐ¸ 'ÐšÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ X' Ð¸Ð»Ð¸ 'ÐŸÑ€Ð¾Ð´ÑƒÐºÑ‚ Y'.\n"
        f"2. Ð•ÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð²ÐµÑ€ÑÐ¸Ð¸, Ñ†Ð¸Ñ„Ñ€Ñ‹, Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ñ‹ â€” Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°Ð¹.\n"
        f"3. ÐŸÐ¸ÑˆÐ¸ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼ ÑÐ·Ñ‹ÐºÐ¾Ð¼, Ð±ÐµÐ· ÐºÐ°Ð½Ñ†ÐµÐ»ÑÑ€Ð¸Ñ‚Ð°.\n"
        f"4. ÐžÐ±ÑŠÑ‘Ð¼ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°: Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾ 400â€“600 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð².\n"
        f"5. Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹:\n"
        f"   [ÑÐ¼Ð¾Ð´Ð¶Ð¸] Ð¡Ñ‚Ñ€Ð¾ÐºÐ° 1: Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð¾\n"
        f"   [ÑÐ¼Ð¾Ð´Ð¶Ð¸] Ð¡Ñ‚Ñ€Ð¾ÐºÐ° 2: ÐºÐ°ÐºÐ°Ñ Ð±Ñ‹Ð»Ð° Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°\n"
        f"   [ÑÐ¼Ð¾Ð´Ð¶Ð¸] Ð¡Ñ‚Ñ€Ð¾ÐºÐ° 3: Ñ‡Ñ‚Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸Ð»Ð¾ÑÑŒ\n"
        f"   [ÑÐ¼Ð¾Ð´Ð¶Ð¸] Ð¡Ñ‚Ñ€Ð¾ÐºÐ° 4: Ð·Ð°Ñ‡ÐµÐ¼ ÑÑ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾\n"
        f"   ÐŸÐ£Ð¡Ð¢ÐÐ¯ Ð¡Ð¢Ð ÐžÐšÐ\n"
        f"   PSðŸ’¥ ÐšÑ‚Ð¾ Ð·Ð° ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ ðŸ‘‰ https://t.me/+EdEfIkn83Wg3ZTE6\n\n"
        f"Ð’ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾ÑÑ‚Ð° Ñ†ÐµÐ»Ð¸ÐºÐ¾Ð¼, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÑƒ PS."
    )

    result = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
    )
    text = result.choices[0].message.content.strip()

    ps = "PSðŸ’¥ ÐšÑ‚Ð¾ Ð·Ð° ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ ðŸ‘‰ https://t.me/+EdEfIkn83Wg3ZTE6"
    if ps not in text:
        text = f"{text.rstrip()}\n\n{ps}"

    return text


# ===== Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° Ð´Ð»Ñ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸ =====
def generate_image_prompt(title: str, summary: str) -> str:
    base_prompt = (
        f"Create a short image prompt for: {title}. "
        f"Style: cinematic realistic, dramatic lighting, dark tech atmosphere, high detail. "
        f"Focus on technology/cybersecurity/internet themes. No text, no logos. Max 200 chars."
    )
    result = openai_client.chat.completions.



































