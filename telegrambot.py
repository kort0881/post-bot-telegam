import os
import json
import asyncio
import random
from datetime import datetime
from typing import List, Dict, Optional

import requests
import feedparser
import urllib.parse
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from openai import OpenAI

# ============ CONFIG ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

if not all([OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    raise ValueError("‚ùå –ù–µ –≤—Å–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

POSTED_FILE = "posted_articles.json"
RETENTION_DAYS = 7

# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ª–∏–º–∏—Ç –ø–æ–¥–ø–∏—Å–∏ –∫ –º–µ–¥–∏–∞ –≤ Telegram (—Ä–µ–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç ~1024 —Å–∏–º–≤–æ–ª–∞)[web:54][web:48]
TELEGRAM_CAPTION_LIMIT = 1000

# ============ –°–¢–ò–õ–ò –ü–û–°–¢–û–í (–°–¢–†–û–ì–û –ù–û–í–û–°–¢–ù–´–ï) ============

POST_STYLES = [
    {
        "name": "news_report",
        "intro": "–¢—ã ‚Äî —Ç–µ—Ö–Ω–æ-–∂—É—Ä–Ω–∞–ª–∏—Å—Ç. –ò–∑–ª–∞–≥–∞–µ—à—å —Ñ–∞–∫—Ç—ã —Å—É—Ö–æ –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ.",
        "tone": "–°–¥–µ—Ä–∂–∞–Ω–Ω—ã–π, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π, –±–µ–∑–æ—Ü–µ–Ω–æ—á–Ω—ã–π.",
        "emojis": "ü§ñüìä"
    },
    {
        "name": "explainer",
        "intro": "–¢—ã ‚Äî –Ω–∞—É—á–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. –û–±—ä—è—Å–Ω—è–µ—à—å —Å—É—Ç—å –±–µ–∑ —ç–º–æ—Ü–∏–π.",
        "tone": "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, –ø–æ—è—Å–Ω–∏—Ç–µ–ª—å–Ω—ã–π, —Å—É—Ö–æ–π.",
        "emojis": "üß†üîç"
    }
]

POST_STRUCTURES = [
    "inverted_pyramid",
    "straight_news"
]

# ============ –ú–Ø–ì–ö–ò–ô –ê–ù–¢–ò–†–ï–ö–õ–ê–ú–ù–´–ô –§–ò–õ–¨–¢–† ============

HARD_BAD_PHRASES = [
    "must-have", "must have", "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ—Ü–µ–Ω–∏—Ç–µ",
    "–Ω–µ —É–ø—É—Å—Ç–∏—Ç–µ", "—É—Å–ø–µ–π—Ç–µ", "—Ç–æ–ª—å–∫–æ —Å–µ–π—á–∞—Å", "–ø—Ä—è–º–æ —Å–µ–π—á–∞—Å",
    "–ø–æ—Å–ø–µ—à–∏—Ç–µ", "—É–±–∏–π—Ü–∞ –≤—Å–µ–≥–æ", "killer —Ñ–∏—á–∞",
    "–ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ", "–∏–¥–µ–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
]

def is_too_promotional(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Ç–∫—Ä–æ–≤–µ–Ω–Ω–æ –ø—Ä–æ–¥–∞—é—â–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏."""
    low = text.lower()
    return any(phrase in low for phrase in HARD_BAD_PHRASES)

# ============ –ü–†–ò–û–†–ò–¢–ï–¢: –ò–ò –ò –ù–ï–ô–†–û–°–ï–¢–ò ============

AI_KEYWORDS = [
    "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å", "–∏–∏", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
    "neural network", "artificial intelligence",
    "llm", "gpt", "gpt-4", "gpt-5", "gpt-4o", "chatgpt", "claude", "gemini",
    "copilot", "mistral", "llama", "qwen", "gigachat", "yandexgpt",
    "kandinsky", "—à–µ–¥–µ–≤—Ä—É–º", "deepseek", "grok",
    "openai", "anthropic", "deepmind", "—Å–±–µ—Ä ai", "—è–Ω–¥–µ–∫—Å ai",
    "hugging face", "stability ai", "meta ai", "google ai",
    "stable diffusion", "midjourney", "dall-e", "sora", "runway",
    "–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
    "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ", "text-to-image", "text-to-video",
    "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "transformer",
    "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π",
    "–¥–æ–æ–±—É—á–µ–Ω–∏–µ", "–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", "–¥–∞—Ç–∞—Å–µ—Ç", "fine-tuning",
    "—á–∞—Ç-–±–æ—Ç", "–≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
    "–Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π", "ai-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "—É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫",
    "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ", "–æ–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–∞", "nlp",
    "agi", "—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "–∞–≥–µ–Ω—Ç", "ai-–∞–≥–µ–Ω—Ç", "–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ",
    "—Ç–æ–∫–µ–Ω", "–±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "reasoning",
    "–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "rlhf", "–ø—Ä–æ–º–ø—Ç", "prompt"
]

TECH_KEYWORDS = [
    "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª", "–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª", "–≤—ã–ø—É—Å—Ç–∏–ª", "—Ä–µ–ª–∏–∑", "–∑–∞–ø—É—Å—Ç–∏–ª",
    "–Ω–æ–≤–∏–Ω–∫–∞", "–¥–µ–±—é—Ç", "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è", "–ø–æ–∫–∞–∑–∞–ª", "unveiled",
    "—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "–Ω–æ—É—Ç–±—É–∫", "–≥–∞–¥–∂–µ—Ç", "–¥–µ–≤–∞–π—Å", "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ",
    "–Ω–æ—Å–∏–º–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞", "—É–º–Ω—ã–µ —á–∞—Å—ã", "–Ω–∞—É—à–Ω–∏–∫–∏",
    "—Ä–æ–±–æ—Ç", "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞", "–¥—Ä–æ–Ω", "–±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç",
    "–∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π", "boston dynamics", "tesla bot",
    "–∫–≤–∞–Ω—Ç–æ–≤—ã–π", "–∫–≤–∞–Ω—Ç–æ–≤—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—á–∏–ø",
    "gpu", "–≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞", "nvidia", "amd", "intel", "apple m",
    "spacex", "starship", "–∫–æ—Å–º–æ—Å", "—Ä–∞–∫–µ—Ç–∞", "—Å–ø—É—Ç–Ω–∏–∫",
    "starlink", "nasa", "—Ä–æ—Å–∫–æ—Å–º–æ—Å",
    "–≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å", "–¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å",
    "vr", "ar", "meta quest", "apple vision",
    "—ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª—å", "tesla", "—ç–ª–µ–∫—Ç—Ä–æ–∫–∞—Ä", "–±–∞—Ç–∞—Ä–µ—è", "–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä",
    "–ø—Ä–æ—Ä—ã–≤", "–∏–Ω–Ω–æ–≤–∞—Ü–∏—è", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è"
]

DISCOVERY_KEYWORDS = [
    "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏", "—É—á—ë–Ω—ã–µ", "—É—á–µ–Ω—ã–µ", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
    "–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–∏–Ω—Å—Ç–∏—Ç—É—Ç",
    "mit", "stanford", "oxford", "berkeley", "cambridge",
    "arxiv", "preprint", "–Ω–∞—É—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞", "–Ω–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è",
    "–æ–±–Ω–∞—Ä—É–∂–∏–ª–∏", "–æ–±–Ω–∞—Ä—É–∂–µ–Ω", "–Ω–∞—à–ª–∏", "–≤—ã—è—Å–Ω–∏–ª–∏", "–¥–æ–∫–∞–∑–∞–ª–∏",
    "—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥", "–Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º", "–Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥",
    "state-of-the-art", "sota", "benchmark", "dataset"
]

EXCLUDE_KEYWORDS = [
    "–∞–∫—Ü–∏–∏", "–∞–∫—Ü–∏—è", "–±–∏—Ä–∂–∞", "–∫–æ—Ç–∏—Ä–æ–≤–∫–∏", "–∏–Ω–¥–µ–∫—Å",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–∏–Ω–≤–µ—Å—Ç–æ—Ä", "–∏–Ω–≤–µ—Å—Ç–æ—Ä—ã", "–¥–∏–≤–∏–¥–µ–Ω–¥—ã",
    "ipo", "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è", "—Ä—ã–Ω–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å",
    "–≤—ã—Ä—É—á–∫–∞", "–ø—Ä–∏–±—ã–ª—å", "—É–±—ã—Ç–æ–∫", "–¥–æ—Ö–æ–¥", "–æ–±–æ—Ä–æ—Ç",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á—ë—Ç", "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç", "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç",
    "–º–∏–ª–ª–∏–∞—Ä–¥ –¥–æ–ª–ª–∞—Ä–æ–≤", "–º–∏–ª–ª–∏–æ–Ω –¥–æ–ª–ª–∞—Ä–æ–≤", "–º–ª—Ä–¥", "–º–ª–Ω —Ä—É–±–ª–µ–π",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "–∫—É—Ä—Å –µ–≤—Ä–æ", "–∫—É—Ä—Å —Ä—É–±–ª—è", "–≤–∞–ª—é—Ç–∞",
    "—Ü–±", "—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "—Å—Ç–∞–≤–∫–∞", "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞", "–∏–Ω—Ñ–ª—è—Ü–∏—è",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞", "—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π", "–≤–≤–ø", "—Ä–µ—Ü–µ—Å—Å–∏—è",
    "–±–∞–Ω–∫", "–∫—Ä–µ–¥–∏—Ç", "–∏–ø–æ—Ç–µ–∫–∞", "–≤–∫–ª–∞–¥", "–¥–µ–ø–æ–∑–∏—Ç",
    "—Ñ–æ–Ω–¥", "–≤–µ–Ω—á—É—Ä–Ω—ã–π", "—Ä–∞—É–Ω–¥ —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è",
    "—Å–¥–µ–ª–∫–∞", "—Å–ª–∏—è–Ω–∏–µ", "–ø–æ–≥–ª–æ—â–µ–Ω–∏–µ", "m&a",
    "—Ä—ã–Ω–æ–∫", "–¥–æ–ª—è —Ä—ã–Ω–∫–∞", "–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã",
    "—Ü–µ–Ω–∞ –∞–∫—Ü–∏–π", "—Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏", "–æ—Ü–µ–Ω–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏",
    "–≤—ã—Ö–æ–¥ –Ω–∞ –±–∏—Ä–∂—É", "—Ä–∞–∑–º–µ—â–µ–Ω–∏–µ", "–ª–∏—Å—Ç–∏–Ω–≥",
    "–Ω–∞–∑–Ω–∞—á–µ–Ω", "–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ", "–æ—Ç—Å—Ç–∞–≤–∫–∞", "—É–≤–æ–ª–µ–Ω",
    "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "ceo", "–æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å —É—à—ë–ª",
    "—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —à—Ç–∞—Ç–∞", "—É–≤–æ–ª—å–Ω–µ–Ω–∏—è", "—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è",
    "–æ—Ñ–∏—Å", "—à—Ç–∞–±-–∫–≤–∞—Ä—Ç–∏—Ä–∞", "–ø–µ—Ä–µ–µ–∑–¥ –∫–æ–º–ø–∞–Ω–∏–∏",
    "—Ç–µ–Ω–Ω–∏—Å", "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "–±–∞—Å–∫–µ—Ç–±–æ–ª", "—Å–ø–æ—Ä—Ç", "–º–∞—Ç—á",
    "–æ–ª–∏–º–ø–∏–∞–¥–∞", "—á–µ–º–ø–∏–æ–Ω–∞—Ç", "—Ç—É—Ä–Ω–∏—Ä", "—Å–±–æ—Ä–Ω–∞—è",
    "–∏–≥—Ä–∞", "–≥–µ–π–º–ø–ª–µ–π", "playstation", "xbox", "steam", "nintendo",
    "–≤–∏–¥–µ–æ–∏–≥—Ä–∞", "–∫–æ–Ω—Å–æ–ª—å", "gaming",
    "–∫–∏–Ω–æ", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "–º—É–∑—ã–∫–∞", "–∫–æ–Ω—Ü–µ—Ä—Ç", "–∞–∫—Ç—ë—Ä", "–∞–∫—Ç–µ—Ä",
    "–ø—Ä–µ–º—å–µ—Ä–∞", "—Ç—Ä–µ–π–ª–µ—Ä", "netflix", "–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä",
    "–≤—ã–±–æ—Ä—ã", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–ø–∞—Ä–ª–∞–º–µ–Ω—Ç", "–ø–æ–ª–∏—Ç–∏–∫", "–¥–µ–ø—É—Ç–∞—Ç",
    "—Å–∞–Ω–∫—Ü–∏–∏", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–º–∏–Ω–∏—Å—Ç—Ä", "–∑–∞–∫–æ–Ω", "–∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç",
    "–±–æ–ª–µ–∑–Ω—å", "covid", "–ø–∞–Ω–¥–µ–º–∏—è", "–≥—Ä–∏–ø–ø", "–≤–∞–∫—Ü–∏–Ω–∞",
    "–∫—Ä–∏–ø—Ç–æ", "bitcoin", "–±–∏—Ç–∫–æ–π–Ω", "–±–∏—Ç–∫–æ–∏–Ω", "ethereum",
    "nft", "–±–ª–æ–∫—á–µ–π–Ω", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "–º–∞–π–Ω–∏–Ω–≥",
    "—Å—É–¥", "—Å—É–¥–µ–±–Ω—ã–π", "–∞—Ä–µ—Å—Ç", "–ø—Ä–∏–≥–æ–≤–æ—Ä", "—Ç—é—Ä—å–º–∞", "—à—Ç—Ä–∞—Ñ",
    "–∏—Å–∫", "–∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω—ã–π"
]

# ============ STATE ============

posted_articles: Dict[str, Optional[float]] = {}

if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        try:
            posted_data = json.load(f)
            posted_articles = {item["id"]: item.get("timestamp") for item in posted_data}
        except Exception:
            posted_articles = {}


def save_posted_articles() -> None:
    data = [{"id": id_str, "timestamp": ts} for id_str, ts in posted_articles.items()]
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def clean_old_posts() -> None:
    global posted_articles
    now = datetime.now().timestamp()
    cutoff = now - (RETENTION_DAYS * 86400)
    posted_articles = {
        id_str: ts for id_str, ts in posted_articles.items()
        if ts is None or ts > cutoff
    }
    save_posted_articles()


def save_posted(article_id: str) -> None:
    posted_articles[article_id] = datetime.now().timestamp()
    save_posted_articles()


# ============ HELPERS ============

def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())


def detect_topic(title: str, summary: str) -> str:
    text = f"{title} {summary}".lower()

    if any(kw in text for kw in ["gpt", "chatgpt", "claude", "llm", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å"]):
        return "llm"
    elif any(kw in text for kw in ["midjourney", "dall-e", "stable diffusion", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂"]):
        return "image_gen"
    elif any(kw in text for kw in ["—Ä–æ–±–æ—Ç", "robot", "–∞–≤—Ç–æ–Ω–æ–º–Ω"]):
        return "robotics"
    elif any(kw in text for kw in ["spacex", "–∫–æ—Å–º–æ—Å", "—Ä–∞–∫–µ—Ç–∞", "—Å–ø—É—Ç–Ω–∏–∫"]):
        return "space"
    elif any(kw in text for kw in ["nvidia", "gpu", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—á–∏–ø"]):
        return "hardware"
    elif any(kw in text for kw in ["–Ω–µ–π—Ä–æ—Å–µ—Ç", "neural", "–∏–∏", "ai", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]):
        return "ai"
    else:
        return "tech"


def get_hashtags(topic: str) -> str:
    hashtag_map = {
        "llm": "#–ò–ò #LLM #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "image_gen": "#–ò–ò #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "robotics": "#—Ä–æ–±–æ—Ç—ã #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "space": "#–∫–æ—Å–º–æ—Å #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "hardware": "#–∂–µ–ª–µ–∑–æ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "ai": "#–ò–ò #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "tech": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–Ω–æ–≤–æ—Å—Ç–∏"
    }
    return hashtag_map.get(topic, "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏")


# ============ PARSERS ============

def load_rss(url: str, source: str) -> List[Dict]:
    articles = []
    try:
        feed = feedparser.parse(url)
        if feed.bozo and not feed.entries:
            print(f"‚ö†Ô∏è RSS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {source}")
            return articles
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RSS {source}: {e}")
        return articles

    for entry in feed.entries[:30]:
        link = entry.get("link", "")
        if not link or link in posted_articles:
            continue
        articles.append({
            "id": link,
            "title": clean_text(entry.get("title") or ""),
            "summary": clean_text(
                entry.get("summary") or entry.get("description") or ""
            )[:700],
            "link": link,
            "source": source,
            "published_parsed": datetime.now()
        })

    if articles:
        print(f"‚úÖ {source}: {len(articles)} —Å—Ç–∞—Ç–µ–π")

    return articles


def load_articles_from_sites() -> List[Dict]:
    articles: List[Dict] = []

    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru",
        "Habr AI"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru",
        "Habr ML"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/neural_networks/all/?fl=ru",
        "Habr Neural"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/data_science/all/?fl=ru",
        "Habr DS"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/natural_language_processing/all/?fl=ru",
        "Habr NLP"
    ))
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/robotics/all/?fl=ru",
        "Habr Robotics"
    ))
    articles.extend(load_rss("https://tproger.ru/feed/", "Tproger"))
    articles.extend(load_rss("https://hightech.fm/feed", "–•–∞–π—Ç–µ–∫"))
    articles.extend(load_rss("https://nplus1.ru/rss", "N+1"))
    articles.extend(load_rss("https://3dnews.ru/news/rss/", "3DNews"))
    articles.extend(load_rss("https://www.ixbt.com/export/news.rss", "iXBT"))
    articles.extend(load_rss("https://servernews.ru/rss", "ServerNews"))

    return articles


def filter_articles(articles: List[Dict]) -> List[Dict]:
    ai_discovery = []
    ai_other = []
    tech_discovery = []

    for e in articles:
        text = f"{e['title']} {e['summary']}".lower()

        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue

        is_ai = any(kw in text for kw in AI_KEYWORDS)
        is_tech = any(kw in text for kw in TECH_KEYWORDS)
        is_discovery = any(kw in text for kw in DISCOVERY_KEYWORDS)

        if is_ai and is_discovery:
            ai_discovery.append(e)
        elif is_ai:
            ai_other.append(e)
        elif is_tech and is_discovery:
            tech_discovery.append(e)

    for lst in (ai_discovery, ai_other, tech_discovery):
        lst.sort(key=lambda x: x["published_parsed"], reverse=True)

    return ai_discovery + ai_other + tech_discovery


# ============ –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê (–ú–Ø–ì–ö–ò–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø) ============

def build_dynamic_prompt(title: str, summary: str, style: dict, structure: str) -> str:
    news_text = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n\n–¢–µ–∫—Å—Ç: {summary}"

    structure_instructions = {
        "inverted_pyramid": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. –õ–ò–î ‚Äî —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏ –≥–ª–∞–≤–Ω–æ–µ –Ω–æ–≤–æ–µ (2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
2. –î–ï–¢–ê–õ–ò ‚Äî –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∫–∞–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è (3‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
3. –ö–û–ù–¢–ï–ö–°–¢ ‚Äî –∫ –∫–∞–∫–æ–º—É –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ò–ò/ML —ç—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∏ —á–µ–º –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
–ó–∞–≤–µ—Ä—à–∏ –º—ã—Å–ª—å —Ç–∞–∫, —á—Ç–æ–±—ã —Ç–µ–∫—Å—Ç –≤—ã–≥–ª—è–¥–µ–ª –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ, –±–µ–∑ –æ–±—Ä—ã–≤–∞.
""",
        "straight_news": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. –ì–õ–ê–í–ù–û–ï ‚Äî —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏ –∑–∞—á–µ–º —ç—Ç–æ –¥–µ–ª–∞–ª–∏ (2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
2. –ü–û–î–†–û–ë–ù–û–°–¢–ò ‚Äî –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã, –º–µ—Ç–æ–¥, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è (3‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
3. –ò–¢–û–ì ‚Äî –∫–∞–∫–æ–µ —ç—Ç–æ –¥–∞—ë—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏–ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
–ó–∞–≤–µ—Ä—à–∏ –∞–±–∑–∞—Ü–µ–º —Å —á—ë—Ç–∫–∏–º –≤—ã–≤–æ–¥–æ–º, –∞ –Ω–µ –æ–±—Ä—ã–≤–æ–º.
"""
    }

    prompt = f"""
–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ª–µ–Ω—Ç—ã –æ–± –ò–ò –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –ü–µ—Ä–µ–ø–∏—à–∏ –Ω–æ–≤–æ—Å—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∑–∞–º–µ—Ç–∫–∏.

–°—Ç–∏–ª—å: {style['tone']}

–ù–û–í–û–°–¢–¨:
{news_text}

{structure_instructions.get(structure, structure_instructions['straight_news'])}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
‚Ä¢ –¶–µ–ª—å: –æ–∫–æ–ª–æ 700 —Å–∏–º–≤–æ–ª–æ–≤. –î–æ–ø—É—Å—Ç–∏–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: –æ—Ç 400 –¥–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤.
‚Ä¢ –¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫.
‚Ä¢ –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 1‚Äì3 –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö —ç–º–æ–¥–∑–∏ –∏–∑ –Ω–∞–±–æ—Ä–∞ {style['emojis']} –∏ –æ–±—â–∏—Ö —Ç–µ—Ö-—ç–º–æ–¥–∑–∏ (‚öôÔ∏è, üíª, üì°, üìà, üõ∞Ô∏è), –µ—Å–ª–∏ –æ–Ω–∏ –ø–æ–º–æ–≥–∞—é—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç.
‚Ä¢ –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤—ã–≥–ª—è–¥–µ—Ç—å –∫–∞–∫ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–π –∞–±–∑–∞—Ü: —Å –≤–≤–æ–¥–æ–º, –¥–µ—Ç–∞–ª—è–º–∏ –∏ —á—ë—Ç–∫–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –≤—ã–≤–æ–¥–æ–º.
‚Ä¢ –ü–∏—à–∏ –≤ —Ç—Ä–µ—Ç—å–µ–º –ª–∏—Ü–µ, –±–µ–∑ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ —á–∏—Ç–∞—Ç–µ–ª—é.

–ó–ê–ü–†–ï–©–ï–ù–û:
‚Ä¢ –õ—é–±—ã–µ –ø—Ä—è–º—ã–µ –ø—Ä–∏–∑—ã–≤—ã: ¬´–ø–æ–ø—Ä–æ–±—É–π—Ç–µ¬ª, ¬´–æ—Ü–µ–Ω–∏—Ç–µ¬ª, ¬´–Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ¬ª –∏ —Ç.–ø.
‚Ä¢ –Ø–≤–Ω–æ —Ä–µ–∫–ª–∞–º–Ω—ã–π –∏ –ø—Ä–æ–¥–∞—é—â–∏–π —Ç–æ–Ω.
‚Ä¢ –û–±—Ä–∞—â–µ–Ω–∏—è –∫ —á–∏—Ç–∞—Ç–µ–ª—é: ¬´–≤—ã¬ª, ¬´–º—ã¬ª, ¬´–¥—Ä—É–∑—å—è¬ª.
‚Ä¢ –ö–ª–∏—à–µ: ¬´–±—É–¥—É—â–µ–µ –Ω–∞—Å—Ç—É–ø–∏–ª–æ¬ª, ¬´–º–∏—Ä –∏–∑–º–µ–Ω–∏–ª—Å—è¬ª, ¬´–Ω–æ–≤–∞—è —ç—Ä–∞¬ª –∏ —Ç.–ø.
‚Ä¢ –ü—Ä–∏–¥—É–º—ã–≤–∞—Ç—å —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏.

–í—ã–¥–∞–π —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞, –±–µ–∑ —Ö–µ—à—Ç–µ–≥–æ–≤ –∏ —Å—Å—ã–ª–æ–∫.
"""
    return prompt


def decorate_post(text: str, topic: str) -> str:
    """–ê–∫–∫—É—Ä–∞—Ç–Ω–æ —É–∫—Ä–∞—à–∞–µ—Ç –ø–æ—Å—Ç: —ç–º–æ–¥–∑–∏-–ª–∏–Ω–∏—è –≤–≤–µ—Ä—Ö—É + —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å."""
    topic_icon_map = {
        "llm": "ü§ñ",
        "ai": "üß†",
        "image_gen": "üé®",
        "robotics": "ü¶æ",
        "hardware": "üíª",
        "space": "üõ∞Ô∏è",
        "tech": "‚öôÔ∏è"
    }
    icon = topic_icon_map.get(topic, "‚öôÔ∏è")
    top_line = f"{icon} {icon} {icon}"
    separator = "\n\n‚Äî ‚Äî ‚Äî\n\n"
    return f"{top_line}\n\n{text}{separator}"


def short_summary(title: str, summary: str, link: str) -> Optional[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ø–æ—Å—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–ª–∏–Ω—É –∏ –º—è–≥–∫–æ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Ä–µ–∫–ª–∞–º—É."""
    style = random.choice(POST_STYLES)
    structure = random.choice(POST_STRUCTURES)

    print(f"   üìù –°—Ç–∏–ª—å: {style['name']}, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {structure}")

    prompt = build_dynamic_prompt(title, summary, style, structure)

    try:
        res = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. –ü–∏—à–µ—à—å –ø–æ —Ñ–∞–∫—Ç–∞–º, –±–µ–∑ —è–≤–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã, "
                        "–Ω–æ –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–æ—á–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞ –±–µ–∑ –ø—Ä–æ–¥–∞–∂–Ω–æ–≥–æ —Ç–æ–Ω–∞."
                    )
                },
                {"role": "user", "content": prompt}
            ],
            temperature=0.4,
            max_tokens=900,
        )
        core = res.choices[0].message.content.strip()

        if core.startswith('"') and core.endswith('"'):
            core = core[1:-1]
        if core.startswith('¬´') and core.endswith('¬ª'):
            core = core[1:-1]

        length = len(core)
        if length < 250:
            print(f"   ‚ö†Ô∏è –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (len={length}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return None

        if is_too_promotional(core):
            print("   ‚ö†Ô∏è –¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∂—ë—Å—Ç–∫–æ —Ä–µ–∫–ª–∞–º–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return None

        topic = detect_topic(title, summary)
        decorated = decorate_post(core, topic)

        hashtags = get_hashtags(topic)
        hashtag_line = f"\n{hashtags}"
        source_line = f"\n\nüîó <a href=\"{link}\">–ò—Å—Ç–æ—á–Ω–∏–∫</a>"

        final_text = decorated + hashtag_line + source_line

        # –ñ—ë—Å—Ç–∫–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø–æ–¥–ø–∏—Å–∏, —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å Bad Request –æ—Ç Telegram[web:54][web:48]
        if len(final_text) > TELEGRAM_CAPTION_LIMIT:
            print(
                f"   ‚ö†Ô∏è –ü–æ–¥–ø–∏—Å—å –¥–ª–∏–Ω–Ω–µ–µ –ª–∏–º–∏—Ç–∞ (len={len(final_text)}), "
                f"–æ–±—Ä–µ–∑–∞–µ–º –¥–æ {TELEGRAM_CAPTION_LIMIT}"
            )
            final_text = final_text[:TELEGRAM_CAPTION_LIMIT]

        return final_text

    except Exception as e:
        print(f"‚ùå OpenAI –æ—à–∏–±–∫–∞: {e}")
        return None


# ============ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–ê–†–¢–ò–ù–û–ö ============

def generate_image(title: str, max_retries: int = 3) -> Optional[str]:
    image_styles = [
        "minimalist tech illustration, soft gradients, ",
        "abstract geometric visualization, ",
        "clean digital art, modern, ",
        "professional tech render, "
    ]

    style = random.choice(image_styles)

    for attempt in range(max_retries):
        seed = random.randint(0, 10**7)
        clean_title = title[:60].replace('"', '').replace("'", "").replace('\n', ' ')

        prompt = (
            f"{style}{clean_title}, "
            "technology, innovation, "
            "4k, no text, no letters, clean composition"
        )

        try:
            encoded = urllib.parse.quote(prompt)
            url = f"https://image.pollinations.ai/prompt/{encoded}?seed={seed}&width=1024&height=1024&nologo=true"

            print(f"   üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})...")

            resp = requests.get(url, timeout=90, headers=HEADERS)

            if resp.status_code == 200:
                content_type = resp.headers.get('content-type', '')
                if 'image' in content_type and len(resp.content) > 10000:
                    fname = f"img_{seed}.jpg"
                    with open(fname, "wb") as f:
                        f.write(resp.content)
                    print(f"   ‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {fname}")
                    return fname
                else:
                    print(f"   ‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω –Ω–µ–≤–µ—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (size: {len(resp.content)})")
            else:
                print(f"   ‚ö†Ô∏è HTTP {resp.status_code}")

        except requests.Timeout:
            print(f"   ‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        except requests.RequestException as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}")
        except Exception as e:
            print(f"   ‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

        if attempt < max_retries - 1:
            await_time = (attempt + 1) * 2
            print(f"   ‚è≥ –ñ–¥—ë–º {await_time}—Å –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
            import time
            time.sleep(await_time)

    print("   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫")
    return None


def cleanup_image(filepath: Optional[str]) -> None:
    if filepath and os.path.exists(filepath):
        try:
            os.remove(filepath)
        except Exception as e:
            print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {filepath}: {e}")


# ============ –ê–í–¢–û–ü–û–°–¢ ============

async def autopost():
    clean_old_posts()
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π...")
    articles = load_articles_from_sites()
    candidates = filter_articles(articles)

    if not candidates:
        print("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.")
        return

    ai_count = sum(1 for a in candidates if any(
        kw in f"{a['title']} {a['summary']}".lower()
        for kw in AI_KEYWORDS
    ))
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(candidates)} —Å—Ç–∞—Ç–µ–π ({ai_count} –ø—Ä–æ –ò–ò)")

    posted_count = 0
    max_posts = 1

    for art in candidates[:15]:
        if posted_count >= max_posts:
            break

        print(f"\nüîç –û–±—Ä–∞–±–æ—Ç–∫–∞: {art['title'][:60]}... [{art['source']}]")

        post_text = short_summary(art["title"], art["summary"], art["link"])

        if not post_text:
            print("   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é")
            continue

        img = generate_image(art["title"])

        try:
            if img:
                await bot.send_photo(
                    CHANNEL_ID,
                    photo=FSInputFile(img),
                    caption=post_text
                )
            else:
                await bot.send_message(CHANNEL_ID, text=post_text)

            save_posted(art["id"])
            posted_count += 1
            print(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {art['source']}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
        finally:
            cleanup_image(img)

    if posted_count == 0:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞")
    else:
        print(f"\nüéâ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {posted_count}")


async def main():
    try:
        await autopost()
    finally:
        await bot.session.close()


if __name__ == "__main__":
    asyncio.run(main())










































































































