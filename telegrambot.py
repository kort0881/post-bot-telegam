import os
import json
import asyncio
import random
import re
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple

import requests
import feedparser
import urllib.parse
from bs4 import BeautifulSoup
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from openai import OpenAI

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å Copilot SDK
try:
    from github_copilot_sdk import CopilotClient
    COPILOT_SDK_AVAILABLE = True
except ImportError:
    COPILOT_SDK_AVAILABLE = False
    print("‚ö†Ô∏è GitHub Copilot SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenAI API")

# ============ CONFIG ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
USE_COPILOT_SDK = os.getenv("USE_COPILOT_SDK", "false").lower() == "true"

if not all([OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    raise ValueError("‚ùå –ù–µ –≤—Å–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Copilot SDK
copilot_client = None
if COPILOT_SDK_AVAILABLE and USE_COPILOT_SDK:
    try:
        copilot_client = CopilotClient()
        print("‚úÖ GitHub Copilot SDK –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Copilot SDK: {e}")

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
}

CACHE_DIR = os.getenv("CACHE_DIR", "cache_tech")
os.makedirs(CACHE_DIR, exist_ok=True)
STATE_FILE = os.path.join(CACHE_DIR, "state_v2.json") # –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è —Ñ–∞–π–ª–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
FAILED_FILE = os.path.join(CACHE_DIR, "failed_attempts.json")

RETENTION_DAYS = 60 # –•—Ä–∞–Ω–∏–º –∏—Å—Ç–æ—Ä–∏—é 2 –º–µ—Å—è—Ü–∞, —á—Ç–æ–±—ã –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å
MAX_ARTICLE_AGE_DAYS = 2 # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–≤—Å–µ–º —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ (–±—ã–ª–æ 3)
TELEGRAM_CAPTION_LIMIT = 1024

# ============ –ö–ê–¢–ï–ì–û–†–ò–ò –ò–°–¢–û–ß–ù–ò–ö–û–í ============

RSS_SOURCES = [
    {"name": "Habr AI", "url": "https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru", "category": "ai"},
    {"name": "Habr ML", "url": "https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru", "category": "ai"},
    {"name": "NeuroHive", "url": "https://neurohive.io/ru/feed/", "category": "ai"},
    {"name": "Reuters AI", "url": "https://www.reuters.com/technology/artificial-intelligence/rss", "category": "ai"},
    {"name": "Futurism AI", "url": "https://futurism.com/categories/ai-artificial-intelligence/feed", "category": "ai"},
    {"name": "3DNews", "url": "https://3dnews.ru/news/rss/", "category": "tech_ru"},
    {"name": "iXBT", "url": "https://www.ixbt.com/export/news.rss", "category": "tech_ru"},
    {"name": "CNews", "url": "https://www.cnews.ru/inc/rss/news.xml", "category": "tech_ru"},
    {"name": "ComNews", "url": "https://www.comnews.ru/rss", "category": "tech_ru"},
    {"name": "Habr Robotics", "url": "https://habr.com/ru/rss/hub/robotics/all/?fl=ru", "category": "robotics"},
    {"name": "SecurityNews", "url": "https://secnews.ru/rss/", "category": "security"},
    {"name": "CyberAlerts", "url": "https://cyberalerts.io/rss/latest-public", "category": "security"},
]

CATEGORY_ROTATION = ["ai", "tech_ru", "ai", "robotics", "ai", "tech_ru", "security"]

# ============ –°–¢–ò–õ–ò –ü–û–°–¢–û–í ============

POST_STYLES = [
    {
        "name": "–≤–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–π_–≥–∏–∫",
        "intro": "–¢—ã ‚Äî —Ç–µ—Ö–Ω–æ-–≥–∏–∫. –†–∞—Å—Å–∫–∞–∂–∏ –æ –Ω–æ–≤–∏–Ω–∫–µ —Å —ç–Ω—Ç—É–∑–∏–∞–∑–º–æ–º.",
        "tone": "–ñ–∏–≤–æ–π, —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π",
        "emojis": "üî•üöÄüí°ü§ñ‚ú®"
    },
    {
        "name": "—Ñ—É—Ç—É—Ä–∏—Å—Ç",
        "intro": "–¢—ã ‚Äî —Ñ—É—Ç—É—Ä–æ–ª–æ–≥. –û–±—ä—è—Å–Ω–∏, –∫–∞–∫ —ç—Ç–æ –∏–∑–º–µ–Ω–∏—Ç –±—É–¥—É—â–µ–µ.",
        "tone": "–í–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–π",
        "emojis": "üåüüîÆüöÄüåç‚ú®"
    },
    {
        "name": "—Å–∫–µ–ø—Ç–∏–∫",
        "intro": "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫. –†–∞–∑–±–µ—Ä–∏ —Å—É—Ç—å –±–µ–∑ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π —à–µ–ª—É—Ö–∏.",
        "tone": "–°–ø–æ–∫–æ–π–Ω—ã–π, –ø–æ —Ñ–∞–∫—Ç–∞–º",
        "emojis": "üßê‚öôÔ∏èüì±üìä"
    }
]

# ============ –§–ò–õ–¨–¢–†–´ ============

AI_KEYWORDS = [
    "–Ω–µ–π—Ä–æ—Å–µ—Ç", "–∏–∏", "ai", "gpt", "gemini", "claude", "llama",
    "midjourney", "stable diffusion", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è", "—á–∞—Ç-–±–æ—Ç",
    "deepfake", "deep learning", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "copilot", 
    "assistant", "sora", "runway", "pika", "hugging face",
    "nvidia", "cuda", "llm", "rag", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "openai", "anthropic"
]

SENSATIONAL_KEYWORDS = [
    "–≤–∑–ª–æ–º", "—É—Ç–µ—á–∫–∞", "ransomware", "–∞—Ç–∞–∫–∞", "ddos", "0-day",
    "breach", "leak", "hacked", "—É—è–∑–≤–∏–º–æ—Å—Ç—å"
]

EXCLUDE_KEYWORDS = [
    "–∞–∫—Ü–∏–∏", "–±–∏—Ä–∂–∞", "–∏–Ω–≤–µ—Å—Ç–∏—Ü", "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç", "ipo",
    "–≤—ã—Ä—É—á–∫–∞", "–ø—Ä–∏–±—ã–ª—å", "—É–±—ã—Ç–æ–∫", "–¥–∏–≤–∏–¥–µ–Ω–¥—ã",
    "–Ω–∞–∑–Ω–∞—á–µ–Ω", "–æ—Ç—Å—Ç–∞–≤–∫–∞", "—É–≤–æ–ª–µ–Ω", "ceo",
    "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "—Å–ø–æ—Ä—Ç", "–º–∞—Ç—á", "—á–µ–º–ø–∏–æ–Ω–∞—Ç",
    "–ø–æ–ª–∏—Ç–∏–∫–∞", "–≤—ã–±–æ—Ä—ã", "–¥–µ–ø—É—Ç–∞—Ç", "—Å–∞–Ω–∫—Ü–∏–∏", "–∑–∞–∫–æ–Ω",
    "—Å—É–¥", "–∞—Ä–µ—Å—Ç", "–ø—Ä–∏–≥–æ–≤–æ—Ä", "–∫—Ä–∏–º–∏–Ω–∞–ª", "—É–±–∏–π—Å—Ç–≤–æ",
    "covid", "–ø–∞–Ω–¥–µ–º–∏—è", "–≤–∞–∫—Ü–∏–Ω–∞"
]

SOURCE_PROMO_PATTERNS = [
    r"–∫—É–ø–∏(—Ç–µ)?[\s\.,!]", r"–∑–∞–∫–∞–∂–∏(—Ç–µ)?[\s\.,!]", 
    r"—Å–∫–∏–¥–∫[–∞–∏]", r"–ø—Ä–æ–º–æ–∫–æ–¥", r"–∞–∫—Ü–∏—è\b", r"—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞",
    r"–±–µ—Å–ø–ª–∞—Ç–Ω(–æ|—ã–π|–∞—è)", r"–≤—ã–≥–æ–¥(–∞|–Ω–æ)", r"—Ü–µ–Ω–∞ –æ—Ç", 
    r"\d+%\s*(off|—Å–∫–∏–¥–∫)", r"—Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è",
    r"–ø—Ä–µ–¥–∑–∞–∫–∞–∑", r"—Å—Ç–∞—Ä—Ç –ø—Ä–æ–¥–∞–∂", r"–≥–¥–µ –∫—É–ø–∏—Ç—å"
]

def is_source_promotional(title: str, summary: str) -> bool:
    text = f"{title} {summary}".lower()
    for pattern in SOURCE_PROMO_PATTERNS:
        if re.search(pattern, text):
            return True
    return False

def is_excluded(title: str, summary: str) -> Tuple[bool, str]:
    text = f"{title} {summary}".lower()
    for kw in EXCLUDE_KEYWORDS:
        if kw in text:
            return True, f"excluded: {kw}"
    return False, ""

# ============ –£–õ–£–ß–®–ï–ù–ù–û–ï –°–û–°–¢–û–Ø–ù–ò–ï (–ü–£–õ–ï–ù–ï–ü–†–û–ë–ò–í–ê–ï–ú–û–ï) ============

class State:
    def __init__(self):
        self.data = {
            "content_hashes": {}, # –•–µ—à (–ó–∞–≥–æ–ª–æ–≤–æ–∫+–¢–µ–∫—Å—Ç) -> timestamp
            "url_hashes": {},     # –•–µ—à (URL) -> timestamp
            "source_index": 0,
            "last_run": None,
            "failed_attempts": {}
        }
        self._load()
    
    def _load(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, "r", encoding="utf-8") as f:
                    loaded = json.load(f)
                    # –ú–∏–≥—Ä–∞—Ü–∏—è —Å–æ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if "posted_ids" in loaded:
                        self.data["url_hashes"] = {k: v["ts"] for k, v in loaded["posted_ids"].items()}
                    else:
                        self.data.update(loaded)
                print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏—Å—Ç–æ—Ä–∏—è: {len(self.data['content_hashes'])} –∑–∞–ø–∏—Å–µ–π")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
    
    def save(self):
        self.data["last_run"] = datetime.now().isoformat()
        try:
            with open(STATE_FILE, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø: {e}")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ö–µ—à–∞ —Ç–æ–ª—å–∫–æ –∏–∑ –°–ú–´–°–õ–ê (–ó–∞–≥–æ–ª–æ–≤–æ–∫ + –¢–µ–∫—Å—Ç –±–µ–∑ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
    def calculate_content_hash(self, title: str, summary: str) -> str:
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ –±—É–∫–≤ –∏ —Ü–∏—Ñ—Ä, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        clean_string = re.sub(r'[^\w]', '', f"{title}{summary}").lower()
        return hashlib.sha256(clean_string.encode()).hexdigest()

    def calculate_url_hash(self, url: str) -> str:
        return hashlib.sha256(url.encode()).hexdigest()

    def is_duplicate(self, title: str, summary: str, url: str) -> bool:
        content_h = self.calculate_content_hash(title, summary)
        url_h = self.calculate_url_hash(url)
        
        if content_h in self.data["content_hashes"]:
            # print(f"  üîí –î—É–±–ª–∏–∫–∞—Ç –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É: {title[:30]}")
            return True
            
        if url_h in self.data["url_hashes"]:
            # print(f"  üîí –î—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Å—ã–ª–∫–µ: {title[:30]}")
            return True
            
        return False
    
    def mark_posted(self, title: str, summary: str, url: str):
        now_ts = datetime.now().timestamp()
        content_h = self.calculate_content_hash(title, summary)
        url_h = self.calculate_url_hash(url)
        
        self.data["content_hashes"][content_h] = now_ts
        self.data["url_hashes"][url_h] = now_ts
        self.save() # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ù–ï–ú–ï–î–õ–ï–ù–ù–û
    
    def cleanup_old(self):
        cutoff = datetime.now().timestamp() - (RETENTION_DAYS * 86400)
        # –ß–∏—Å—Ç–∏–º –æ–±–∞ —Å–ª–æ–≤–∞—Ä—è
        self.data["content_hashes"] = {k: v for k, v in self.data["content_hashes"].items() if v > cutoff}
        self.data["url_hashes"] = {k: v for k, v in self.data["url_hashes"].items() if v > cutoff}
        self.save()
    
    def get_next_category(self) -> str:
        idx = self.data.get("category_index", 0)
        cat = CATEGORY_ROTATION[idx % len(CATEGORY_ROTATION)]
        self.data["category_index"] = (idx + 1) % len(CATEGORY_ROTATION)
        self.save()
        return cat

state = State()

# ============ PARSING & PROCESSING ============

def clean_text(text: str) -> str:
    if not text: return ""
    return re.sub(r'<[^>]+>', ' ', text).strip()

def apply_social_disclaimer(text: str) -> str:
    targets = ["instagram", "facebook", "tiktok", "–∏–Ω—Å—Ç–∞–≥—Ä–∞–º", "—Ñ–µ–π—Å–±—É–∫", "—Ç–∏–∫—Ç–æ–∫", "meta"]
    if any(t in text.lower() for t in targets):
        return text + "\n\n* <i>Instagram, Facebook –∏ TikTok ‚Äî –∑–∞–ø—Ä–µ—â–µ–Ω—ã –∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–§.</i>"
    return text

def detect_topic(title: str, summary: str) -> str:
    text = f"{title} {summary}".lower()
    if any(kw in text for kw in SENSATIONAL_KEYWORDS): return "sensational"
    if any(kw in text for kw in AI_KEYWORDS): return "ai"
    if any(kw in text for kw in ["—Ä–æ–±–æ—Ç", "robot"]): return "robotics"
    if any(kw in text for kw in ["space", "–∫–æ—Å–º–æ—Å"]): return "space"
    return "tech"

def get_hashtags(topic: str) -> str:
    hashtag_map = {
        "ai": "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "robotics": "#—Ä–æ–±–æ—Ç—ã #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–±—É–¥—É—â–µ–µ",
        "space": "#–∫–æ—Å–º–æ—Å #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "tech": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–Ω–æ–≤–∏–Ω–∫–∏ #–≥–∞–¥–∂–µ—Ç—ã",
        "sensational": "#–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å #–≤–∑–ª–æ–º #—É—Ç–µ—á–∫–∞"
    }
    return hashtag_map.get(topic, "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏")

def build_final_post(text: str, link: str, topic: str) -> str:
    text = apply_social_disclaimer(text)
    hashtags = get_hashtags(topic)
    cta = "\n\nüëç ‚Äî –ø–æ–ª–µ–∑–Ω–æ | üëé ‚Äî –º–∏–º–æ | üî• ‚Äî –æ–≥–æ–Ω—å"
    source = f'\n\nüîó <a href="{link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
    
    full_post = text + cta + "\n\n" + hashtags + source
    
    if len(full_post) > TELEGRAM_CAPTION_LIMIT:
        cut = TELEGRAM_CAPTION_LIMIT - len(cta) - len(hashtags) - len(source) - 100
        text = text[:cut] + "..."
        text = apply_social_disclaimer(text)
        
    return text + cta + "\n\n" + hashtags + source

# ============ RSS LOAD ============

def fetch_full_article(url: str) -> Optional[str]:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        for tag in soup(['script', 'style', 'nav', 'header', 'footer']): tag.decompose()
        content = soup.find('div', class_=re.compile(r'article|content|post|entry'))
        if content: return content.get_text(separator='\n', strip=True)[:4000]
        return None
    except: return None

def load_rss(source: Dict) -> List[Dict]:
    articles = []
    try:
        resp = requests.get(source["url"], headers=HEADERS, timeout=20)
        feed = feedparser.parse(resp.content)
    except: return []
    
    if not feed.entries: return []
    now = datetime.now()
    max_age = timedelta(days=MAX_ARTICLE_AGE_DAYS)
    
    for entry in feed.entries[:30]:
        title = clean_text(entry.get("title", ""))
        link = entry.get("link", "")
        summary = clean_text(entry.get("summary", "") or entry.get("description", ""))

        if not title or not link: continue
        
        # === –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í ===
        if state.is_duplicate(title, summary, link):
            continue
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã
        pub_date = now
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            try: pub_date = datetime(*entry.published_parsed[:6])
            except: pass
        
        if now - pub_date > max_age:
            continue
        
        # –§–∏–ª—å—Ç—Ä—ã
        excluded, _ = is_excluded(title, summary)
        if excluded: continue
        if is_source_promotional(title, summary): continue
        
        articles.append({
            "title": title,
            "summary": summary[:1500],
            "link": link,
            "source": source["name"],
            "category": source["category"],
            "published": pub_date
        })
    
    return articles

# ============ GENERATION ============

async def generate_post_with_copilot_sdk(article: Dict, style: Dict) -> Optional[str]:
    if not copilot_client: return None
    try:
        full_text = fetch_full_article(article["link"])
        content = full_text[:3000] if full_text else article["summary"]
        
        prompt = f"""
{style['intro']}
–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {style['tone']}

–ù–û–í–û–°–¢–¨:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title']}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content}

–°–¢–†–£–ö–¢–£–†–ê:
1. –ó–ê–•–í–ê–¢ ‚Äî –∏–Ω—Ç—Ä–∏–≥—É—é—â–µ–µ –Ω–∞—á–∞–ª–æ
2. –°–£–¢–¨ ‚Äî —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏ –¥–µ—Ç–∞–ª–∏
3. –í–´–í–û–î ‚Äî –ø–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û:
- –ü–∏—à–∏ –°–í–û–ò–ú–ò —Å–ª–æ–≤–∞–º–∏, –Ω–µ –∫–æ–ø–∏—Ä—É–π –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
- –ó–∞–∫–æ–Ω—á–∏ –ø–æ–ª–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º
- –ú–∞–∫—Å–∏–º—É–º 3 —ç–º–æ–¥–∑–∏: {style['emojis']}
"""
        session = copilot_client.create_session(
            system="–¢—ã ‚Äî –∞–≤—Ç–æ—Ä Telegram-–∫–∞–Ω–∞–ª–∞ –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö.",
            temperature=0.8, # –ü–æ–≤—ã—à–∞–µ–º –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            max_tokens=800
        )
        response = await session.send_message(prompt)
        text = response.text.strip().strip('"')
        
        if len(text) < 150: return None
        return build_final_post(text, article["link"], detect_topic(article["title"], article["summary"]))
    except: return None

def generate_post(article: Dict) -> Optional[str]:
    style = random.choice(POST_STYLES)
    
    if copilot_client and USE_COPILOT_SDK:
        print("  ü§ñ SDK...")
        res = asyncio.run(generate_post_with_copilot_sdk(article, style))
        if res: return res
    
    full_text = fetch_full_article(article["link"])
    content = full_text[:3000] if full_text else article["summary"]
    
    prompt = f"""
{style['intro']}
–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {style['tone']}

–ù–û–í–û–°–¢–¨:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title']}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content}

–ó–ê–î–ê–ß–ê:
–ù–∞–ø–∏—à–∏ –£–ù–ò–ö–ê–õ–¨–ù–´–ô –ø–æ—Å—Ç –æ–± —ç—Ç–æ–º —Å–æ–±—ã—Ç–∏–∏.
–ù–µ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–π —Å—É—Ö–æ, –∞ —Å–¥–µ–ª–∞–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –æ–±–∑–æ—Ä.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
‚Ä¢ –û–±—ä—ë–º 600-800 —Å–∏–º–≤–æ–ª–æ–≤
‚Ä¢ –ù–µ –±–æ–ª–µ–µ 3 —ç–º–æ–¥–∑–∏: {style['emojis']}
‚Ä¢ –ó–∞–∫–æ–Ω—á–∏ –º—ã—Å–ª—å
‚Ä¢ –ë–µ–∑ —Ä–µ–∫–ª–∞–º—ã
"""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã ‚Äî –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –∞–≤—Ç–æ—Ä —Ç–µ—Ö–Ω–æ-–±–ª–æ–≥–∞."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.85, # –í—ã—Å–æ–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            max_tokens=800,
        )
        text = response.choices[0].message.content.strip().strip('"')
        if len(text) < 150: return None
        return build_final_post(text, article["link"], detect_topic(article["title"], article["summary"]))
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

# ============ IMAGE ============

def generate_image(title: str) -> Optional[str]:
    styles = [
        "cyberpunk style illustration",
        "futuristic 3d render, neon lighting",
        "minimalist tech art, blue and violet",
        "isometric technology concept"
    ]
    style = random.choice(styles)
    clean_title = re.sub(r'["\'\n]', ' ', title)[:50]
    prompt = f"{style}, {clean_title}, high quality, 4k, no text"
    
    url = f"https://image.pollinations.ai/prompt/{urllib.parse.quote(prompt)}?seed={random.randint(0, 10**7)}&width=1024&height=1024&nologo=true"
    
    try:
        resp = requests.get(url, timeout=60, headers=HEADERS)
        if resp.status_code == 200 and len(resp.content) > 10000:
            fname = f"img_{int(time.time())}.jpg"
            with open(fname, "wb") as f: f.write(resp.content)
            return fname
    except: pass
    return None

def cleanup_image(path):
    if path and os.path.exists(path): os.remove(path)

# ============ MAIN ============

async def autopost():
    state.cleanup_old()
    print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    all_articles = []
    for source in RSS_SOURCES:
        all_articles.extend(load_rss(source))
    
    if not all_articles:
        print("‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö (–Ω–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞–Ω–µ–µ) —Å—Ç–∞—Ç–µ–π")
        return

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    categorized = {"sensational": [], "ai": [], "robotics": [], "tech_ru": [], "security": []}
    
    for art in all_articles:
        topic = detect_topic(art["title"], art["summary"])
        if topic == "sensational": categorized["sensational"].append(art)
        elif art["category"] in categorized: categorized[art["category"]].append(art)
        else: categorized["tech_ru"].append(art)

    # –í—ã–±–æ—Ä –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
    target_category = "sensational" if categorized["sensational"] else state.get_next_category()
    candidates = categorized.get(target_category, [])
    
    if not candidates:
        for cat in ["ai", "tech_ru"]:
            if categorized[cat]: candidates = categorized[cat]; break
            
    if not candidates:
        print("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        return

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Å–≤–µ–∂–∏—Ö –≤–ø–µ—Ä–µ–¥
    candidates.sort(key=lambda x: x["published"], reverse=True)

    print(f"üîÑ –í—ã–±—Ä–∞–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {target_category} (–î–æ—Å—Ç—É–ø–Ω–æ: {len(candidates)})")

    for article in candidates[:5]: # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–≤—ã–µ 5
        print(f"\nüì∞ {article['title'][:50]}...")
        
        # –ü–û–í–¢–û–†–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í –ü–ï–†–ï–î –ì–ï–ù–ï–†–ê–¶–ò–ï–ô
        if state.is_duplicate(article["title"], article["summary"], article["link"]):
            print("  üîí –ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç –≤ –±–∞–∑–µ (race condition check)")
            continue

        post_text = generate_post(article)
        if not post_text: continue
        
        img = generate_image(article["title"])
        
        try:
            if img:
                await bot.send_photo(CHANNEL_ID, photo=FSInputFile(img), caption=post_text)
            else:
                await bot.send_message(CHANNEL_ID, text=post_text)
                
            # –í–ê–ñ–ù–û: –ú–∞—Ä–∫–∏—Ä—É–µ–º –∫–∞–∫ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω–æ–µ –°–†–ê–ó–£
            state.mark_posted(article["title"], article["summary"], article["link"])
            print("‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ!")
            cleanup_image(img)
            return
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
            cleanup_image(img)

async def main():
    try: await autopost()
    finally: await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())














































































































































































































































