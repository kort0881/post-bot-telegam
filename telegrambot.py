import os
import json
import asyncio
import random
import re
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional

import requests
import feedparser
import urllib.parse
from bs4 import BeautifulSoup
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from openai import OpenAI

# ============ COPILOT SDK SETUP ============
try:
    from github_copilot_sdk import CopilotClient
    COPILOT_SDK_AVAILABLE = True
except ImportError:
    COPILOT_SDK_AVAILABLE = False
    print("‚ö†Ô∏è SDK –Ω–µ –Ω–∞–π–¥–µ–Ω, —Ä–∞–±–æ—Ç–∞–µ–º —á–µ—Ä–µ–∑ OpenAI")

# ============ CONFIG ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
# –í–∫–ª—é—á–∞–µ–º SDK —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω
USE_COPILOT_SDK = os.getenv("USE_COPILOT_SDK", "false").lower() == "true" and COPILOT_SDK_AVAILABLE

if not all([OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    raise ValueError("‚ùå –ù–µ –≤—Å–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Copilot Client
copilot_client = None
if USE_COPILOT_SDK:
    try:
        copilot_client = CopilotClient()
        print("ü§ñ Copilot Client –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Copilot: {e}")
        USE_COPILOT_SDK = False

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
}

CACHE_DIR = os.getenv("CACHE_DIR", "cache_tech")
os.makedirs(CACHE_DIR, exist_ok=True)
STATE_FILE = os.path.join(CACHE_DIR, "state_ai_v3.json") # –í–µ—Ä—Å–∏—è 3 —á—Ç–æ–±—ã —Å–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ä—ã–π –∫—ç—à —Å security

RETENTION_DAYS = 60
MAX_ARTICLE_AGE_DAYS = 2
TELEGRAM_CAPTION_LIMIT = 1024

# ============ –ò–°–¢–û–ß–ù–ò–ö–ò (–¢–û–õ–¨–ö–û AI/TECH) ============

RSS_SOURCES = [
    # –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ AI
    {"name": "Habr AI", "url": "https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru", "category": "ai"},
    {"name": "Habr ML", "url": "https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru", "category": "ai"},
    {"name": "NeuroHive", "url": "https://neurohive.io/ru/feed/", "category": "ai"},
    
    # –ê–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ AI (–ø–µ—Ä–µ–≤–µ–¥–µ–º)
    {"name": "OpenAI Blog", "url": "https://openai.com/blog/rss.xml", "category": "ai"},
    {"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed/", "category": "ai"},
    {"name": "The Verge AI", "url": "https://www.theverge.com/rss/artificial-intelligence/index.xml", "category": "ai"},
    
    # –û–±—â–∏–µ –¢–µ—Ö–Ω–æ (—Ñ–∏–ª—å—Ç—Ä—É–µ–º)
    {"name": "3DNews", "url": "https://3dnews.ru/news/rss/", "category": "tech_ru"},
    {"name": "iXBT", "url": "https://www.ixbt.com/export/news.rss", "category": "tech_ru"},
    
    # –†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞
    {"name": "Habr Robotics", "url": "https://habr.com/ru/rss/hub/robotics/all/?fl=ru", "category": "robotics"},
]

# –£–ë–†–ê–õ–ò SECURITY –ò–ó –†–û–¢–ê–¶–ò–ò
CATEGORY_ROTATION = ["ai", "ai", "tech_ru", "ai", "robotics", "ai", "tech_ru"]

# ============ –°–¢–ò–õ–ò –ü–û–°–¢–û–í ============

POST_STYLES = [
    {
        "name": "–≥–∏–∫",
        "intro": "–ù–æ–≤–æ—Å—Ç–∏ –±—É–¥—É—â–µ–≥–æ! ü§ñ",
        "tone": "–≠–Ω–µ—Ä–≥–∏—á–Ω—ã–π, —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π",
        "emojis": "‚ö°Ô∏èüß†üöÄ"
    },
    {
        "name": "–∞–Ω–∞–ª–∏—Ç–∏–∫",
        "intro": "–í–∞–∂–Ω–æ–µ –∏–∑ –º–∏—Ä–∞ AI.",
        "tone": "–°–ø–æ–∫–æ–π–Ω—ã–π, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π",
        "emojis": "üìäüí°üì±"
    }
]

# ============ –§–ò–õ–¨–¢–†–´ ============

# –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å —ç—Ç–∏ —Å–ª–æ–≤–∞ - –≠–¢–û –¢–û–ß–ù–û –ù–ï –î–õ–Ø –≠–¢–û–ì–û –ö–ê–ù–ê–õ–ê
BLOCK_KEYWORDS = [
    "ddos", "—Ö–∞–∫–µ—Ä—ã", "–≤–∑–ª–æ–º", "–∫–∏–±–µ—Ä–º–æ—à–µ–Ω", "—Ñ–∏—à–∏–Ω–≥", "infowatch", 
    "—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "–Ω–∫—Ü–∫–∏", "–≤—Ä–µ–¥–æ–Ω–æ—Å", "—É—è–∑–≤–∏–º–æ—Å—Ç—å", "cve-",
    "–∞–∫—Ü–∏–∏", "–¥–∏–≤–∏–¥–µ–Ω–¥—ã", "—Ü–± —Ä—Ñ", "–∏–Ω—Ñ–ª—è—Ü–∏—è"
]

AI_KEYWORDS = [
    "–Ω–µ–π—Ä–æ—Å–µ—Ç", "–∏–∏", "ai", "gpt", "llm", "diffusion", "genai", 
    "nvidia", "—Ä–æ–±–æ—Ç", "automata", "deepmind", "openai", "sam altman",
    "mask", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–∞–ª–≥–æ—Ä–∏—Ç–º"
]

def is_blocked(title: str, summary: str) -> bool:
    text = f"{title} {summary}".lower()
    for kw in BLOCK_KEYWORDS:
        if kw in text: return True
    return False

# ============ STATE MANAGEMENT ============

class State:
    def __init__(self):
        self.data = {"content_hashes": {}, "url_hashes": {}, "category_index": 0}
        self._load()
    
    def _load(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, "r") as f: self.data.update(json.load(f))
            except: pass
    
    def save(self):
        try:
            with open(STATE_FILE, "w") as f: json.dump(self.data, f, indent=2)
        except: pass
    
    def calculate_hash(self, text: str) -> str:
        return hashlib.sha256(text.strip().lower().encode()).hexdigest()

    def is_duplicate(self, title: str, link: str) -> bool:
        if self.calculate_hash(title) in self.data["content_hashes"]: return True
        if self.calculate_hash(link) in self.data["url_hashes"]: return True
        return False
    
    def mark_posted(self, title: str, link: str):
        ts = datetime.now().timestamp()
        self.data["content_hashes"][self.calculate_hash(title)] = ts
        self.data["url_hashes"][self.calculate_hash(link)] = ts
        self.save()
    
    def cleanup_old(self):
        cutoff = datetime.now().timestamp() - (RETENTION_DAYS * 86400)
        self.data["content_hashes"] = {k: v for k, v in self.data["content_hashes"].items() if v > cutoff}
        self.data["url_hashes"] = {k: v for k, v in self.data["url_hashes"].items() if v > cutoff}
        self.save()
    
    def get_next_category(self) -> str:
        idx = self.data.get("category_index", 0)
        cat = CATEGORY_ROTATION[idx % len(CATEGORY_ROTATION)]
        self.data["category_index"] = (idx + 1) % len(CATEGORY_ROTATION)
        self.save()
        return cat

state = State()

# ============ TEXT TOOLS ============

def clean_text(text: str) -> str:
    return re.sub(r'<[^>]+>', ' ', text).strip() if text else ""

def force_complete_sentence(text: str) -> str:
    """–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if not text: return ""
    # –ï—Å–ª–∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Ç–æ—á–∫—É/–≤–æ—Å–∫–ª/–≤–æ–ø—Ä–æ—Å - –æ–∫
    if text[-1] in ".!?": return text
    
    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É
    last_p = text.rfind('.')
    last_e = text.rfind('!')
    last_q = text.rfind('?')
    
    cut_pos = max(last_p, last_e, last_q)
    
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ
    if cut_pos > len(text) * 0.7:
        return text[:cut_pos+1]
        
    return text.strip() + "."

def build_final_post(text: str, link: str) -> str:
    # 1. –°–Ω–∞—á–∞–ª–∞ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    text = force_complete_sentence(text)
    
    cta = "\n\nüî• ‚Äî –∫—Ä—É—Ç–æ | üëæ ‚Äî –∂—É—Ç–∫–æ"
    source = f'\nüîó <a href="{link}">–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é</a>'
    tags = "\n\n#AI #Tech #–ë—É–¥—É—â–µ–µ #–ù–µ–π—Ä–æ—Å–µ—Ç–∏"
    
    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
    full_len = len(text) + len(cta) + len(source) + len(tags)
    
    if full_len > TELEGRAM_CAPTION_LIMIT:
        # –ï—Å–ª–∏ –Ω–µ –≤–ª–µ–∑–∞–µ—Ç, –æ–±—Ä–µ–∑–∞–µ–º –∂–µ—Å—Ç—á–µ, –Ω–æ —Å–Ω–æ–≤–∞ –∏—â–µ–º —Ç–æ—á–∫—É
        available = TELEGRAM_CAPTION_LIMIT - len(cta) - len(source) - len(tags) - 50
        text = text[:available]
        text = force_complete_sentence(text)
        
    return text + cta + tags + source

# ============ PARSING ============

def fetch_full_article(url: str) -> Optional[str]:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.text, 'html.parser')
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']): tag.decompose()
        
        # –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content = soup.find('div', class_=re.compile(r'article|post-content|entry-content'))
        if content: return content.get_text(separator='\n', strip=True)[:3000]
    except: pass
    return None

def load_rss(source: Dict) -> List[Dict]:
    articles = []
    try:
        resp = requests.get(source["url"], headers=HEADERS, timeout=20)
        feed = feedparser.parse(resp.content)
    except: return []
    
    now = datetime.now()
    for entry in feed.entries[:20]:
        title = clean_text(entry.get("title", ""))
        link = entry.get("link", "")
        summary = clean_text(entry.get("summary", "") or entry.get("description", ""))
        
        if not title or not link: continue
        if state.is_duplicate(title, link): continue
        if is_blocked(title, summary): continue # –ë–ª–æ–∫–∏—Ä—É–µ–º Security —Ç–µ–º—ã
        
        # –î–ª—è Tech_Ru –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ AI
        if source["category"] == "tech_ru":
            full_check = f"{title} {summary}".lower()
            if not any(k in full_check for k in AI_KEYWORDS):
                continue

        pub_date = now
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            try: pub_date = datetime(*entry.published_parsed[:6])
            except: pass
            
        if now - pub_date > timedelta(days=MAX_ARTICLE_AGE_DAYS): continue
        
        articles.append({
            "title": title, 
            "summary": summary[:1500], 
            "link": link, 
            "source": source["name"],
            "published": pub_date
        })
    return articles

# ============ GENERATION ============

async def generate_post(article: Dict, style: Dict) -> Optional[str]:
    full_text = fetch_full_article(article["link"])
    content = full_text if full_text else article["summary"]
    
    prompt = f"""
–¢—ã –≤–µ–¥–µ—à—å Telegram –∫–∞–Ω–∞–ª –ø—Ä–æ –ù–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ AI. –¢–≤–æ—è –∞—É–¥–∏—Ç–æ—Ä–∏—è - –≥–∏–∫–∏ –∏ —ç–Ω—Ç—É–∑–∏–∞—Å—Ç—ã.
–ù–ï –ø–∏—à–∏ –ø—Ä–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –≤–∑–ª–æ–º—ã, –ø–æ–ª–∏—Ç–∏–∫—É. –ü–∏—à–∏ –ø—Ä–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.

–ò—Å—Ç–æ—á–Ω–∏–∫: {article['source']}
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title']}
–¢–µ–∫—Å—Ç: {content}

–¢–í–û–Ø –ó–ê–î–ê–ß–ê:
–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–π, –µ–º–∫–∏–π –ø–æ—Å—Ç (–¥–æ 700 –∑–Ω–∞–∫–æ–≤).
1. –û —á–µ–º —Ä–µ—á—å (—Å—É—Ç—å –Ω–æ–≤–∏–Ω–∫–∏/–æ—Ç–∫—Ä—ã—Ç–∏—è)?
2. –ü–æ—á–µ–º—É —ç—Ç–æ –∫—Ä—É—Ç–æ?
3. –ó–∞–∫–æ–Ω—á–∏ –º—ã—Å–ª—å (–Ω–µ –æ–±—Ä—ã–≤–∞–π —Ç–µ–∫—Å—Ç).

–°—Ç–∏–ª—å: {style['tone']}
–≠–º–æ–¥–∑–∏: –∏—Å–ø–æ–ª—å–∑—É–π 1-3 —à—Ç.
–Ø–∑—ã–∫: –†—É—Å—Å–∫–∏–π.
"""
    
    response_text = None

    # 1. –ü—Ä–æ–±—É–µ–º Copilot SDK
    if USE_COPILOT_SDK and copilot_client:
        try:
            session = copilot_client.create_session(system="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ AI.", temperature=0.7)
            resp = await session.send_message(prompt)
            response_text = resp.text
        except Exception as e:
            print(f"‚ö†Ô∏è Copilot Error: {e}")

    # 2. –ï—Å–ª–∏ –Ω–µ –≤—ã—à–ª–æ - OpenAI
    if not response_text:
        try:
            resp = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=800
            )
            response_text = resp.choices[0].message.content
        except: pass

    if not response_text: return None
    
    # –ß–∏—Å—Ç–∏–º
    clean = response_text.strip().strip('"').replace("**", "")
    return build_final_post(clean, article["link"])

# ============ IMAGE ============

def generate_image(title: str) -> Optional[str]:
    # –î–µ–ª–∞–µ–º –ø—Ä–æ–º–ø—Ç –±–æ–ª–µ–µ "—Ñ—É—Ç—É—Ä–∏—Å—Ç–∏—á–Ω—ã–º"
    prompt = f"futuristic ai technology, neural network visualization, {re.sub(r'[^a-zA-Z]', ' ', title)[:40]}, 3d render, 8k, blue and purple neon light"
    url = f"https://image.pollinations.ai/prompt/{urllib.parse.quote(prompt)}?seed={random.randint(0,10**7)}&width=1024&height=1024&nologo=true"
    try:
        resp = requests.get(url, timeout=30, headers=HEADERS)
        if resp.status_code == 200 and len(resp.content) > 10000:
            fname = f"img_{int(time.time())}.jpg"
            with open(fname, "wb") as f: f.write(resp.content)
            return fname
    except: pass
    return None

def cleanup_image(path):
    if path and os.path.exists(path): os.remove(path)

# ============ MAIN ============

async def autopost():
    state.cleanup_old()
    print("üöÄ [AI Bot] –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –ù–µ–π—Ä–æ—Å–µ—Ç–∏...")
    
    all_articles = []
    
    # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
    target_cat = state.get_next_category()
    print(f"üéØ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ–∏—Å–∫–∞: {target_cat}")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    sources = [s for s in RSS_SOURCES if s["category"] == target_cat]
    
    for source in sources:
        print(f"   –°–∫–∞–Ω–∏—Ä—É—é {source['name']}...")
        found = load_rss(source)
        all_articles.extend(found)

    if not all_articles:
        print("‚ùå –ù–æ–≤–æ—Å—Ç–µ–π –≤ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º –≤—Å–µ...")
        for source in RSS_SOURCES:
            all_articles.extend(load_rss(source))
    
    if not all_articles:
        print("üí§ –í–æ–æ–±—â–µ –ø—É—Å—Ç–æ.")
        return

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–≤–µ–∂–∏–µ —Å–≤–µ—Ä—Ö—É
    all_articles.sort(key=lambda x: x["published"], reverse=True)
    
    for article in all_articles[:10]:
        print(f"\nüìù –û–±—Ä–∞–±–æ—Ç–∫–∞: {article['title'][:40]}...")
        
        post_text = await generate_post(article, random.choice(POST_STYLES))
        if not post_text: continue
        
        img = generate_image(article["title"])
        try:
            if img: await bot.send_photo(CHANNEL_ID, photo=FSInputFile(img), caption=post_text)
            else: await bot.send_message(CHANNEL_ID, text=post_text)
            
            state.mark_posted(article["title"], article["link"])
            print("‚úÖ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û!")
            cleanup_image(img)
            return # –£—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ 1 –ø–æ—Å—Ç–∞
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Telegram: {e}")
            cleanup_image(img)

async def main():
    try: await autopost()
    finally: await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())



















































































































































































































































