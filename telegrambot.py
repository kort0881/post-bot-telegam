import os
import json
import asyncio
from datetime import datetime

import requests
from aiogram import Bot
from aiogram.client.bot import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile

from openai import OpenAI

# ===== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ =====
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

print("DEBUG OPENAI KEY LEN:", len(OPENAI_API_KEY) if OPENAI_API_KEY else 0)

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)

openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

# ===== –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ =====
STRONG_KEYWORDS = [
    "vpn", "–≤–ø–Ω", "–ø—Ä–æ–∫—Å–∏", "proxy", "tor", "shadowsocks",
    "wireguard", "openvpn", "ikev2",
    "–æ–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫", "–æ–±—Ö–æ–¥ —Ü–µ–Ω–∑—É—Ä—ã", "–∞–Ω–æ–Ω–∏–º–Ω–æ—Å—Ç—å",
    "—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "—Ä–∫–Ω",
    "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ü–µ–Ω–∑—É—Ä–∞", "—Ü–µ–Ω–∑—É—Ä–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ",
    "–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–∞–π—Ç–æ–≤", "–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ä–µ—Å—É—Ä—Å–∞",
    "—Ä–µ–µ—Å—Ç—Ä –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–∞",
    "—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç—Ä–∞—Ñ–∏–∫–∞", "dpi", "deep packet inspection",
    "—Ç–µ–ª–µ–≥—Ä–∞–º", "telegram",
    "whatsapp", "signal", "viber",
    "messenger", "–º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä",
    "–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "–ø–∞—Ç—á –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
    "–∞–Ω—Ç–∏–≤–∏—Ä—É—Å", "firewall", "—Ñ–∞–µ—Ä–≤–æ–ª",
    "–±—Ä–∞—É–∑–µ—Ä", "–±—Ä–∞—É–∑–µ—Ä tor",
    "–∫–ª–∏–µ–Ω—Ç vpn", "vpn-–∫–ª–∏–µ–Ω—Ç",
]

SOFT_KEYWORDS = [
    "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø–∫", "desktop-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—É—Ç–∏–ª–∏—Ç–∞ –¥–ª—è windows",
    "–ø—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è macos", "open source",
    "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
    "ai", "machine learning",
    "–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
    "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ", "privacy",
    "—Å—É–≤–µ—Ä–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞",
]

EXCLUDE_KEYWORDS = [
    "–∏–≥—Ä–∞", "–∏–≥—Ä—ã", "game", "games", "–≥–µ–π–º–ø–ª–µ–π", "gameplay",
    "dungeon", "quest", "–±–æ—Å—Å", "boss", "—Ä–µ–π–¥", "raid",
    "–æ–Ω–ª–∞–π–Ω-–∏–≥—Ä–∞", "–∏–≥—Ä–æ–≤–æ–π", "–≥–µ–π–º–∏–Ω–≥", "gaming",
    "playstation", "xbox", "nintendo", "steam",
    "—à—É—Ç–µ—Ä", "rpg", "mmorpg", "moba", "battle royale",
]

POSTED_FILE = "posted_articles.json"

if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        posted_articles = set(json.load(f))
else:
    posted_articles = set()

def save_posted(article_id: str) -> None:
    posted_articles.add(article_id)
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(list(posted_articles), f, ensure_ascii=False, indent=2)

def safe_get(url: str) -> str | None:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            print(f"HTTP {resp.status_code} –¥–ª—è {url}")
            return None
        return resp.text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}:", e)
        return None

def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())

# ===== –ü–∞—Ä—Å–∏–Ω–≥ =====
def load_3dnews():
    url = "https://3dnews.ru/"
    html = safe_get(url)
    if not html:
        return []

    articles = []
    parts = html.split('<a href="/')
    for part in parts[1:6]:
        href_end = part.find('"')
        title_start = part.find(">")
        title_end = part.find("</a>")
        if href_end == -1 or title_start == -1 or title_end == -1:
            continue

        href = part[:href_end]
        title = clean_text(part[title_start + 1:title_end])
        if not title:
            continue

        link = "https://3dnews.ru/" + href.lstrip("/")
        
        summary = ""
        desc_start = part.find('class="')
        if desc_start != -1:
            desc_chunk = part[desc_start:desc_start+500]
            p_start = desc_chunk.find(">")
            if p_start != -1:
                p_end = desc_chunk.find("</", p_start)
                if p_end != -1:
                    summary = clean_text(desc_chunk[p_start+1:p_end])[:300]

        articles.append({
            "id": link,
            "title": title,
            "summary": summary,
            "link": link,
            "published_parsed": datetime.now(),
        })

    print("DEBUG: —Å—Ç–∞—Ç–µ–π –∏–∑ 3DNews:", len(articles))
    return articles

def load_habr():
    url = "https://habr.com/ru/feed/"
    html = safe_get(url)
    if not html:
        return []

    articles = []
    chunks = html.split("<article")
    for chunk in chunks[1:6]:
        title_marker = 'data-test-id="article-title-link"'
        idx = chunk.find(title_marker)
        if idx == -1:
            continue
        sub = chunk[idx:]
        href_pos = sub.find('href="')
        if href_pos == -1:
            continue
        href_start = href_pos + len('href="')
        href_end = sub.find('"', href_start)
        href = sub[href_start:href_end]

        title_start = sub.find(">", href_end) + 1
        title_end = sub.find("</a>", title_start)
        title = clean_text(sub[title_start:title_end])

        link = "https://habr.com" + href

        p_start = chunk.find("<p")
        if p_start != -1:
            p_start = chunk.find(">", p_start) + 1
            p_end = chunk.find("</p>", p_start)
            summary = clean_text(chunk[p_start:p_end])[:300]
        else:
            summary = ""

        articles.append({
            "id": link,
            "title": title,
            "summary": summary,
            "link": link,
            "published_parsed": datetime.now(),
        })

    print("DEBUG: —Å—Ç–∞—Ç–µ–π –∏–∑ –•–∞–±—Ä–∞:", len(articles))
    return articles

def load_tproger():
    url = "https://tproger.ru/news"
    html = safe_get(url)
    if not html:
        return []

    articles = []
    parts = html.split('<a ')
    for part in parts[1:6]:
        if "href=" not in part or "news" not in part:
            continue

        href_pos = part.find('href="')
        href_start = href_pos + len('href="')
        href_end = part.find('"', href_start)
        href = part[href_start:href_end]

        title_start = part.find(">", href_end) + 1
        title_end = part.find("</a>", title_start)
        title = clean_text(part[title_start:title_end])
        if not title:
            continue

        if href.startswith("http"):
            link = href
        else:
            link = "https://tproger.ru" + href

        summary = ""

        articles.append({
            "id": link,
            "title": title,
            "summary": summary,
            "link": link,
            "published_parsed": datetime.now(),
        })

    print("DEBUG: —Å—Ç–∞—Ç–µ–π –∏–∑ Tproger:", len(articles))
    return articles

def load_articles_from_sites():
    articles = []
    articles.extend(load_3dnews())
    articles.extend(load_habr())
    articles.extend(load_tproger())
    print("DEBUG: –≤—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π:", len(articles))
    return articles

def filter_article(entry):
    title = entry.get("title", "")
    summary = entry.get("summary", "")
    text = (title + " " + summary).lower()

    if any(kw.lower() in text for kw in EXCLUDE_KEYWORDS):
        return None

    if any(kw.lower() in text for kw in STRONG_KEYWORDS):
        return "strong"
    if any(kw.lower() in text for kw in SOFT_KEYWORDS):
        return "soft"
    return None

def pick_article(articles):
    scored = []
    for e in articles:
        level = filter_article(e)
        if not level:
            continue
        score = 2 if level == "strong" else 1
        scored.append((score, e))

    if scored:
        scored.sort(
            key=lambda x: (x[0], x[1].get("published_parsed", datetime.now())),
            reverse=True,
        )
        return scored[0][1]
    return None

# ===== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ =====
def short_summary(title: str, summary: str) -> str:
    news_text = f"{title}. {summary}" if summary else title
    
    prompt = (
        f"–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –Ω–æ–≤–æ—Å—Ç—å –≤ —Å—Ç–∏–ª–µ Telegram-–∫–∞–Ω–∞–ª–∞:\n\n"
        f"{news_text}\n\n"
        f"–ü–†–ê–í–ò–õ–ê:\n"
        f"1. –ü–∏—à–∏ –ö–û–ù–ö–†–ï–¢–ù–û ‚Äî –µ—Å–ª–∏ —É–ø–æ–º—è–Ω—É—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏/–ø—Ä–æ–¥—É–∫—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ! –ù–ï –ø–∏—à–∏ '–ö–æ–º–ø–∞–Ω–∏—è X' –∏–ª–∏ '–ü—Ä–æ–¥—É–∫—Ç Y'.\n"
        f"2. –ï—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Ä—Å–∏–∏, —Ü–∏—Ñ—Ä—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π.\n"
        f"3. –ü–∏—à–∏ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –±–µ–∑ –∫–∞–Ω—Ü–µ–ª—è—Ä–∏—Ç–∞.\n"
        f"4. –û–±—ä—ë–º: 650-700 —Å–∏–º–≤–æ–ª–æ–≤ (–±–µ–∑ PS).\n"
        f"5. –§–æ—Ä–º–∞—Ç:\n"
        f"   [—ç–º–æ–¥–∑–∏] –°—Ç—Ä–æ–∫–∞ 1: —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ\n"
        f"   [—ç–º–æ–¥–∑–∏] –°—Ç—Ä–æ–∫–∞ 2: –∫–∞–∫–∞—è –±—ã–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞\n"
        f"   [—ç–º–æ–¥–∑–∏] –°—Ç—Ä–æ–∫–∞ 3: —á—Ç–æ —É–ª—É—á—à–∏–ª–æ—Å—å\n"
        f"   [—ç–º–æ–¥–∑–∏] –°—Ç—Ä–æ–∫–∞ 4: –∑–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ\n"
        f"   \n"
        f"   PSüí• –ö—Ç–æ –∑–∞ –∫–ª—é—á–∞–º–∏ üëâ https://t.me/+EdEfIkn83Wg3ZTE6\n\n"
        f"–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞!"
    )
    
    result = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
    )
    return result.choices[0].message.content.strip()[:850]

# ===== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–∫–∏ =====
def generate_image_prompt(title: str, summary: str) -> str:
    prompt = (
        f"Create a short image prompt for: {title}. "
        f"Style: cinematic realistic, dramatic lighting, dark tech atmosphere, high detail. "
        f"Focus on technology/cybersecurity/internet themes. No text, no logos. Max 200 chars."
    )
    result = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
    )
    return result.choices[0].message.content.strip()[:200]

# ===== POLLINATIONS.AI - –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–æ–∫ =====
def generate_image_pollinations(prompt: str) -> str | None:
    try:
        print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Pollinations: {prompt}")
        
        # URL –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É
        url = f"https://image.pollinations.ai/prompt/{requests.utils.quote(prompt)}"
        params = {
            "width": "1024",
            "height": "1024",
            "nologo": "true",
            "model": "flux"
        }
        
        response = requests.get(url, params=params, timeout=60)
        
        if response.status_code == 200:
            filename = f"news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"‚úÖ –ö–∞—Ä—Ç–∏–Ω–∫–∞: {filename}")
            return filename
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ Pollinations: {response.status_code}")
            return None
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return None

# ===== –ê–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ =====
async def autopost():
    articles = load_articles_from_sites()

    if not articles:
        print("–ù–µ—Ç —Å—Ç–∞—Ç–µ–π")
        return

    art = pick_article(articles)

    if not art:
        print("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç–∞—Ç–µ–π")
        return

    article_id = art.get("id", art.get("link"))
    if article_id in posted_articles:
        print("–£–∂–µ –±—ã–ª–∞ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞")
        return

    title = art.get("title", "")
    summary = art.get("summary", "")[:400]

    print(f"\n{'='*60}")
    print(f"–í–´–ë–†–ê–ù–ê: {title}")
    print(f"–û–ü–ò–°–ê–ù–ò–ï: {summary}")
    print(f"–°–°–´–õ–ö–ê: {art.get('link')}")
    print(f"{'='*60}\n")

    news = short_summary(title, summary)
    print(f"–¢–ï–ö–°–¢ ({len(news)} —Å–∏–º–≤.):\n{news}\n{'='*60}\n")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏ —á–µ—Ä–µ–∑ Pollinations
    image_prompt = generate_image_prompt(title, summary)
    image_file = generate_image_pollinations(image_prompt)

    all_keywords = STRONG_KEYWORDS + SOFT_KEYWORDS
    text_for_tags = (title + " " + summary).lower()
    hashtags = [f"#{kw.replace(' ', '')}" for kw in all_keywords if kw.lower() in text_for_tags]
    hashtags += ["#–ù–æ–≤–æ—Å—Ç–∏", "#Telegram", "#–ö–∞–Ω–∞–ª"]

    caption = f"{news}\n\n{' '.join(hashtags)}"

    # –û—Ç–ø—Ä–∞–≤–∫–∞
    if image_file and os.path.exists(image_file):
        photo = FSInputFile(image_file)
        await bot.send_photo(CHANNEL_ID, photo=photo, caption=caption)
        os.remove(image_file)
        print("‚úÖ –ü–æ—Å—Ç —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω!")
    else:
        await bot.send_message(CHANNEL_ID, caption)
        print("‚ö†Ô∏è –ü–æ—Å—Ç –±–µ–∑ –∫–∞—Ä—Ç–∏–Ω–∫–∏")

    save_posted(article_id)
    print(f"[OK] {datetime.now()}")

async def main():
    await autopost()

if __name__ == "__main__":
    asyncio.run(main())























