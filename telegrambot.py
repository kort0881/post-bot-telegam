import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional

import requests
import feedparser
from aiogram import Bot
from aiogram.client.bot import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile

from openai import OpenAI

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

print("DEBUG OPENAI KEY LEN:", len(OPENAI_API_KEY) if OPENAI_API_KEY else 0)

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)

openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

STRONG_KEYWORDS = [
    "vpn", "–≤–ø–Ω", "–ø—Ä–æ–∫—Å–∏", "proxy", "tor", "shadowsocks",
    "wireguard", "openvpn", "ikev2",
    "–æ–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫", "–æ–±—Ö–æ–¥ —Ü–µ–Ω–∑—É—Ä—ã", "–∞–Ω–æ–Ω–∏–º–Ω–æ—Å—Ç—å",
    "—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "—Ä–∫–Ω",
    "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ü–µ–Ω–∑—É—Ä–∞", "—Ü–µ–Ω–∑—É—Ä–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ",
    "–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–∞–π—Ç–æ–≤", "–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ä–µ—Å—É—Ä—Å–∞",
    "—Ä–µ–µ—Å—Ç—Ä –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–∞",
    "—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç—Ä–∞—Ñ–∏–∫–∞", "dpi", "deep packet inspection",
    "—Ç–µ–ª–µ–≥—Ä–∞–º", "telegram",
    "whatsapp", "signal", "viber",
    "messenger", "–º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä",
    "–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "–ø–∞—Ç—á –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
    "–∞–Ω—Ç–∏–≤–∏—Ä—É—Å", "firewall", "—Ñ–∞–µ—Ä–≤–æ–ª",
    "–±—Ä–∞—É–∑–µ—Ä", "–±—Ä–∞—É–∑–µ—Ä tor",
    "–∫–ª–∏–µ–Ω—Ç vpn", "vpn-–∫–ª–∏–µ–Ω—Ç",
    "–º–∏–Ω—Ü–∏—Ñ—Ä—ã", "–º–∏–Ω—Ü–∏—Ñ—Ä—ã —Ä—Ñ",
    "–±–µ–ª—ã–µ —Å–ø–∏—Å–∫–∏", "–±–µ–ª—ã–π —Å–ø–∏—Å–æ–∫",
]

SOFT_KEYWORDS = [
    "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø–∫", "desktop-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—É—Ç–∏–ª–∏—Ç–∞ –¥–ª—è windows",
    "–ø—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è macos", "open source",
    "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
    "ai", "machine learning",
    "–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
    "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ", "privacy",
    "—Å—É–≤–µ—Ä–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞",
]

EXCLUDE_KEYWORDS = [
    "–∏–≥—Ä–∞", "–∏–≥—Ä—ã", "game", "games", "–≥–µ–π–º–ø–ª–µ–π", "gameplay",
    "dungeon", "quest", "–±–æ—Å—Å", "boss", "—Ä–µ–π–¥", "raid",
    "–æ–Ω–ª–∞–π–Ω-–∏–≥—Ä–∞", "–∏–≥—Ä–æ–≤–æ–π", "–≥–µ–π–º–∏–Ω–≥", "gaming",
    "playstation", "xbox", "nintendo", "steam",
    "—à—É—Ç–µ—Ä", "rpg", "mmorpg", "moba", "battle royale",
]

POSTED_FILE = "posted_articles.json"
RETENTION_DAYS = 7

if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        try:
            posted_data = json.load(f)
            if isinstance(posted_data, list) and posted_data and isinstance(posted_data[0], dict):
                posted_articles = {item["id"]: item.get("timestamp") for item in posted_data}
            else:
                posted_articles = {id_str: None for id_str in posted_data}
        except Exception:
            posted_articles = {}
else:
    posted_articles = {}


def clean_old_posts() -> None:
    global posted_articles
    now = datetime.now().timestamp()
    cutoff = now - (RETENTION_DAYS * 86400)
    
    old_count = len(posted_articles)
    posted_articles = {
        id_str: ts for id_str, ts in posted_articles.items()
        if ts is None or ts > cutoff
    }
    removed = old_count - len(posted_articles)
    if removed > 0:
        print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö –ø–æ—Å—Ç–æ–≤: {removed}")
    
    save_posted_articles()


def save_posted_articles() -> None:
    data = [
        {"id": id_str, "timestamp": ts}
        for id_str, ts in posted_articles.items()
    ]
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def save_posted(article_id: str) -> None:
    posted_articles[article_id] = datetime.now().timestamp()
    save_posted_articles()


def safe_get(url: str) -> Optional[str]:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            print(f"HTTP {resp.status_code} –¥–ª—è {url}")
            return None
        return resp.text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}:", e)
        return None


def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())


def load_3dnews() -> List[Dict]:
    url = "https://3dnews.ru/"
    html = safe_get(url)
    if not html:
        return []

    articles: List[Dict] = []
    parts = html.split('<a href="/')

    for part in parts[1:4]:
        href_end = part.find('"')
        title_start = part.find(">")
        title_end = part.find("</a>")
        if href_end == -1 or title_start == -1 or title_end == -1:
            continue

        href = part[:href_end]
        title = clean_text(part[title_start + 1:title_end])
        if not title:
            continue

        link = "https://3dnews.ru/" + href.lstrip("/")

        summary = ""
        desc_start = part.find('class="')
        if desc_start != -1:
            desc_chunk = part[desc_start:desc_start + 500]
            p_start = desc_chunk.find(">")
            if p_start != -1:
                p_end = desc_chunk.find("</", p_start)
                if p_end != -1:
                    summary = clean_text(desc_chunk[p_start + 1:p_end])[:300]

        articles.append({
            "id": link,
            "title": title,
            "summary": summary,
            "link": link,
            "source": "3DNews",
            "published_parsed": datetime.now(),
        })

    print(f"DEBUG: 3DNews - {len(articles)} —Å—Ç–∞—Ç–µ–π")
    return articles


VC_RU_FEED = "https://vc.ru/rss"


def load_vcru_from_rss() -> List[Dict]:
    articles: List[Dict] = []

    print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º RSS VC.ru: {VC_RU_FEED}")
    try:
        feed = feedparser.parse(VC_RU_FEED)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ RSS {VC_RU_FEED}: {e}")
        return articles

    for entry in feed.entries[:30]:
        link = entry.get("link", "")
        title = clean_text(entry.get("title", "") or "")
        summary = clean_text(
            entry.get("summary", "") or entry.get("description", "") or ""
        )[:400]

        if not link or not title:
            continue

        published_parsed = datetime.now()
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            try:
                published_parsed = datetime(*entry.published_parsed[:6])
            except Exception:
                pass

        articles.append({
            "id": link,
            "title": title,
            "summary": summary,
            "link": link,
            "source": "VC.ru/rss",
            "published_parsed": published_parsed,
        })

    print(f"DEBUG: VC.ru RSS - {len(articles)} —Å—Ç–∞—Ç–µ–π")
    return articles


def load_articles_from_sites() -> List[Dict]:
    articles: List[Dict] = []
    articles.extend(load_3dnews())
    articles.extend(load_vcru_from_rss())

    print(f"\n{'=' * 60}")
    print(f"–í–°–ï–ì–û –°–ü–ê–†–°–ï–ù–û: {len(articles)} —Å—Ç–∞—Ç–µ–π")
    print(f"–í –ø–∞–º—è—Ç–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {len(posted_articles)} —Å—Ç–∞—Ç–µ–π")
    print(f"{'=' * 60}")
    for i, art in enumerate(articles, 1):
        print(f"{i}. [{art['source']}] {art['title'][:80]}")
    print(f"{'=' * 60}\n")

    return articles


def filter_article(entry: Dict) -> Optional[str]:
    title = entry.get("title", "")
    summary = entry.get("summary", "")
    text = (title + " " + summary).lower()

    if any(kw.lower() in text for kw in EXCLUDE_KEYWORDS):
        return None

    if any(kw.lower() in text for kw in STRONG_KEYWORDS):
        return "strong"
    if any(kw.lower() in text for kw in SOFT_KEYWORDS):
        return "soft"
    return None


def pick_article(articles: List[Dict]) -> Optional[Dict]:
    scored = []
    skipped_count = 0
    
    for e in articles:
        article_id = e.get("id", e.get("link"))
        if article_id in posted_articles:
            skipped_count += 1
            continue

        level = filter_article(e)
        if not level:
            continue

        score = 2 if level == "strong" else 1
        scored.append((score, e))

    print(f"–ü–†–û–ü–£–©–ï–ù–û –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–ù–´–•: {skipped_count}")
    print(f"–ü–û–î–•–û–î–Ø–©–ò–• –ù–û–í–´–• –°–¢–ê–¢–ï–ô: {len(scored)}")
    for i, (score, art) in enumerate(scored[:5], 1):
        level = "STRONG" if score == 2 else "SOFT"
        print(f"{i}. [{level}] [{art['source']}] {art['title'][:80]}")
    print(f"{'=' * 60}\n")

    if scored:
        scored.sort(
            key=lambda x: (
                x[0],
                x[1].get("published_parsed", datetime.now())
            ),
            reverse=True,
        )
        return scored[0][1]

    return None


def short_summary(title: str, summary: str) -> str:
    news_text = f"{title}. {summary}" if summary else title

    prompt = (
        f"–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –Ω–æ–≤–æ—Å—Ç—å –≤ —Å—Ç–∏–ª–µ Telegram-–∫–∞–Ω–∞–ª–∞:\n\n"
        f"{news_text}\n\n"
        f"–ü–†–ê–í–ò–õ–ê:\n"
        f"1. –ü–∏—à–∏ –ö–û–ù–ö–†–ï–¢–ù–û ‚Äî –µ—Å–ª–∏ —É–ø–æ–º—è–Ω—É—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏/–ø—Ä–æ–¥—É–∫—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ! –ù–ï –ø–∏—à–∏ '–ö–æ–º–ø–∞–Ω–∏—è X' –∏–ª–∏ '–ü—Ä–æ–¥—É–∫—Ç Y'.\n"
        f"2. –ï—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Ä—Å–∏–∏, —Ü–∏—Ñ—Ä—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π.\n"
        f"3. –ü–∏—à–∏ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –±–µ–∑ –∫–∞–Ω—Ü–µ–ª—è—Ä–∏—Ç–∞.\n"
        f"4. –û–±—ä—ë–º –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: –ø—Ä–∏–º–µ—Ä–Ω–æ 400‚Äì600 —Å–∏–º–≤–æ–ª–æ–≤.\n"
        f"5. –§–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–≥–æ —Ç–∞–∫–æ–π:\n"
        f"   [—ç–º–æ–¥–∂–∏] –°—Ç—Ä–æ–∫–∞ 1: —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ\n"
        f"   [—ç–º–æ–¥–∂–∏] –°—Ç—Ä–æ–∫–∞ 2: –∫–∞–∫–∞—è –±—ã–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞\n"
        f"   [—ç–º–æ–¥–∂–∏] –°—Ç—Ä–æ–∫–∞ 3: —á—Ç–æ —É–ª—É—á—à–∏–ª–æ—Å—å\n"
        f"   [—ç–º–æ–¥–∂–∏] –°—Ç—Ä–æ–∫–∞ 4: –∑–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ\n"
        f"   –ü–£–°–¢–ê–Ø –°–¢–†–û–ö–ê\n"
        f"   PSüí• –ö—Ç–æ –∑–∞ –∫–ª—é—á–∞–º–∏ üëâ https://t.me/+EdEfIkn83Wg3ZTE6\n\n"
        f"–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ —Ü–µ–ª–∏–∫–æ–º, –≤–∫–ª—é—á–∞—è —Å—Ç—Ä–æ–∫—É PS."
    )

    result = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
    )
    text = result.choices[0].message.content.strip()

    ps = "PSüí• –ö—Ç–æ –∑–∞ –∫–ª—é—á–∞–º–∏ üëâ https://t.me/+EdEfIkn83Wg3ZTE6"
    if ps not in text:
        text = f"{text.rstrip()}\n\n{ps}"

    return text


def generate_image_prompt(title: str, summary: str) -> str:
    base_prompt = (
        f"Create a short image prompt for: {title}. "
        f"Style: cinematic realistic, dramatic lighting, dark tech atmosphere, high detail. "
        f"Focus on technology/cybersecurity/internet themes. No text, no logos. Max 200 chars."
    )
    result = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": base_prompt}],
    )
    return result.choices[0].message.content.strip()[:200]


def generate_image_pollinations(prompt: str) -> Optional[str]:
    try:
        print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Pollinations: {prompt}")

        url = f"https://image.pollinations.ai/prompt/{requests.utils.quote(prompt)}"
        params = {
            "width": "1024",
            "height": "1024",
            "nologo": "true",
            "model": "flux",
        }

        response = requests.get(url, params=params, timeout=60)

        if response.status_code == 200:
            filename = f"news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"‚úÖ –ö–∞—Ä—Ç–∏–Ω–∫–∞: {filename}")
            return filename
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ Pollinations: {response.status_code}")
            return None
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return None


async def autopost():








































