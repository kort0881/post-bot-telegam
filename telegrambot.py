import os
import json
import asyncio
import random
import re
import hashlib
import logging
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urlparse, quote

import aiohttp
import feedparser
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq

# ====================== –õ–û–ì–ò ======================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("ai_poster.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ====================== CONFIG ======================
class Config:
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.telegram_token = os.getenv("TELEGRAM_BOT_TOKEN")
        self.channel_id = os.getenv("CHANNEL_ID")
        self.retention_days = int(os.getenv("RETENTION_DAYS", "30"))
        self.caption_limit = 1024
        self.posted_file = "posted_articles.json"

        missing = []
        for var, name in [(self.groq_api_key, "GROQ_API_KEY"),
                          (self.telegram_token, "TELEGRAM_BOT_TOKEN"),
                          (self.channel_id, "CHANNEL_ID")]:
            if not var:
                missing.append(name)
        if missing:
            raise SystemExit(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {', '.join(missing)}")

config = Config()

bot = Bot(token=config.telegram_token, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=config.groq_api_key)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0 Safari/537.36"
}

# ====================== RSS ======================
RSS_FEEDS = [
    ("https://techcrunch.com/category/artificial-intelligence/feed/", "TechCrunch AI"),
    ("https://venturebeat.com/category/ai/feed/", "VentureBeat AI"),
    ("https://www.technologyreview.com/feed/topic/artificial-intelligence", "MIT Tech Review"),
    ("https://www.theverge.com/ai-artificial-intelligence/rss/index.xml", "The Verge AI"),
    ("https://arstechnica.com/tag/artificial-intelligence/feed/", "Ars Technica AI"),
    ("https://www.wired.com/feed/tag/ai/latest/rss", "WIRED AI"),
]

# ====================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ======================
AI_KEYWORDS = [
    "ai ", " ai", "artificial intelligence", "machine learning", "deep learning", "neural network",
    "llm", "large language model", "gpt", "chatgpt", "claude", "gemini", "grok", "llama",
    "mistral", "qwen", "deepseek", "midjourney", "dall-e", "stable diffusion", "sora", 
    "groq", "openai", "anthropic", "deepmind", "hugging face", "nvidia", "agi", 
    "inference", "rlhf", "transformer", "generative", "chatbot"
]

EXCLUDE_KEYWORDS = [
    "stock price", "ipo", "earnings call", "quarterly results", "revenue beat", "profit margin", 
    "dividend", "market cap", "wall street",
    "ps5", "xbox", "nintendo switch", "game review", "gameplay", "gaming pc",
    "netflix series", "movie review", "box office", "trailer", "premiere",
    "tesla stock", "ev sales", "model 3", "model y", "cybertruck",
    "bitcoin", "crypto", "blockchain", "nft", "ethereum",
    "election", "trump", "biden", "congress", "senate", "white house"
]

BAD_PHRASES = ["sponsored", "partner content", "advertisement", "black friday", "deal alert", "coupon"]

# ====================== DATACLASSES ======================
from dataclasses import dataclass, field

@dataclass
class Article:
    title: str
    summary: str
    link: str
    source: str
    published: datetime = field(default_factory=datetime.utcnow)

# ====================== TOPIC & HASHTAGS ======================
class Topic:
    LLM = "llm"
    IMAGE_GEN = "image_gen"
    ROBOTICS = "robotics"
    HARDWARE = "hardware"
    GENERAL = "general"
    
    HASHTAGS = {
        LLM: "#ChatGPT #LLM #OpenAI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        IMAGE_GEN: "#Midjourney #DALLE #StableDiffusion #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
        ROBOTICS: "#—Ä–æ–±–æ—Ç—ã #Humanoid #—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
        HARDWARE: "#NVIDIA #GPU #—á–∏–ø—ã #–∂–µ–ª–µ–∑–æ",
        GENERAL: "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
    }

    @staticmethod
    def detect(text: str) -> str:
        t = text.lower()
        if any(x in t for x in ["gpt", "chatgpt", "claude", "gemini", "llama", "grok", "llm", "language model"]):
            return Topic.LLM
        if any(x in t for x in ["midjourney", "dall-e", "dalle", "stable diffusion", "flux", "image gen", "sora"]):
            return Topic.IMAGE_GEN
        if any(x in t for x in ["robot", "humanoid", "boston dynamics", "optimus", "figure ai"]):
            return Topic.ROBOTICS
        if any(x in t for x in ["nvidia", "h100", "h200", "blackwell", "gpu", "tensor core", "cuda"]):
            return Topic.HARDWARE
        return Topic.GENERAL

# ====================== HELPERS ======================
def normalize_url(url: str) -> str:
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        path = parsed.path.rstrip("/")
        domain = parsed.netloc.lower().replace("www.", "")
        return f"{domain}{path}".split("?")[0].split("#")[0]
    except:
        return url.split("?")[0].split("#")[0]

def article_id(url: str) -> str:
    return hashlib.md5(normalize_url(url).encode()).hexdigest()[:16]

def clean_text(text: str) -> str:
    if not text:
        return ""
    # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
    text = re.sub(r'<[^>]+>', '', text)
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def ai_relevance(text: str) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –∫ AI (0.0 - 1.0)"""
    lower = text.lower()
    matches = sum(1 for kw in AI_KEYWORDS if kw in lower)
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –∞ –Ω–µ –Ω–∞ –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
    return min(matches / 3.0, 1.0)  # 3+ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è = 100% —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å

# ====================== POSTED MANAGER (–ò–°–ü–†–ê–í–õ–ï–ù–û!) ======================
class PostedManager:
    def __init__(self, file="posted_articles.json"):
        self.file = file
        self.data = []  # –•—Ä–∞–Ω–∏–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–∞–º—è—Ç–∏
        self.ids = set()
        self.urls = set()
        self._load()

    def _load(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(self.file):
            self._save()
            return
        try:
            with open(self.file, "r", encoding="utf-8") as f:
                self.data = json.load(f)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            for item in self.data:
                url = item.get("url", "")
                if url:
                    self.ids.add(article_id(url))
                    self.urls.add(normalize_url(url))
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ posted_articles.json: {e}")
            self.data = []

    def _save(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.file, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

    def is_posted(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –±—ã–ª–∞ –ª–∏ —Å—Ç–∞—Ç—å—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞"""
        return article_id(url) in self.ids or normalize_url(url) in self.urls

    def add(self, url: str, title: str):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π —Å—Ç–∞—Ç—å–∏"""
        aid = article_id(url)
        nurl = normalize_url(url)
        
        if aid in self.ids or nurl in self.urls:
            return  # –£–∂–µ –µ—Å—Ç—å
        
        self.ids.add(aid)
        self.urls.add(nurl)
        self.data.append({
            "url": url,
            "title": title[:100],
            "ts": datetime.utcnow().isoformat() + "Z"
        })
        self._save()

    def cleanup(self, days=30):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        cutoff = datetime.utcnow().timestamp() - days * 86400
        old_count = len(self.data)
        
        self.data = [
            item for item in self.data
            if self._parse_ts(item.get("ts")) > cutoff
        ]
        
        removed = old_count - len(self.data)
        if removed > 0:
            # –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
            self.ids.clear()
            self.urls.clear()
            for item in self.data:
                url = item.get("url", "")
                if url:
                    self.ids.add(article_id(url))
                    self.urls.add(normalize_url(url))
            
            self._save()
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ {removed} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π")
    
    def _parse_ts(self, ts_str: Optional[str]) -> float:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ timestamp"""
        if not ts_str:
            return 0
        try:
            dt = datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
            return dt.timestamp()
        except:
            return 0

# ====================== RSS LOADER ======================
async def fetch_feed(session: aiohttp.ClientSession, url: str, source: str, posted: PostedManager) -> List[Article]:
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as resp:
            if resp.status != 200:
                logger.warning(f"{source}: HTTP {resp.status}")
                return []
            text = await resp.text()
    except Exception as e:
        logger.warning(f"{source} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return []

    try:
        feed = feedparser.parse(text)
    except Exception as e:
        logger.error(f"{source}: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS - {e}")
        return []

    articles = []
    for entry in feed.entries[:25]:
        link = entry.get("link", "").strip()
        if not link or posted.is_posted(link):
            continue

        title = clean_text(entry.get("title") or "")
        summary = clean_text(entry.get("summary") or entry.get("description") or "")[:1500]

        if not title or len(title) < 15:
            continue

        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        published = datetime.utcnow()
        for date_field in ["published", "updated", "created"]:
            date_str = entry.get(date_field)
            if date_str:
                try:
                    parsed = feedparser._parse_date(date_str)
                    if parsed:
                        published = datetime(*parsed[:6])
                        break
                except:
                    pass

        articles.append(Article(
            title=title,
            summary=summary,
            link=link,
            source=source,
            published=published
        ))

    return articles

async def load_all_feeds(posted: PostedManager) -> List[Article]:
    logger.info("üîÑ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø–∞–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    
    connector = aiohttp.TCPConnector(limit_per_host=5, limit=30)
    async with aiohttp.ClientSession(headers=HEADERS, connector=connector) as session:
        tasks = [fetch_feed(session, url, name, posted) for url, name in RSS_FEEDS]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    all_articles = []
    for res, (url, name) in zip(results, RSS_FEEDS):
        if isinstance(res, list) and res:
            all_articles.extend(res)
            logger.info(f"‚úÖ {name}: {len(res)} —Å—Ç–∞—Ç–µ–π")
        elif isinstance(res, Exception):
            logger.error(f"‚ùå {name}: {res}")

    logger.info(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_articles)}")
    return all_articles

# ====================== FILTER ======================
def filter_articles(articles: List[Article]) -> List[Article]:
    candidates = []
    
    for a in articles:
        text = f"{a.title} {a.summary}".lower()

        # 1. –ò—Å–∫–ª—é—á–µ–Ω–∏—è (—Ä–µ–∫–ª–∞–º–Ω—ã–µ —Ñ—Ä–∞–∑—ã)
        if any(phrase in text for phrase in BAD_PHRASES):
            continue
        
        # 2. –ò—Å–∫–ª—é—á–µ–Ω–∏—è (–Ω–µ-AI —Ç–µ–º—ã)
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue
        
        # 3. –¢—Ä–µ–±—É–µ–º –Ω–∞–ª–∏—á–∏–µ AI –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if not any(kw in text for kw in AI_KEYWORDS):
            continue
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–º–∏–Ω–∏–º—É–º 2 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
        if ai_relevance(text) < 0.5:  # –ú–µ–Ω—å—à–µ 2 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            continue

        candidates.append(a)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
    candidates.sort(key=lambda x: x.published, reverse=True)
    logger.info(f"üéØ –ü—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä—ã: {len(candidates)} —Å—Ç–∞—Ç–µ–π")
    return candidates

# ====================== SUMMARY + TRANSLATE ======================
GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "llama3-70b-8192",
    "mixtral-8x7b-32768",
]

async def generate_summary(article: Article) -> Optional[str]:
    logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞: {article.title[:60]}...")
    
    prompt = f"""–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ç–æ–ø–æ–≤–æ–≥–æ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (50–∫+ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤).

–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å (English):
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}
–û–ø–∏—Å–∞–Ω–∏–µ: {article.summary[:2000]}

–ù–∞–ø–∏—à–∏ –ø–æ—Å—Ç –Ω–∞ –†–£–°–°–ö–û–ú —è–∑—ã–∫–µ –≤ –∂–∏–≤–æ–º, —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º —Å—Ç–∏–ª–µ:
- –ù–∞—á–Ω–∏ —Å —è—Ä–∫–æ–≥–æ —Ö—É–∫–∞ (–≤–æ–ø—Ä–æ—Å, –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ, —ç–º–æ–¥–∑–∏)
- –û–±—ä—è—Å–Ω–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –ß–¢–û –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏ –ü–û–ß–ï–ú–£ —ç—Ç–æ –≤–∞–∂–Ω–æ
- –î–æ–±–∞–≤—å 1-2 —Å–≤–æ–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –≤ –¥—É—Ö–µ ¬´—ç—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–æ—Ä—ã–≤¬ª, ¬´–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –≤ —à–æ–∫–µ¬ª, ¬´–∂–¥–∞–ª–∏ –≥–æ–¥–∞–º–∏¬ª
- –î–ª–∏–Ω–∞: 600-850 —Å–∏–º–≤–æ–ª–æ–≤
- –ó–∞–∫–æ–Ω—á–∏ –≤–æ–ø—Ä–æ—Å–æ–º –∏–ª–∏ –ø—Ä–∏–∑—ã–≤–æ–º –∫ –æ–±—Å—É–∂–¥–µ–Ω–∏—é

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å –ù–ï –ø—Ä–æ –ò–ò, –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏–ª–∏ ML (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ —Ñ–∏–Ω–∞–Ω—Å—ã, –ø–æ–ª–∏—Ç–∏–∫—É, –∏–≥—Ä—ã) ‚Äî –æ—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û: SKIP

–ü–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏!"""

    for attempt in range(3):
        try:
            await asyncio.sleep(1)  # Rate limiting
            
            resp = await asyncio.to_thread(
                groq_client.chat.completions.create,
                model=random.choice(GROQ_MODELS),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1100,
            )
            text = resp.choices[0].message.content.strip()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ SKIP
            if "SKIP" in text.upper()[:50]:
                logger.info("   ‚ö†Ô∏è LLM –æ—Ç–∫–ª–æ–Ω–∏–ª–∞ —Ç–µ–º—É (SKIP)")
                return None

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É –∏ —Ö–µ—à—Ç–µ–≥–∏
            topic = Topic.detect(f"{article.title} {article.summary}")
            hashtags = Topic.HASHTAGS.get(topic, Topic.HASHTAGS[Topic.GENERAL])

            # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç
            cta = "\n\nüî• ‚Äî –æ–≥–æ–Ω—å! | üóø ‚Äî –Ω—É —Ç–∞–∫–æ–µ | ‚ö° ‚Äî –ø—Ä–∏–∫–æ–ª—å–Ω–æ"
            source = f'\n\nüîó <a href="{article.link}">–û—Ä–∏–≥–∏–Ω–∞–ª</a>'
            final = text + cta + "\n\n" + hashtags + source

            # –û–±—Ä–µ–∑–∞–µ–º, –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
            if len(final) > config.caption_limit:
                # –û–±—Ä–µ–∑–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
                overflow = len(final) - config.caption_limit + 50
                text = text[:-overflow]
                # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                for punct in ['.', '!', '?']:
                    last = text.rfind(punct)
                    if last > len(text) // 2:
                        text = text[:last + 1]
                        break
                final = text + cta + "\n\n" + hashtags + source

            return final
            
        except Exception as e:
            logger.error(f"   ‚ùå Groq –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/3): {e}")
            await asyncio.sleep(3)

    return None

# ====================== IMAGE ======================
async def generate_image(title: str) -> Optional[str]:
    clean_title = re.sub(r'[^\w\s]', '', title)[:60]
    prompt = f"minimalist futuristic AI technology illustration, {clean_title}, dark background, neon glow, cyberpunk aesthetic, 4k quality"
    url = f"https://image.pollinations.ai/prompt/{quote(prompt)}?width=1024&height=1024&nologo=true&enhance=true&seed={random.randint(1,999999)}"

    for attempt in range(2):
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as sess:
                async with sess.get(url) as resp:
                    if resp.status == 200:
                        content_length = int(resp.headers.get("Content-Length", 0))
                        if content_length > 30000:
                            fname = f"img_{int(datetime.now().timestamp())}_{random.randint(1000,9999)}.jpg"
                            with open(fname, "wb") as f:
                                f.write(await resp.read())
                            logger.info("   üñº –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ")
                            return fname
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            await asyncio.sleep(2)
    
    return None

# ====================== POST ======================
async def post_article(article: Article, text: str, posted: PostedManager) -> bool:
    img = await generate_image(article.title)
    
    try:
        if img and os.path.exists(img):
            await bot.send_photo(config.channel_id, FSInputFile(img), caption=text)
            os.remove(img)
        else:
            await bot.send_message(config.channel_id, text, disable_web_page_preview=False)

        posted.add(article.link, article.title)
        logger.info(f"‚úÖ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û: {article.title[:60]}...")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
        if img and os.path.exists(img):
            try:
                os.remove(img)
            except:
                pass
        return False

# ====================== MAIN ======================
async def autopost():
    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –ê–í–¢–û–ü–û–°–¢–ï–†–ê (Western AI News ‚Üí RU)")
    logger.info("=" * 60)

    posted = PostedManager(config.posted_file)
    posted.cleanup(config.retention_days)

    raw = await load_all_feeds(posted)
    if not raw:
        logger.info("‚ùå –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return

    candidates = filter_articles(raw)
    if not candidates:
        logger.info("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        return

    # –ü—Ä–æ–±—É–µ–º –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è
    for i, article in enumerate(candidates[:10], 1):
        logger.info(f"\n[{i}/{min(10, len(candidates))}] –ü–æ–ø—ã—Ç–∫–∞: {article.source}")
        
        summary = await generate_summary(article)
        if not summary:
            logger.info("   ‚è© –ü—Ä–æ–ø—É—Å–∫, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é...")
            continue

        if await post_article(article, summary, posted):
            logger.info("\n‚ú® –ü–æ—Å—Ç —É—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω! –ó–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É.")
            break
        
        await asyncio.sleep(3)
    else:
        logger.warning("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ç–æ–ø-10")

    logger.info("=" * 60)
    logger.info("üèÅ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    logger.info("=" * 60)

async def main():
    try:
        await autopost()
    except KeyboardInterrupt:
        logger.info("\n‚õî –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    finally:
        await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())



























































































































































































































































