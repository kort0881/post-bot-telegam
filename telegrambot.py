import os
import json
import asyncio
import random
import re
import time
from datetime import datetime
from typing import List, Dict, Optional

import requests
import feedparser
import urllib.parse
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from openai import OpenAI

# ============ CONFIG ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

if not all([OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    print("âš ï¸ WARNING: ĞĞµ Ğ²ÑĞµ ĞºĞ»ÑÑ‡Ğ¸ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ² ENV!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

POSTED_FILE = "posted_articles.json"
RETENTION_DAYS = 7
LAST_TYPE_FILE = "last_post_type.json"
TELEGRAM_CAPTION_LIMIT = 1024

# ============ ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• Ğ¡Ğ›ĞĞ’Ğ ============

AI_KEYWORDS = [
    "Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ", "Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸", "Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ", "Ğ¸Ğ¸", "Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚",
    "neural network", "artificial intelligence",
    "llm", "gpt", "gpt-4", "gpt-5", "gpt-4o", "chatgpt", "claude", "gemini",
    "copilot", "mistral", "llama", "qwen", "gigachat", "yandexgpt",
    "kandinsky", "ÑˆĞµĞ´ĞµĞ²Ñ€ÑƒĞ¼", "deepseek", "grok",
    "openai", "anthropic", "deepmind", "ÑĞ±ĞµÑ€ ai", "ÑĞ½Ğ´ĞµĞºÑ ai",
    "hugging face", "stability ai", "meta ai", "google ai",
    "stable diffusion", "midjourney", "dall-e", "sora", "runway",
    "Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹", "Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°",
    "Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾", "text-to-image", "text-to-video",
    "Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ", "Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ", "transformer",
    "Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€", "ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ", "Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹",
    "Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ", "Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸", "Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚", "fine-tuning",
    "Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚", "Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº", "Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¸Ğ»Ğ¾Ñ‚", "Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ",
    "Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹", "ai-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚", "ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº",
    "ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ", "Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ·Ñ‹ĞºĞ°", "nlp",
    "agi", "Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ", "Ğ°Ğ³ĞµĞ½Ñ‚", "ai-Ğ°Ğ³ĞµĞ½Ñ‚", "ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾",
    "Ñ‚Ğ¾ĞºĞµĞ½", "Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ", "reasoning",
    "Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼", "rlhf", "Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚", "prompt"
]

TECH_KEYWORDS = [
    "Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»", "Ğ°Ğ½Ğ¾Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ»", "Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»", "Ñ€ĞµĞ»Ğ¸Ğ·", "Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ğ»",
    "Ğ½Ğ¾Ğ²Ğ¸Ğ½ĞºĞ°", "Ğ´ĞµĞ±ÑÑ‚", "Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ", "Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»", "unveiled",
    "ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½", "Ğ½Ğ¾ÑƒÑ‚Ğ±ÑƒĞº", "Ğ³Ğ°Ğ´Ğ¶ĞµÑ‚", "Ğ´ĞµĞ²Ğ°Ğ¹Ñ", "ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾",
    "Ğ½Ğ¾ÑĞ¸Ğ¼Ğ°Ñ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ¸ĞºĞ°", "ÑƒĞ¼Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‹", "Ğ½Ğ°ÑƒÑˆĞ½Ğ¸ĞºĞ¸",
    "Ñ€Ğ¾Ğ±Ğ¾Ñ‚", "Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°", "Ğ´Ñ€Ğ¾Ğ½", "Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ğ¸Ğº", "Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¸Ğ»Ğ¾Ñ‚",
    "Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹", "boston dynamics", "tesla bot",
    "ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¹", "ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€", "Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€", "Ñ‡Ğ¸Ğ¿",
    "gpu", "Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ñ€Ñ‚Ğ°", "nvidia", "amd", "intel", "apple m",
    "spacex", "starship", "ĞºĞ¾ÑĞ¼Ğ¾Ñ", "Ñ€Ğ°ĞºĞµÑ‚Ğ°", "ÑĞ¿ÑƒÑ‚Ğ½Ğ¸Ğº",
    "starlink", "nasa", "Ñ€Ğ¾ÑĞºĞ¾ÑĞ¼Ğ¾Ñ",
    "Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ", "Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ",
    "vr", "ar", "meta quest", "apple vision",
    "ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒ", "tesla", "ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ĞºĞ°Ñ€", "Ğ±Ğ°Ñ‚Ğ°Ñ€ĞµÑ",
    "Ğ°ĞºĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€",
    "Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²", "Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ", "Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ"
]

EXCLUDE_KEYWORDS = [
    "Ğ°ĞºÑ†Ğ¸Ğ¸", "Ğ°ĞºÑ†Ğ¸Ñ", "Ğ±Ğ¸Ñ€Ğ¶Ğ°", "ĞºĞ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸", "Ğ¸Ğ½Ğ´ĞµĞºÑ",
    "Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ğ¸Ğ¸", "Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¾Ñ€", "Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¾Ñ€Ñ‹", "Ğ´Ğ¸Ğ²Ğ¸Ğ´ĞµĞ½Ğ´Ñ‹",
    "ipo", "ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ", "Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ",
    "Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°", "Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ", "ÑƒĞ±Ñ‹Ñ‚Ğ¾Ğº", "Ğ´Ğ¾Ñ…Ğ¾Ğ´", "Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚",
    "Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚", "Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚", "ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚",
    "Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ¾Ğ²", "Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ¾Ğ²", "Ğ¼Ğ»Ñ€Ğ´", "Ğ¼Ğ»Ğ½ Ñ€ÑƒĞ±Ğ»ĞµĞ¹",
    "ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ°", "ĞºÑƒÑ€Ñ ĞµĞ²Ñ€Ğ¾", "ĞºÑƒÑ€Ñ Ñ€ÑƒĞ±Ğ»Ñ", "Ğ²Ğ°Ğ»ÑÑ‚Ğ°",
    "Ñ†Ğ±", "Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ±Ğ°Ğ½Ğº", "ÑÑ‚Ğ°Ğ²ĞºĞ°", "ĞºĞ»ÑÑ‡ĞµĞ²Ğ°Ñ ÑÑ‚Ğ°Ğ²ĞºĞ°", "Ğ¸Ğ½Ñ„Ğ»ÑÑ†Ğ¸Ñ",
    "ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°", "ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹", "Ğ²Ğ²Ğ¿", "Ñ€ĞµÑ†ĞµÑÑĞ¸Ñ",
    "Ğ±Ğ°Ğ½Ğº", "ĞºÑ€ĞµĞ´Ğ¸Ñ‚", "Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞºĞ°", "Ğ²ĞºĞ»Ğ°Ğ´", "Ğ´ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚",
    "Ñ„Ğ¾Ğ½Ğ´", "Ğ²ĞµĞ½Ñ‡ÑƒÑ€Ğ½Ñ‹Ğ¹", "Ñ€Ğ°ÑƒĞ½Ğ´ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
    "ÑĞ´ĞµĞ»ĞºĞ°", "ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ", "Ğ¿Ğ¾Ğ³Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ", "m&a",
    "Ñ€Ñ‹Ğ½Ğ¾Ğº", "Ğ´Ğ¾Ğ»Ñ Ñ€Ñ‹Ğ½ĞºĞ°", "ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹",
    "Ñ†ĞµĞ½Ğ° Ğ°ĞºÑ†Ğ¸Ğ¹", "ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸", "Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸",
    "Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ±Ğ¸Ñ€Ğ¶Ñƒ", "Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ", "Ğ»Ğ¸ÑÑ‚Ğ¸Ğ½Ğ³",
    "Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½", "Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ", "Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²ĞºĞ°", "ÑƒĞ²Ğ¾Ğ»ĞµĞ½",
    "Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€", "ceo", "Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ ÑƒÑˆÑ‘Ğ»",
    "ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ÑˆÑ‚Ğ°Ñ‚Ğ°", "ÑƒĞ²Ğ¾Ğ»ÑŒĞ½ĞµĞ½Ğ¸Ñ", "ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ",
    "Ğ¾Ñ„Ğ¸Ñ", "ÑˆÑ‚Ğ°Ğ±-ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€Ğ°", "Ğ¿ĞµÑ€ĞµĞµĞ·Ğ´ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸",
    "Ñ‚ĞµĞ½Ğ½Ğ¸Ñ", "Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»", "Ñ…Ğ¾ĞºĞºĞµĞ¹", "Ğ±Ğ°ÑĞºĞµÑ‚Ğ±Ğ¾Ğ»", "ÑĞ¿Ğ¾Ñ€Ñ‚", "Ğ¼Ğ°Ñ‚Ñ‡",
    "Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ°", "Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½Ğ°Ñ‚", "Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€", "ÑĞ±Ğ¾Ñ€Ğ½Ğ°Ñ",
    "Ğ¸Ğ³Ñ€Ğ°", "Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµĞ¹", "playstation", "xbox", "steam", "nintendo",
    "Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ğ°", "ĞºĞ¾Ğ½ÑĞ¾Ğ»ÑŒ", "gaming",
    "ĞºĞ¸Ğ½Ğ¾", "Ñ„Ğ¸Ğ»ÑŒĞ¼", "ÑĞµÑ€Ğ¸Ğ°Ğ»", "Ğ¼ÑƒĞ·Ñ‹ĞºĞ°", "ĞºĞ¾Ğ½Ñ†ĞµÑ€Ñ‚", "Ğ°ĞºÑ‚Ñ‘Ñ€", "Ğ°ĞºÑ‚ĞµÑ€",
    "Ğ¿Ñ€ĞµĞ¼ÑŒĞµÑ€Ğ°", "Ñ‚Ñ€ĞµĞ¹Ğ»ĞµÑ€", "netflix", "ĞºĞ¸Ğ½Ğ¾Ñ‚ĞµĞ°Ñ‚Ñ€",
    "Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñ‹", "Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚", "Ğ¿Ğ°Ñ€Ğ»Ğ°Ğ¼ĞµĞ½Ñ‚", "Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº", "Ğ´ĞµĞ¿ÑƒÑ‚Ğ°Ñ‚",
    "ÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¸", "Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾", "Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€", "Ğ·Ğ°ĞºĞ¾Ğ½", "Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¿Ñ€Ğ¾ĞµĞºÑ‚",
    "Ğ±Ğ¾Ğ»ĞµĞ·Ğ½ÑŒ", "covid", "Ğ¿Ğ°Ğ½Ğ´ĞµĞ¼Ğ¸Ñ", "Ğ³Ñ€Ğ¸Ğ¿Ğ¿", "Ğ²Ğ°ĞºÑ†Ğ¸Ğ½Ğ°",
    "ĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾", "bitcoin", "Ğ±Ğ¸Ñ‚ĞºĞ¾Ğ¹Ğ½", "Ğ±Ğ¸Ñ‚ĞºĞ¾Ğ¸Ğ½", "ethereum",
    "nft", "Ğ±Ğ»Ğ¾ĞºÑ‡ĞµĞ¹Ğ½", "ĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ²Ğ°Ğ»ÑÑ‚Ğ°", "Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³",
    "ÑÑƒĞ´", "ÑÑƒĞ´ĞµĞ±Ğ½Ñ‹Ğ¹", "Ğ°Ñ€ĞµÑÑ‚", "Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ²Ğ¾Ñ€", "Ñ‚ÑÑ€ÑŒĞ¼Ğ°", "ÑˆÑ‚Ñ€Ğ°Ñ„",
    "Ğ¸ÑĞº", "Ğ°Ğ½Ñ‚Ğ¸Ğ¼Ğ¾Ğ½Ğ¾Ğ¿Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹"
]

# ============ ĞĞĞ¢Ğ˜Ğ Ğ•ĞšĞ›ĞĞœĞĞ«Ğ™ Ğ¤Ğ˜Ğ›Ğ¬Ğ¢Ğ  ============

BAD_PHRASES = [
    "Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ",
    "Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ",
    "Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ",
    "Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ",
    "Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ",
    "Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
    "Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğµ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ¾Ğ± ÑƒĞ³Ñ€Ğ¾Ğ·Ğ°Ñ…",
    "Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ±Ğ¸Ğ·Ğ½ĞµÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ĞµĞµ",
    "Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¸Ğ·Ğ½ĞµÑÑƒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ĞµĞµ",
    "Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚",
    "ĞºĞ°Ñ€Ğ´Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚",
    "ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ",
    "Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ",
    "Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¸Ğ·Ğ½ĞµÑÑƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ",
]

def is_too_promotional(text: str) -> bool:
    low = text.lower()
    if any(p in low for p in BAD_PHRASES):
        return True
    if ("Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚" in low or "Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚" in low or "Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ" in low) and \
       not any(k in low for k in ["Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚", "Ğ·Ğ° ÑÑ‡ĞµÑ‚", "Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ", "Ñ‡ĞµÑ€ĞµĞ·", "Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€", "Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ", "Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸", "Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ñ€Ğ°Ñ„Ğ¸ĞºĞ°", "rate limiting", "Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº"]):
        return True
    return False

# ============ STATE ============

posted_articles: Dict[str, Optional[float]] = {}

if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        try:
            posted_data = json.load(f)
            posted_articles = {item["id"]: item.get("timestamp") for item in posted_data}
        except Exception:
            posted_articles = {}

def save_posted_articles() -> None:
    data = [{"id": id_str, "timestamp": ts} for id_str, ts in posted_articles.items()]
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def clean_old_posts() -> None:
    global posted_articles
    now = datetime.now().timestamp()
    cutoff = now - (RETENTION_DAYS * 86400)
    posted_articles = {
        id_str: ts for id_str, ts in posted_articles.items()
        if ts is None or ts > cutoff
    }
    save_posted_articles()

def save_posted(article_id: str) -> None:
    posted_articles[article_id] = datetime.now().timestamp()
    save_posted_articles()

def load_last_post_type() -> Optional[str]:
    if not os.path.exists(LAST_TYPE_FILE):
        return None
    try:
        with open(LAST_TYPE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("type")
    except Exception:
        return None

def save_last_post_type(post_type: str) -> None:
    try:
        with open(LAST_TYPE_FILE, "w", encoding="utf-8") as f:
            json.dump({"type": post_type}, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

# ============ HELPERS ============

def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())

def detect_topic(title: str, summary: str) -> str:
    text = f"{title} {summary}".lower()

    if any(kw in text for kw in ["gpt", "chatgpt", "claude", "llm", "ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ"]):
        return "llm"
    elif any(kw in text for kw in ["midjourney", "dall-e", "stable diffusion", "Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶"]):
        return "image_gen"
    elif any(kw in text for kw in ["Ñ€Ğ¾Ğ±Ğ¾Ñ‚", "robot", "Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½"]):
        return "robotics"
    elif any(kw in text for kw in ["spacex", "ĞºĞ¾ÑĞ¼Ğ¾Ñ", "Ñ€Ğ°ĞºĞµÑ‚Ğ°", "ÑĞ¿ÑƒÑ‚Ğ½Ğ¸Ğº"]):
        return "space"
    elif any(kw in text for kw in ["nvidia", "gpu", "Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€", "Ñ‡Ğ¸Ğ¿"]):
        return "hardware"
    elif any(kw in text for kw in ["Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚", "neural", "Ğ¸Ğ¸", "ai", "Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚"]):
        return "ai"
    else:
        return "tech"

def get_hashtags(topic: str) -> str:
    hashtag_map = {
        "llm": "#ChatGPT #LLM #Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸",
        "image_gen": "#AI #Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ #Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸",
        "robotics": "#Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ #Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ #Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ",
        "space": "#ĞºĞ¾ÑĞ¼Ğ¾Ñ #SpaceX #Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
        "hardware": "#Ğ¶ĞµĞ»ĞµĞ·Ğ¾ #GPU #Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
        "ai": "#AI #Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ #Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
        "tech": "#Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ #Ğ½Ğ¾Ğ²Ğ¸Ğ½ĞºĞ¸ #Ğ³Ğ°Ğ´Ğ¶ĞµÑ‚Ñ‹"
    }
    return hashtag_map.get(topic, "#Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ #Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸")

def ensure_complete_sentence(text: str) -> str:
    text = text.strip()
    if not text: return text
    if text[-1] in '.!?': return text
    last_period = text.rfind('.')
    last_exclaim = text.rfind('!')
    last_question = text.rfind('?')
    last_end = max(last_period, last_exclaim, last_question)
    if last_end > 0: return text[:last_end + 1]
    return text + '.'

def trim_core_text_to_limit(core_text: str, max_core_length: int) -> str:
    core_text = core_text.strip()
    if len(core_text) <= max_core_length:
        return ensure_complete_sentence(core_text)
    sentence_pattern = r'(?<=[.!?])\s+'
    sentences = re.split(sentence_pattern, core_text)
    result = ""
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence: continue
        candidate = (result + " " + sentence).strip() if result else sentence
        if len(candidate) <= max_core_length:
            result = candidate
        else:
            break
    if not result and sentences:
        result = sentences[0][:max_core_length]
        if len(result) == max_core_length and ' ' in result:
            result = result.rsplit(' ', 1)[0]
    return ensure_complete_sentence(result)

def build_final_post(core_text: str, hashtags: str, link: str, max_total: int = 1024) -> str:
    cta_line = "\n\nğŸ”¥ â€” Ğ¾Ğ³Ğ¾Ğ½ÑŒ! | ğŸ—¿ â€” Ğ½Ñƒ Ñ‚Ğ°ĞºĞ¾Ğµ | âš¡ â€” Ğ±ÑƒĞ´Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ"
    source_line = f'\nğŸ”— <a href="{link}">Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº</a>'
    hashtag_line = f"\n\n{hashtags}"
    
    service_length = len(cta_line) + len(hashtag_line) + len(source_line)
    max_core_length = max_total - service_length - 10
    
    trimmed_core = trim_core_text_to_limit(core_text, max_core_length)
    
    final = trimmed_core + cta_line + hashtag_line + source_line
    if len(final) > max_total:
        overflow = len(final) - max_total
        trimmed_core = trim_core_text_to_limit(core_text, max_core_length - overflow - 20)
        final = trimmed_core + cta_line + hashtag_line + source_line
    return final

# ============ PARSERS ============

def load_rss(url: str, source: str) -> List[Dict]:
    articles = []
    try:
        feed = feedparser.parse(url)
        if feed.bozo and not feed.entries:
            print(f"âš ï¸ RSS Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½: {source}")
            return articles
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ RSS {source}: {e}")
        return articles

    for entry in feed.entries[:30]:
        link = entry.get("link", "")
        if not link or link in posted_articles:
            continue
        articles.append({
            "id": link,
            "title": clean_text(entry.get("title") or ""),
            "summary": clean_text(
                entry.get("summary") or entry.get("description") or ""
            )[:700],
            "link": link,
            "source": source,
            "published_parsed": datetime.now()
        })
    return articles

def load_articles_from_sites() -> List[Dict]:
    articles: List[Dict] = []
    # HABR
    articles.extend(load_rss("https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru", "Habr AI"))
    articles.extend(load_rss("https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru", "Habr ML"))
    articles.extend(load_rss("https://habr.com/ru/rss/hub/neural_networks/all/?fl=ru", "Habr Neural"))
    
    # TECH
    articles.extend(load_rss("https://3dnews.ru/news/rss/", "3DNews"))
    articles.extend(load_rss("https://www.ixbt.com/export/news.rss", "iXBT"))
    articles.extend(load_rss("https://nplus1.ru/rss", "N+1"))
    articles.extend(load_rss("https://hightech.fm/feed", "Ğ¥Ğ°Ğ¹Ñ‚ĞµĞº"))
    
    return articles

def filter_articles(articles: List[Dict]) -> List[Dict]:
    ai_articles = []
    tech_articles = []

    for e in articles:
        text = f"{e['title']} {e['summary']}".lower()

        # Ğ¤Ğ˜Ğ›Ğ¬Ğ¢Ğ ĞĞ¦Ğ˜Ğ¯ ĞŸĞ Ğ¡Ğ¢ĞĞŸ-Ğ¡Ğ›ĞĞ’ĞĞœ (Ğ‘Ğ˜Ğ Ğ–Ğ, ĞŸĞĞ›Ğ˜Ğ¢Ğ˜ĞšĞ, Ğ¡ĞŸĞĞ Ğ¢)
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue

        source = e.get("source", "")
        if source in ["3DNews", "iXBT", "Overclockers"]:
            e["post_type"] = "hardware"
        else:
            e["post_type"] = "it"

        if any(kw in text for kw in AI_KEYWORDS):
            ai_articles.append(e)
        elif any(kw in text for kw in TECH_KEYWORDS):
            tech_articles.append(e)

    ai_articles.sort(key=lambda x: x["published_parsed"], reverse=True)
    tech_articles.sort(key=lambda x: x["published_parsed"], reverse=True)

    return ai_articles + tech_articles

# ============ Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ¯ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ (Ğ’Ğ•Ğ¡Ğ•Ğ›Ğ«Ğ™, ĞĞ ĞĞ”Ğ•ĞšĞ’ĞĞ¢ĞĞ«Ğ™) ============

def build_dynamic_prompt(title: str, summary: str) -> str:
    news_text = f"Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº: {title}\n\nĞ¢ĞµĞºÑÑ‚: {summary}"

    prompt = f"""
Ğ¢Ñ‹ â€” Ğ¾ÑÑ‚Ñ€Ğ¾ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¾-Ğ±Ğ»Ğ¾Ğ³ĞµÑ€. 
Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°: ĞĞ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¿Ğ¾ÑÑ‚ Ğ¾ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±ÑƒĞ´ĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾ Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ.

ĞĞĞ’ĞĞ¡Ğ¢Ğ¬:
{news_text}

Ğ¢Ğ Ğ•Ğ‘ĞĞ’ĞĞĞ˜Ğ¯:
1. Ğ’Ğ¡Ğ¢Ğ£ĞŸĞ›Ğ•ĞĞ˜Ğ•: Ğ¡Ñ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ: "Ğ’ÑĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚! ğŸ‘‹" Ğ¸Ğ»Ğ¸ "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, Ğ´Ñ€ÑƒĞ·ÑŒÑ! âœŒï¸". 
   - ĞĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ "Ğ™Ğ¾Ñƒ", "Ğ“Ğ¸ĞºĞ¸", "ĞĞ° ÑĞ²ÑĞ·Ğ¸" Ğ¸ Ñ‚.Ğ¿.
2. Ğ¡Ğ¢Ğ˜Ğ›Ğ¬: Ğ–Ğ¸Ğ²Ğ¾Ğ¹, Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ¹ Ğ¸Ñ€Ğ¾Ğ½Ğ¸ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ±Ñ€Ñ‹Ğ¼ ÑĞ¼Ğ¾Ñ€Ğ¾Ğ¼. ĞŸĞ¸ÑˆĞ¸ Ñ‚Ğ°Ğº, ĞºĞ°Ğº Ğ±ÑƒĞ´Ñ‚Ğ¾ Ñ€Ğ°ÑÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑˆÑŒ Ğ´Ñ€ÑƒĞ³Ñƒ.
   - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¼ĞµÑ‚Ğ°Ñ„Ğ¾Ñ€Ñ‹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ.
   - ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾ÑˆÑƒÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ´ Ñ‚ĞµĞ¼, Ñ‡Ñ‚Ğ¾ "Skynet ÑƒĞ¶Ğµ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾" Ğ¸Ğ»Ğ¸ "Ğ¾Ğ¿ÑÑ‚ÑŒ Ğ²ÑÑ‘ Ğ¿ĞµÑ€ĞµĞ¸Ğ·Ğ¾Ğ±Ñ€ĞµĞ»Ğ¸", Ğ½Ğ¾ Ğ² Ğ¼ĞµÑ€Ñƒ.
   - ĞĞµ ÑƒÑ…Ğ¾Ğ´Ğ¸ Ğ² ĞºĞ»Ğ¾ÑƒĞ½Ğ°Ğ´Ñƒ, Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑƒÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ°.
3. Ğ—ĞĞŸĞ Ğ•Ğ¢Ğ«:
   - ĞĞ¸ĞºĞ°ĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ· ("Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ğ¹Ñ‚Ğµ", "Ğ»ÑƒÑ‡ÑˆĞµĞµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ").
   - ĞĞ¸ĞºĞ°ĞºĞ¸Ñ… ÑĞºÑƒÑ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ñ†ĞµĞ»ÑÑ€Ğ¸Ğ·Ğ¼Ğ¾Ğ².
4. Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ:
   - ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ.
   - ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ·Ğ°Ñ…Ğ¾Ğ´ (Ñ…ÑƒĞº/ÑˆÑƒÑ‚ĞºĞ°).
   - Ğ¡ÑƒÑ‚ÑŒ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ (Ñ‡Ñ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ¸Ğ»Ğ¾ÑÑŒ Ğ¸ ĞºĞ°Ğº ÑÑ‚Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚).
   - Ğ’Ñ‹Ğ²Ğ¾Ğ´ (Ñ‚Ğ²Ğ¾Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ: ĞºÑ€ÑƒÑ‚Ğ¾ ÑÑ‚Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ‚).
5. ĞĞ‘ĞªĞ•Ğœ: Ğ´Ğ¾ 800 Ğ·Ğ½Ğ°ĞºĞ¾Ğ².
"""
    return prompt

def validate_generated_text(text: str) -> tuple[bool, str]:
    text = text.strip()
    if not text: return False, "ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚"
    if len(text) < 50: return False, "Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹"
    return True, "OK"

def short_summary(title: str, summary: str, link: str) -> Optional[str]:
    prompt = build_dynamic_prompt(title, summary)
    print(f"  ğŸ“ Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾ÑÑ‚ (Fun but Normal)...")

    try:
        res = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7, # Ğ§ÑƒÑ‚ÑŒ Ğ²Ñ‹ÑˆĞµ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑˆÑƒÑ‚Ğ¾Ğº
            max_tokens=700,
        )
        core = res.choices[0].message.content.strip()

        if core.startswith('"') and core.endswith('"'): core = core[1:-1]
        
        if is_too_promotional(core):
            print("  âš ï¸ Ğ¢ĞµĞºÑÑ‚ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ğ½Ñ‹Ğ¹, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼.")
            return None

        topic = detect_topic(title, summary)
        hashtags = get_hashtags(topic)
        final = build_final_post(core, hashtags, link, max_total=TELEGRAM_CAPTION_LIMIT)
        return final

    except Exception as e:
        print(f"âŒ OpenAI Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")
        return None

# ============ Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ¯ ĞšĞĞ Ğ¢Ğ˜ĞĞĞš ============

def generate_image(title: str, max_retries: int = 2) -> Optional[str]:
    # Ğ”ĞµĞ»Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ñ‡ÑƒÑ‚ÑŒ ÑÑ€Ñ‡Ğµ Ğ´Ğ»Ñ Ğ²ĞµÑĞµĞ»Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ¸Ğ»Ñ
    style_prompt = "futuristic concept art, vibrant colors, technology, 3d render, detailed, cyberpunk, neon"
    
    for attempt in range(max_retries):
        seed = random.randint(0, 10**7)
        clean_title = re.sub(r'[^a-zA-Z0-9]', ' ', title)[:50]
        prompt = f"{style_prompt}, {clean_title}"
        
        encoded = urllib.parse.quote(prompt)
        url = f"https://image.pollinations.ai/prompt/{encoded}?seed={seed}&width=1024&height=1024&nologo=true"
        
        try:
            print(f"  ğŸ¨ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸...")
            resp = requests.get(url, timeout=40, headers=HEADERS)
            if resp.status_code == 200 and len(resp.content) > 10000:
                fname = f"img_{seed}.jpg"
                with open(fname, "wb") as f: f.write(resp.content)
                return fname
        except Exception as e:
            print(f"  âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸: {e}")
    return None

def cleanup_image(filepath: Optional[str]) -> None:
    if filepath and os.path.exists(filepath):
        try: os.remove(filepath)
        except: pass

# ============ ĞĞ’Ğ¢ĞĞŸĞĞ¡Ğ¢ ============

async def autopost():
    clean_old_posts()
    print("ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑÑ‚Ğ°Ñ‚ĞµĞ¹...")
    articles = load_articles_from_sites()
    candidates = filter_articles(articles)

    if not candidates:
        print("âŒ ĞĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹.")
        return

    print(f"ğŸ“Š ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾: {len(candidates)} ÑÑ‚Ğ°Ñ‚ĞµĞ¹.")
    
    last_type = load_last_post_type()
    posted_count = 0
    max_posts = 1 

    hardware_candidates = [c for c in candidates if c.get("post_type") == "hardware"]
    it_candidates = [c for c in candidates if c.get("post_type") == "it"]

    def pick_next_article() -> Optional[Dict]:
        nonlocal last_type
        if last_type == "hardware":
            if it_candidates: return it_candidates.pop(0)
            elif hardware_candidates: return hardware_candidates.pop(0)
        else:
            if hardware_candidates: return hardware_candidates.pop(0)
            elif it_candidates: return it_candidates.pop(0)
        return None

    while posted_count < max_posts:
        art = pick_next_article()
        if not art: break

        print(f"\nğŸ” ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°: {art['title']}")
        post_text = short_summary(art["title"], art["summary"], art["link"])

        if not post_text: continue

        img = generate_image(art["title"])
        
        try:
            if img:
                await bot.send_photo(CHANNEL_ID, photo=FSInputFile(img), caption=post_text)
            else:
                await bot.send_message(CHANNEL_ID, text=post_text, disable_web_page_preview=False)

            save_posted(art["id"])
            posted_count += 1
            last_type = art.get("post_type")
            save_last_post_type(last_type)
            print(f"âœ… ĞĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ¾!")

        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ TG: {e}")
        finally:
            cleanup_image(img)

async def main():
    try: await autopost()
    finally: await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())



















































































































































































































































