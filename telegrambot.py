import os
import json
import asyncio
import random
import re
import hashlib
import logging
import difflib
import tempfile
import shutil
from datetime import datetime, timezone
from typing import List, Set, Optional
from urllib.parse import urlparse, quote
from dataclasses import dataclass, field

import aiohttp
import feedparser
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq

# –î–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–æ–≤ (Linux/Mac)
try:
    import fcntl
    HAS_FCNTL = True
except ImportError:
    HAS_FCNTL = False

# ====================== –õ–û–ì–ò ======================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("ai_poster.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ====================== CONFIG ======================
class Config:
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.telegram_token = os.getenv("TELEGRAM_BOT_TOKEN")
        self.channel_id = os.getenv("CHANNEL_ID")
        self.retention_days = int(os.getenv("RETENTION_DAYS", "30"))
        self.caption_limit = 1024
        self.posted_file = "posted_articles.json"
        
        self.similarity_threshold = 0.60  # –ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        self.entity_overlap_threshold = 0.55  # –ü–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.min_post_length = 500
        
        # üÜï –ù–û–í–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø –†–ê–ó–ù–û–û–ë–†–ê–ó–ò–Ø
        self.recent_posts_check = 5  # –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
        self.recent_similarity_threshold = 0.45  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤
        self.min_entity_distance = 2  # –ú–∏–Ω. –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π

        missing = []
        for var, name in [(self.groq_api_key, "GROQ_API_KEY"),
                          (self.telegram_token, "TELEGRAM_BOT_TOKEN"),
                          (self.channel_id, "CHANNEL_ID")]:
            if not var:
                missing.append(name)
        if missing:
            raise SystemExit(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(missing)}")

config = Config()

bot = Bot(token=config.telegram_token, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=config.groq_api_key)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

# ====================== RSS (–†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö) ======================
RSS_FEEDS = [
    # –û—Å–Ω–æ–≤–Ω—ã–µ
    ("https://techcrunch.com/category/artificial-intelligence/feed/", "TechCrunch"),
    ("https://venturebeat.com/category/ai/feed/", "VentureBeat"),
    ("https://www.technologyreview.com/topic/artificial-intelligence/feed", "MIT Tech Review"),
    ("https://www.theverge.com/rss/index.xml", "The Verge"),
    ("https://arstechnica.com/tag/artificial-intelligence/feed/", "Ars Technica"),
    ("https://www.wired.com/feed/tag/ai/latest/rss", "WIRED"),
    
    # üÜï –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò
    ("https://www.artificialintelligence-news.com/feed/", "AI News"),
    ("https://hai.stanford.edu/news/rss.xml", "Stanford HAI"),
    ("https://deepmind.google/blog/rss.xml", "DeepMind Blog"),
    ("https://openai.com/blog/rss/", "OpenAI Blog"),
    ("https://blog.google/technology/ai/rss/", "Google AI Blog"),
    ("https://www.marktechpost.com/feed/", "MarkTechPost"),
    ("https://syncedreview.com/feed/", "Synced AI"),
    ("https://news.ycombinator.com/rss", "Hacker News"),  # –ú–Ω–æ–≥–æ AI-–Ω–æ–≤–æ—Å—Ç–µ–π
    ("https://www.unite.ai/feed/", "Unite.AI"),
    ("https://analyticsindiamag.com/feed/", "AIM"),
]

# ====================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ======================
AI_KEYWORDS = [
    "ai ", " ai", "artificial intelligence", "machine learning", "deep learning",
    "neural network", "llm", "large language model", "gpt", "chatgpt", "claude",
    "gemini", "grok", "llama", "mistral", "qwen", "deepseek", "midjourney",
    "dall-e", "stable diffusion", "sora", "groq", "openai", "anthropic",
    "deepmind", "hugging face", "nvidia", "agi", "transformer", "generative",
    "agents", "reasoning", "multimodal", "fine-tuning", "rlhf"
]

EXCLUDE_KEYWORDS = [
    "stock price", "ipo", "earnings call", "quarterly results", "dividend",
    "market cap", "wall street", "ps5", "xbox", "nintendo", "game review",
    "netflix", "movie review", "box office", "trailer", "tesla stock",
    "bitcoin", "crypto", "blockchain", "nft", "ethereum", "election",
    "trump", "biden", "congress", "senate"
]

BAD_PHRASES = ["sponsored", "partner content", "advertisement", "black friday", "deal alert"]

# ====================== –ö–õ–Æ–ß–ï–í–´–ï –°–£–©–ù–û–°–¢–ò –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò –î–£–ë–õ–ï–ô ======================
KEY_ENTITIES = [
    # –ö–æ–º–ø–∞–Ω–∏–∏
    "openai", "google", "meta", "microsoft", "anthropic", "nvidia", "apple",
    "amazon", "deepmind", "hugging face", "stability ai", "midjourney",
    "mistral", "cohere", "perplexity", "runway", "pika", "character ai",
    "inflection", "xai", "baidu", "alibaba", "tencent", "bytedance",
    
    # –ü—Ä–æ–¥—É–∫—Ç—ã –∏ –º–æ–¥–µ–ª–∏
    "gpt-4", "gpt-5", "gpt-4o", "gpt-4.5", "chatgpt", "claude", "claude 3",
    "gemini", "gemini 2", "llama", "llama 3", "mistral", "mixtral",
    "copilot", "dall-e", "dall-e 3", "sora", "stable diffusion", "flux",
    "midjourney v6", "runway gen", "firefly", "imagen",
    
    # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
    "linux foundation", "agentic", "ai agent", "agi", "asi",
    "regulation", "safety", "alignment", "open source", "open-source",
    "robotics", "humanoid", "boston dynamics", "figure", "optimus",
    
    # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    "transformer", "diffusion", "multimodal", "reasoning", "chain of thought",
    "fine-tuning", "rlhf", "inference", "training", "benchmark"
]

# ====================== DATACLASS ======================
@dataclass
class Article:
    title: str
    summary: str
    link: str
    source: str
    published: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

# ====================== TOPIC & HASHTAGS ======================
class Topic:
    LLM = "llm"
    IMAGE_GEN = "image_gen"
    ROBOTICS = "robotics"
    HARDWARE = "hardware"
    REGULATION = "regulation"
    RESEARCH = "research"
    GENERAL = "general"
    
    HASHTAGS = {
        LLM: "#ChatGPT #LLM #OpenAI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        IMAGE_GEN: "#Midjourney #DALLE #StableDiffusion #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
        ROBOTICS: "#—Ä–æ–±–æ—Ç—ã #Humanoid #—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
        HARDWARE: "#NVIDIA #GPU #—á–∏–ø—ã #–∂–µ–ª–µ–∑–æ",
        REGULATION: "#—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ #–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å #—ç—Ç–∏–∫–∞",
        RESEARCH: "#–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è #–Ω–∞—É–∫–∞ #DeepMind",
        GENERAL: "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #–ò–ò"
    }

    @staticmethod
    def detect(text: str) -> str:
        t = text.lower()
        if any(x in t for x in ["gpt", "chatgpt", "claude", "gemini", "llama", "grok", "llm"]):
            return Topic.LLM
        if any(x in t for x in ["midjourney", "dall-e", "stable diffusion", "flux", "sora"]):
            return Topic.IMAGE_GEN
        if any(x in t for x in ["robot", "humanoid", "boston dynamics", "optimus", "figure"]):
            return Topic.ROBOTICS
        if any(x in t for x in ["nvidia", "h100", "h200", "blackwell", "gpu", "cuda"]):
            return Topic.HARDWARE
        if any(x in t for x in ["regulation", "safety", "alignment", "ethics", "policy"]):
            return Topic.REGULATION
        if any(x in t for x in ["research", "paper", "study", "breakthrough", "discovery"]):
            return Topic.RESEARCH
        return Topic.GENERAL

# ====================== HELPERS ======================
def normalize_url(url: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    if not url:
        return ""
    try:
        url = url.strip()
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace("www.", "")
        path = parsed.path.rstrip("/")
        return f"{parsed.scheme}://{domain}{path}"
    except:
        return url.split("?")[0].split("#")[0]

def calculate_similarity(text1: str, text2: str) -> float:
    """–°—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Å—Ç—Ä–æ–∫ (0.0 - 1.0)"""
    return difflib.SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def extract_key_entities(text: str) -> Set[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_lower = text.lower()
    found = set()
    
    for entity in KEY_ENTITIES:
        if entity in text_lower:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            normalized = entity.replace("-", " ").replace("_", " ")
            found.add(normalized)
    
    return found

def clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def ai_relevance(text: str) -> float:
    lower = text.lower()
    matches = sum(1 for kw in AI_KEYWORDS if kw in lower)
    return min(matches / 3.0, 1.0)

def get_content_hash(text: str) -> str:
    """MD5 —Ö–µ—à –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    if not text:
        return ""
    normalized = re.sub(r'\s+', ' ', text.strip().lower())
    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Ö–µ—à–∞ (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏)
    return hashlib.md5(normalized[:500].encode()).hexdigest()[:16]

# ====================== POSTED MANAGER ======================
class PostedManager:
    def __init__(self, file="posted_articles.json"):
        self.file = file
        self.lock_file = file + ".lock"
        self.data: List[dict] = []
        self.urls: Set[str] = set()
        self.titles: List[str] = []
        self.content_hashes: Set[str] = set()
        self.topic_entities: List[Set[str]] = []
        self.topics: List[str] = []  # üÜï –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–µ–º
        self._lock_fd = None
        
        self._acquire_lock()
        self._load()

    def _acquire_lock(self):
        """–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
        if not HAS_FCNTL:
            # Windows fallback
            if os.path.exists(self.lock_file):
                try:
                    age = datetime.now().timestamp() - os.path.getmtime(self.lock_file)
                    if age < 600:
                        logger.warning("‚ö†Ô∏è –î—Ä—É–≥–æ–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç. –í—ã—Ö–æ–¥.")
                        raise SystemExit(0)
                except OSError:
                    pass
            with open(self.lock_file, 'w') as f:
                f.write(str(os.getpid()))
            return
        
        # Linux/Mac
        self._lock_fd = open(self.lock_file, 'w')
        try:
            fcntl.flock(self._lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
            self._lock_fd.write(str(os.getpid()))
            self._lock_fd.flush()
        except BlockingIOError:
            logger.warning("‚ö†Ô∏è –°–∫—Ä–∏–ø—Ç —É–∂–µ –∑–∞–ø—É—â–µ–Ω. –í—ã—Ö–æ–¥.")
            raise SystemExit(0)

    def _release_lock(self):
        try:
            if HAS_FCNTL and self._lock_fd:
                fcntl.flock(self._lock_fd, fcntl.LOCK_UN)
                self._lock_fd.close()
            if os.path.exists(self.lock_file):
                os.remove(self.lock_file)
        except:
            pass

    def _load(self):
        if not os.path.exists(self.file):
            self._save()
            return
        
        try:
            with open(self.file, "r", encoding="utf-8") as f:
                self.data = json.load(f)
            self._rebuild_caches()
            logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} —Å—Ç–∞—Ç–µ–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            self.data = []

    def _rebuild_caches(self):
        """–ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤—Å–µ –∫—ç—à–∏ –∏–∑ self.data"""
        self.urls.clear()
        self.titles.clear()
        self.content_hashes.clear()
        self.topic_entities.clear()
        self.topics.clear()
        
        for item in self.data:
            # URL
            url = item.get("url", "")
            if url:
                self.urls.add(normalize_url(url))
            
            # Title
            title = item.get("title", "")
            if title:
                self.titles.append(title)
            else:
                self.titles.append("")
            
            # Entities
            saved_entities = item.get("entities", [])
            if saved_entities:
                self.topic_entities.append(set(saved_entities))
            elif title:
                self.topic_entities.append(extract_key_entities(title))
            else:
                self.topic_entities.append(set())
            
            # Content hash
            chash = item.get("content_hash", "")
            if chash:
                self.content_hashes.add(chash)
            
            # üÜï Topic
            topic = item.get("topic", Topic.GENERAL)
            self.topics.append(topic)

    def _save(self):
        """–ê—Ç–æ–º–∞—Ä–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"""
        try:
            dir_name = os.path.dirname(self.file) or '.'
            fd, tmp_path = tempfile.mkstemp(suffix='.json', dir=dir_name)
            with os.fdopen(fd, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
            shutil.move(tmp_path, self.file)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

    def is_duplicate(self, url: str, title: str, summary: str = "") -> bool:
        """
        4-—É—Ä–æ–≤–Ω–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç:
        1. URL (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π)
        2. –•–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        3. –ü–æ—Ö–æ–∂–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ (fuzzy)
        4. –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        """
        
        # === 1. URL ===
        norm_url = normalize_url(url)
        if norm_url in self.urls:
            logger.info(f"üö´ [URL] –î—É–±–ª–∏–∫–∞—Ç: {title[:50]}...")
            return True

        # === 2. –•–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ===
        if summary:
            chash = get_content_hash(summary)
            if chash and chash in self.content_hashes:
                logger.info(f"üö´ [HASH] –î—É–±–ª–∏–∫–∞—Ç: {title[:50]}...")
                return True

        # === 3. –ü–æ—Ö–æ–∂–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ ===
        title_len = len(title)
        for i, existing_title in enumerate(self.titles):
            if not existing_title:
                continue
            
            # –ë—ã—Å—Ç—Ä—ã–π —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–ª–∏–Ω–µ
            if abs(len(existing_title) - title_len) > title_len * 0.6:
                continue
            
            sim = calculate_similarity(title, existing_title)
            if sim > config.similarity_threshold:
                logger.info(f"üö´ [TITLE {int(sim*100)}%] '{title[:35]}' ‚âà '{existing_title[:35]}'")
                return True

        # === 4. –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π ===
        full_text = f"{title} {summary}".strip()
        new_entities = extract_key_entities(full_text)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π
        if len(new_entities) >= 2:
            for i, existing_entities in enumerate(self.topic_entities):
                if len(existing_entities) < 2:
                    continue
                
                common = new_entities & existing_entities
                
                # –°—á–∏—Ç–∞–µ–º overlap –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ–≥–æ –Ω–∞–±–æ—Ä–∞
                min_size = min(len(new_entities), len(existing_entities))
                overlap_ratio = len(common) / min_size if min_size > 0 else 0
                
                # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç 2+ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ overlap > –ø–æ—Ä–æ–≥–∞
                if len(common) >= 2 and overlap_ratio >= config.entity_overlap_threshold:
                    existing_title = self.titles[i] if i < len(self.titles) else "?"
                    logger.info(f"üö´ [TOPIC] –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {common} | '{existing_title[:35]}'")
                    return True
        
        return False

    # üÜï –ü–†–û–í–ï–†–ö–ê –†–ê–ó–ù–û–û–ë–†–ê–ó–ò–Ø –° –ü–û–°–õ–ï–î–ù–ò–ú–ò –ü–û–°–¢–ê–ú–ò
    def is_too_similar_to_recent(self, title: str, summary: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –ø–æ—Ö–æ–∂–∞ —Å—Ç–∞—Ç—å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤
        –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è —Å–≤–µ–∂–∏—Ö –ø–æ—Å—Ç–æ–≤
        """
        if len(self.data) < 2:
            return False
        
        recent_posts = self.data[-config.recent_posts_check:]
        full_text = f"{title} {summary}".strip()
        new_entities = extract_key_entities(full_text)
        detected_topic = Topic.detect(full_text)
        
        for post in recent_posts:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –ü–æ—Ö–æ–∂–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ (—Å—Ç—Ä–æ–∂–µ)
            post_title = post.get("title", "")
            if post_title:
                sim = calculate_similarity(title, post_title)
                if sim > config.recent_similarity_threshold:
                    logger.info(f"üîÑ [RECENT] –°–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–π –ø–æ—Å—Ç: {post_title[:40]}")
                    return True
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ–º—ã + —Å—É—â–Ω–æ—Å—Ç–µ–π
            post_topic = post.get("topic", "")
            post_entities = set(post.get("entities", []))
            
            if detected_topic == post_topic and post_entities:
                common = new_entities & post_entities
                if len(common) >= config.min_entity_distance:
                    logger.info(f"üîÑ [RECENT] –¢–∞ –∂–µ —Ç–µ–º–∞ '{detected_topic}' —Å –ø–æ—Ö–æ–∂–∏–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏: {common}")
                    return True
        
        return False
    
    # üÜï –ü–û–õ–£–ß–ò–¢–¨ –°–¢–ê–¢–ò–°–¢–ò–ö–£ –ü–û–°–õ–ï–î–ù–ò–• –ü–û–°–¢–û–í
    def get_recent_topics_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–º–∞–º –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤"""
        if len(self.data) < 3:
            return {}
        
        recent = self.data[-10:]
        stats = {}
        for post in recent:
            topic = post.get("topic", Topic.GENERAL)
            stats[topic] = stats.get(topic, 0) + 1
        
        return stats

    def add(self, url: str, title: str, summary: str = "", topic: str = Topic.GENERAL):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—å—é –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        norm_url = normalize_url(url)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª—É—á–∞–π –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        if norm_url in self.urls:
            logger.debug(f"–£–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ: {title[:40]}")
            return
        
        chash = get_content_hash(summary) if summary else ""
        full_text = f"{title} {summary}".strip()
        entities = extract_key_entities(full_text)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à–∏
        self.urls.add(norm_url)
        self.titles.append(title)
        self.topic_entities.append(entities)
        self.topics.append(topic)
        if chash:
            self.content_hashes.add(chash)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å
        self.data.append({
            "url": url,
            "norm_url": norm_url,
            "title": title[:200],
            "content_hash": chash,
            "entities": list(entities),
            "topic": topic,
            "ts": datetime.now(timezone.utc).isoformat() + "Z"
        })
        
        self._save()
        logger.info(f"üíæ [{topic.upper()}] {title[:45]}... | –°—É—â–Ω–æ—Å—Ç–∏: {entities if entities else '–Ω–µ—Ç'}")

    def cleanup(self, days: int = 30):
        """–£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π"""
        cutoff = datetime.now(timezone.utc).timestamp() - days * 86400
        old_count = len(self.data)
        
        self.data = [
            item for item in self.data
            if self._parse_ts(item.get("ts")) > cutoff
        ]
        
        removed = old_count - len(self.data)
        if removed > 0:
            self._rebuild_caches()
            self._save()
            logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–æ {removed} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π")

    def _parse_ts(self, ts: Optional[str]) -> float:
        if not ts:
            return 0
        try:
            return datetime.fromisoformat(ts.replace("Z", "+00:00")).timestamp()
        except:
            return 0

    def __del__(self):
        self._release_lock()

# ====================== RSS LOADER ======================
async def fetch_feed(session: aiohttp.ClientSession, url: str, source: str, posted: PostedManager) -> List[Article]:
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=20)) as resp:
            if resp.status != 200:
                logger.warning(f"{source}: HTTP {resp.status}")
                return []
            text = await resp.text()
    except Exception as e:
        logger.warning(f"{source}: {e}")
        return []

    try:
        feed = feedparser.parse(text)
    except:
        return []

    articles = []
    for entry in feed.entries[:25]:
        link = entry.get("link", "").strip()
        title = clean_text(entry.get("title") or "")
        summary = clean_text(entry.get("summary") or entry.get("description") or "")[:1500]

        if not link or len(title) < 15:
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–µ–π –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        if posted.is_duplicate(link, title, summary):
            continue

        published = datetime.now(timezone.utc)
        for df in ["published", "updated", "created"]:
            ds = entry.get(df)
            if ds:
                try:
                    parsed = feedparser._parse_date(ds)
                    if parsed:
                        published = datetime(*parsed[:6], tzinfo=timezone.utc)
                        break
                except:
                    pass

        articles.append(Article(
            title=title,
            summary=summary,
            link=link,
            source=source,
            published=published
        ))

    return articles

async def load_all_feeds(posted: PostedManager) -> List[Article]:
    logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ RSS...")
    
    conn = aiohttp.TCPConnector(limit=30)
    async with aiohttp.ClientSession(headers=HEADERS, connector=conn) as session:
        tasks = [fetch_feed(session, url, name, posted) for url, name in RSS_FEEDS]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    all_articles = []
    for i, res in enumerate(results):
        source_name = RSS_FEEDS[i][1]
        if isinstance(res, list) and res:
            all_articles.extend(res)
            logger.info(f"  ‚úì {source_name}: {len(res)} –Ω–æ–≤—ã—Ö")
        elif isinstance(res, Exception):
            logger.error(f"  ‚úó {source_name}: {res}")

    logger.info(f"üìä –í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_articles)}")
    return all_articles

# ====================== FILTER ======================
def filter_articles(articles: List[Article], posted: PostedManager) -> List[Article]:
    candidates = []
    
    # üÜï –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ç–µ–º
    recent_stats = posted.get_recent_topics_stats()
    logger.info(f"üìä –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ç–µ–º—ã: {recent_stats}")
    
    for a in articles:
        text = f"{a.title} {a.summary}".lower()
        
        if any(p in text for p in BAD_PHRASES):
            continue
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue
        if not any(kw in text for kw in AI_KEYWORDS):
            continue
        if ai_relevance(text) < 0.4:
            continue
        
        # üÜï –ü–†–û–í–ï–†–ö–ê –ù–ê –ü–û–•–û–ñ–ï–°–¢–¨ –° –ü–û–°–õ–ï–î–ù–ò–ú–ò –ü–û–°–¢–ê–ú–ò
        if posted.is_too_similar_to_recent(a.title, a.summary):
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (—Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ): {a.title[:40]}")
            continue
        
        candidates.append(a)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (—Å–≤–µ–∂–∏–µ –ø–µ—Ä–≤—ã–µ)
    candidates.sort(key=lambda x: x.published, reverse=True)
    
    # üÜï –ü–†–ò–û–†–ò–¢–ï–¢ –†–ê–ó–ù–´–ú –¢–ï–ú–ê–ú
    # –ï—Å–ª–∏ –æ–¥–Ω–∞ —Ç–µ–º–∞ –ø—Ä–µ–æ–±–ª–∞–¥–∞–µ—Ç –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–∞—Ö, –æ—Ç–¥–∞—ë–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥—Ä—É–≥–∏–º
    if recent_stats:
        dominant_topic = max(recent_stats, key=recent_stats.get)
        if recent_stats[dominant_topic] >= 3:  # –ï—Å–ª–∏ 3+ –ø–æ—Å—Ç–∞ –ø–æ–¥—Ä—è–¥ –æ–± –æ–¥–Ω–æ–º
            logger.info(f"‚öñÔ∏è –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é (–º–Ω–æ–≥–æ '{dominant_topic}' –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö)")
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –¥–æ–º–∏–Ω–∞–Ω—Ç–Ω—É—é —Ç–µ–º—É –∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
            other_topics = []
            same_topic = []
            
            for art in candidates:
                detected = Topic.detect(f"{art.title} {art.summary}")
                if detected == dominant_topic:
                    same_topic.append(art)
                else:
                    other_topics.append(art)
            
            # –°–Ω–∞—á–∞–ª–∞ –¥—Ä—É–≥–∏–µ —Ç–µ–º—ã, –ø–æ—Ç–æ–º –¥–æ–º–∏–Ω–∞–Ω—Ç–Ω–∞—è
            candidates = other_topics + same_topic
    
    logger.info(f"üéØ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤: {len(candidates)} —Å—Ç–∞—Ç–µ–π")
    return candidates

# ====================== –ì–ï–ù–ï–†–ê–¢–û–† –ü–û–°–¢–û–í ======================
GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "llama3-70b-8192",
]

async def generate_summary(article: Article) -> Optional[str]:
    logger.info(f"üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {article.title[:55]}...")
    
    prompt = f"""–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –∫—Ä—É–ø–Ω–æ–≥–æ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –ò–ò –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.

–ù–û–í–û–°–¢–¨:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}
–¢–µ–∫—Å—Ç: {article.summary[:2200]}
–ò—Å—Ç–æ—á–Ω–∏–∫: {article.source}

–ó–ê–î–ê–ß–ê: –ù–∞–ø–∏—à–∏ –ø–æ—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–°–¢–†–£–ö–¢–£–†–ê (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):
1. üî• –ó–ê–ì–û–õ–û–í–û–ö ‚Äî —Ü–µ–ø–ª—è—é—â–∏–π, —Å —ç–º–æ–¥–∑–∏, –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å—É—Ç—å
2. –ß–¢–û –°–õ–£–ß–ò–õ–û–°–¨ ‚Äî 3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —Ñ–∞–∫—Ç–∞–º–∏ (–∫—Ç–æ, —á—Ç–æ, –∫–æ–≥–¥–∞, —Ü–∏—Ñ—Ä—ã)
3. –ü–û–ß–ï–ú–£ –í–ê–ñ–ù–û ‚Äî 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ –≤–ª–∏—è–Ω–∏–∏ –Ω–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—é/–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π  
4. –í–´–í–û–î ‚Äî –æ—Å—Ç—Ä—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏–ª–∏ –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –î–ª–∏–Ω–∞: 600-850 —Å–∏–º–≤–æ–ª–æ–≤ (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û)
- –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã, –Ω–∏–∫–∞–∫–æ–π –≤–æ–¥—ã
- –ö–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã

–ó–ê–ü–†–ï–©–ï–ù–û:
- –§—Ä–∞–∑—ã: "—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å", "–≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —á—Ç–æ", "–¥—Ä—É–∑—å—è"
- –®–∞–±–ª–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Ç–∏–ø–∞ "–ß—Ç–æ –¥—É–º–∞–µ—Ç–µ?"
- –ü—É—Å—Ç—ã–µ –æ–±–æ–±—â–µ–Ω–∏—è –±–µ–∑ —Ñ–∞–∫—Ç–æ–≤

–•–û–†–û–®–ò–ï –í–û–ü–†–û–°–´:
‚úì "Google —Å–Ω–æ–≤–∞ –¥–æ–≥–æ–Ω—è–µ—Ç ‚Äî –∏–ª–∏ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –æ–±–≥–æ–Ω–∏—Ç?"
‚úì "–°–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ –ø–æ—Ö–æ—Ä–æ–Ω–∏—Ç —ç—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ?"

–ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å ‚Äî –º—É—Å–æ—Ä/—Ä–µ–∫–ª–∞–º–∞/–Ω–µ –ø—Ä–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –æ—Ç–≤–µ—Ç—å: SKIP

–ü–û–°–¢:"""

    for attempt in range(3):
        try:
            await asyncio.sleep(0.5)
            
            resp = await asyncio.to_thread(
                groq_client.chat.completions.create,
                model=random.choice(GROQ_MODELS),
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1100,
            )
            text = resp.choices[0].message.content.strip()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ SKIP
            if "SKIP" in text.upper()[:15]:
                logger.info("  ‚è≠Ô∏è LLM: SKIP")
                return None

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
            if len(text) < config.min_post_length:
                logger.warning(f"  ‚ö†Ô∏è –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤.), –ø–æ–≤—Ç–æ—Ä...")
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–æ–¥—É
            water = ["—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å", "–≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ", 
                    "–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º—Å—è", "–Ω–µ —Å–µ–∫—Ä–µ—Ç", "–æ—á–µ–≤–∏–¥–Ω–æ, —á—Ç–æ"]
            if any(w in text.lower() for w in water):
                logger.warning("  ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤–æ–¥–∞, –ø–æ–≤—Ç–æ—Ä...")
                continue

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç
            topic = Topic.detect(f"{article.title} {article.summary}")
            hashtags = Topic.HASHTAGS.get(topic, Topic.HASHTAGS[Topic.GENERAL])
            
            cta = "\n\nüî• ‚Äî –æ–≥–æ–Ω—å  |  üóø ‚Äî –Ω—É —Ç–∞–∫–æ–µ  |  ‚ö° ‚Äî –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ"
            source_link = f'\n\nüîó <a href="{article.link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
            
            final = f"{text}{cta}\n\n{hashtags}{source_link}"

            # –û–±—Ä–µ–∑–∫–∞ –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç
            if len(final) > config.caption_limit:
                excess = len(final) - config.caption_limit + 20
                text = text[:-excess]
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É/–≤–æ–ø—Ä–æ—Å
                for p in ['. ', '! ', '? ']:
                    idx = text.rfind(p)
                    if idx > len(text) * 0.6:
                        text = text[:idx+1]
                        break
                final = f"{text}{cta}\n\n{hashtags}{source_link}"

            logger.info(f"  ‚úÖ –ì–æ—Ç–æ–≤–æ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ | –¢–µ–º–∞: {topic}")
            return final
            
        except Exception as e:
            logger.error(f"  ‚ùå Groq –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
            await asyncio.sleep(2)

    return None

# ====================== –ö–ê–†–¢–ò–ù–ö–ò ======================
async def generate_image(title: str) -> Optional[str]:
    logger.info("  üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏...")
    
    clean = re.sub(r'[^\w\s]', '', title)[:50]
    prompt = f"tech editorial illustration {clean} neon purple blue dark background 8k"
    url = f"https://image.pollinations.ai/prompt/{quote(prompt)}?width=1024&height=1024&nologo=true&seed={random.randint(1,99999)}"
    
    for attempt in range(2):
        try:
            async with aiohttp.ClientSession() as sess:
                async with sess.get(url, timeout=aiohttp.ClientTimeout(total=40)) as resp:
                    if resp.status != 200:
                        continue
                    data = await resp.read()
                    if len(data) < 10000:
                        continue
                    
                    fname = f"img_{random.randint(1000,9999)}.jpg"
                    with open(fname, "wb") as f:
                        f.write(data)
                    logger.info(f"  ‚úÖ –ö–∞—Ä—Ç–∏–Ω–∫–∞: {fname}")
                    return fname
        except:
            await asyncio.sleep(2)
    
    logger.warning("  ‚ö†Ô∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞")
    return None

# ====================== –ü–£–ë–õ–ò–ö–ê–¶–ò–Ø ======================
async def post_article(article: Article, text: str, posted: PostedManager) -> bool:
    img = await generate_image(article.title)
    
    try:
        if img and os.path.exists(img):
            await bot.send_photo(config.channel_id, FSInputFile(img), caption=text)
            os.remove(img)
        else:
            await bot.send_message(config.channel_id, text, disable_web_page_preview=False)
        
        # üÜï –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å —Ç–µ–º–æ–π
        topic = Topic.detect(f"{article.title} {article.summary}")
        posted.add(article.link, article.title, article.summary, topic)
        
        logger.info(f"‚úÖ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û [{topic.upper()}]: {article.title[:50]}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Telegram –æ—à–∏–±–∫–∞: {e}")
        if img and os.path.exists(img):
            try:
                os.remove(img)
            except:
                pass
        return False

# ====================== MAIN ======================
async def main():
    logger.info("=" * 50)
    logger.info("üöÄ –ó–ê–ü–£–°–ö AI-POSTER v2.0")
    logger.info("=" * 50)
    
    posted = PostedManager(config.posted_file)
    posted.cleanup(config.retention_days)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
    raw_articles = await load_all_feeds(posted)
    candidates = filter_articles(raw_articles, posted)
    
    if not candidates:
        logger.info("üì≠ –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return

    # –ü—Ä–æ–±—É–µ–º –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –æ–¥–Ω—É —Å—Ç–∞—Ç—å—é
    for article in candidates[:20]:  # üÜï –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ 20 –ø–æ–ø—ã—Ç–æ–∫
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
        if posted.is_duplicate(article.link, article.title, article.summary):
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (–¥—É–±–ª—å): {article.title[:40]}")
            continue
        
        # üÜï –ï—â—ë –æ–¥–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
        if posted.is_too_similar_to_recent(article.title, article.summary):
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (–ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ): {article.title[:40]}")
            continue
        
        summary = await generate_summary(article)
        if not summary:
            continue
        
        if await post_article(article, summary, posted):
            logger.info("\nüèÅ –ì–æ—Ç–æ–≤–æ! –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à—ë–Ω.")
            break
        
        await asyncio.sleep(3)
    else:
        logger.info("üòî –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏")

    await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())






























































































































































































































































