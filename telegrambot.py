import os
import json
import asyncio
import random
import re
import hashlib
import logging
import difflib
import tempfile
import shutil
from datetime import datetime, timezone
from typing import List, Set, Optional
from urllib.parse import urlparse, quote
from dataclasses import dataclass, field

import aiohttp
import feedparser
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq

# –î–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–æ–≤ (Linux/Mac)
try:
    import fcntl
    HAS_FCNTL = True
except ImportError:
    HAS_FCNTL = False

# ====================== –õ–û–ì–ò ======================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("ai_poster.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ====================== CONFIG ======================
class Config:
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.telegram_token = os.getenv("TELEGRAM_BOT_TOKEN")
        self.channel_id = os.getenv("CHANNEL_ID")
        self.retention_days = int(os.getenv("RETENTION_DAYS", "30"))
        self.caption_limit = 1024
        self.posted_file = "posted_articles.json"
        
        self.similarity_threshold = 0.60  # –ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        self.entity_overlap_threshold = 0.55  # –ü–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.min_post_length = 500
        
        # –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø –†–ê–ó–ù–û–û–ë–†–ê–ó–ò–Ø
        self.recent_posts_check = 5  # –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
        self.recent_similarity_threshold = 0.45  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤
        self.min_entity_distance = 2  # –ú–∏–Ω. –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π

        missing = []
        for var, name in [(self.groq_api_key, "GROQ_API_KEY"),
                          (self.telegram_token, "TELEGRAM_BOT_TOKEN"),
                          (self.channel_id, "CHANNEL_ID")]:
            if not var:
                missing.append(name)
        if missing:
            raise SystemExit(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(missing)}")

config = Config()

bot = Bot(token=config.telegram_token, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=config.groq_api_key)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

# ====================== RSS (–†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö) ======================
RSS_FEEDS = [
    # –û—Å–Ω–æ–≤–Ω—ã–µ
    ("https://techcrunch.com/category/artificial-intelligence/feed/", "TechCrunch"),
    ("https://venturebeat.com/category/ai/feed/", "VentureBeat"),
    ("https://www.technologyreview.com/topic/artificial-intelligence/feed", "MIT Tech Review"),
    ("https://www.theverge.com/rss/index.xml", "The Verge"),
    ("https://arstechnica.com/tag/artificial-intelligence/feed/", "Ars Technica"),
    ("https://www.wired.com/feed/tag/ai/latest/rss", "WIRED"),
    
    # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò
    ("https://www.artificialintelligence-news.com/feed/", "AI News"),
    ("https://hai.stanford.edu/news/rss.xml", "Stanford HAI"),
    ("https://deepmind.google/blog/rss.xml", "DeepMind Blog"),
    ("https://openai.com/blog/rss/", "OpenAI Blog"),
    ("https://blog.google/technology/ai/rss/", "Google AI Blog"),
    ("https://www.marktechpost.com/feed/", "MarkTechPost"),
    ("https://syncedreview.com/feed/", "Synced AI"),
    ("https://news.ycombinator.com/rss", "Hacker News"),
    ("https://www.unite.ai/feed/", "Unite.AI"),
    ("https://analyticsindiamag.com/feed/", "AIM"),
]

# ====================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ======================
AI_KEYWORDS = [
    "ai ", " ai", "artificial intelligence", "machine learning", "deep learning",
    "neural network", "llm", "large language model", "gpt", "chatgpt", "claude",
    "gemini", "grok", "llama", "mistral", "qwen", "deepseek", "midjourney",
    "dall-e", "stable diffusion", "sora", "groq", "openai", "anthropic",
    "deepmind", "hugging face", "nvidia", "agi", "transformer", "generative",
    "agents", "reasoning", "multimodal", "fine-tuning", "rlhf"
]

EXCLUDE_KEYWORDS = [
    "stock price", "ipo", "earnings call", "quarterly results", "dividend",
    "market cap", "wall street", "ps5", "xbox", "nintendo", "game review",
    "netflix", "movie review", "box office", "trailer", "tesla stock",
    "bitcoin", "crypto", "blockchain", "nft", "ethereum", "election",
    "trump", "biden", "congress", "senate"
]

BAD_PHRASES = ["sponsored", "partner content", "advertisement", "black friday", "deal alert"]

# ====================== –ö–õ–Æ–ß–ï–í–´–ï –°–£–©–ù–û–°–¢–ò –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò –î–£–ë–õ–ï–ô ======================
KEY_ENTITIES = [
    # –ö–æ–º–ø–∞–Ω–∏–∏
    "openai", "google", "meta", "microsoft", "anthropic", "nvidia", "apple",
    "amazon", "deepmind", "hugging face", "stability ai", "midjourney",
    "mistral", "cohere", "perplexity", "runway", "pika", "character ai",
    "inflection", "xai", "baidu", "alibaba", "tencent", "bytedance",
    
    # –ü—Ä–æ–¥—É–∫—Ç—ã –∏ –º–æ–¥–µ–ª–∏
    "gpt-4", "gpt-5", "gpt-4o", "gpt-4.5", "chatgpt", "claude", "claude 3",
    "gemini", "gemini 2", "llama", "llama 3", "mistral", "mixtral",
    "copilot", "dall-e", "dall-e 3", "sora", "stable diffusion", "flux",
    "midjourney v6", "runway gen", "firefly", "imagen",
    
    # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
    "linux foundation", "agentic", "ai agent", "agi", "asi",
    "regulation", "safety", "alignment", "open source", "open-source",
    "robotics", "humanoid", "boston dynamics", "figure", "optimus",
    
    # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    "transformer", "diffusion", "multimodal", "reasoning", "chain of thought",
    "fine-tuning", "rlhf", "inference", "training", "benchmark"
]

# ====================== DATACLASS ======================
@dataclass
class Article:
    title: str
    summary: str
    link: str
    source: str
    published: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

# ====================== TOPIC & HASHTAGS ======================
class Topic:
    LLM = "llm"
    IMAGE_GEN = "image_gen"
    ROBOTICS = "robotics"
    HARDWARE = "hardware"
    REGULATION = "regulation"
    RESEARCH = "research"
    GENERAL = "general"
    
    HASHTAGS = {
        LLM: "#ChatGPT #LLM #OpenAI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        IMAGE_GEN: "#Midjourney #DALLE #StableDiffusion #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
        ROBOTICS: "#—Ä–æ–±–æ—Ç—ã #Humanoid #—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
        HARDWARE: "#NVIDIA #GPU #—á–∏–ø—ã #–∂–µ–ª–µ–∑–æ",
        REGULATION: "#—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ #–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å #—ç—Ç–∏–∫–∞",
        RESEARCH: "#–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è #–Ω–∞—É–∫–∞ #DeepMind",
        GENERAL: "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #–ò–ò"
    }
    
    # –°–¢–ò–õ–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô –ü–û –¢–ï–ú–ê–ú (–±–µ–∑ –∫–∏–±–µ—Ä–ø–∞–Ω–∫–∞!)
    IMAGE_STYLES = {
        LLM: [
            "clean minimalist illustration, chat interface, soft blue and white gradient, modern UI design, professional",
            "friendly robot assistant illustration, soft colors, white background, cute character design",
            "abstract conversation bubbles, flowing shapes, light blue tones, editorial style illustration",
            "modern flat design, speech bubbles and text symbols, pastel colors, tech magazine cover",
        ],
        IMAGE_GEN: [
            "artistic watercolor illustration, creative palette, splashes of color, gallery aesthetic",
            "paintbrush and canvas artistic concept, warm colors, creative studio atmosphere",
            "abstract art composition, flowing colors, creative expression, museum quality",
            "digital art creation concept, colorful gradients, artistic tools, inspiring atmosphere",
        ],
        ROBOTICS: [
            "technical blueprint illustration, soft gray background, precise mechanical drawings, engineering style",
            "friendly humanoid robot, soft studio lighting, white background, product photography style",
            "isometric robot illustration, clean lines, soft shadows, modern industrial design",
            "robotic arm in laboratory setting, clean environment, professional photography style",
        ],
        HARDWARE: [
            "product photography of tech hardware, studio lighting, reflective surfaces, premium feel",
            "clean circuit board illustration, green and gold tones, technical precision, macro style",
            "isometric computer chip illustration, metallic textures, soft gradients, professional",
            "modern data center visualization, clean rows of servers, soft blue lighting, corporate",
        ],
        REGULATION: [
            "corporate illustration, scales of justice with tech elements, muted blue tones, professional",
            "formal document and gavel illustration, clean design, government style, serious tone",
            "handshake between human and robot, diplomatic setting, soft neutral colors, editorial",
            "policy document with AI symbols, clean infographic style, trustworthy blue palette",
        ],
        RESEARCH: [
            "scientific laboratory illustration, clean white environment, research equipment, academic",
            "brain and neural connections visualization, soft purple and blue, medical illustration style",
            "scientist working with data, modern lab setting, clean aesthetic, educational",
            "abstract knowledge graph, interconnected nodes, soft colors, scientific visualization",
        ],
        GENERAL: [
            "modern flat illustration, geometric shapes, pastel gradient colors, editorial magazine style",
            "clean tech illustration, simple icons, white background, professional presentation",
            "isometric technology concept, soft shadows, modern design, business friendly",
            "minimalist abstract design, flowing lines, soft blue and white, corporate clean",
        ],
    }

    @staticmethod
    def detect(text: str) -> str:
        t = text.lower()
        if any(x in t for x in ["gpt", "chatgpt", "claude", "gemini", "llama", "grok", "llm"]):
            return Topic.LLM
        if any(x in t for x in ["midjourney", "dall-e", "stable diffusion", "flux", "sora"]):
            return Topic.IMAGE_GEN
        if any(x in t for x in ["robot", "humanoid", "boston dynamics", "optimus", "figure"]):
            return Topic.ROBOTICS
        if any(x in t for x in ["nvidia", "h100", "h200", "blackwell", "gpu", "cuda"]):
            return Topic.HARDWARE
        if any(x in t for x in ["regulation", "safety", "alignment", "ethics", "policy"]):
            return Topic.REGULATION
        if any(x in t for x in ["research", "paper", "study", "breakthrough", "discovery"]):
            return Topic.RESEARCH
        return Topic.GENERAL
    
    @staticmethod
    def get_image_style(topic: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ç–µ–º—ã"""
        styles = Topic.IMAGE_STYLES.get(topic, Topic.IMAGE_STYLES[Topic.GENERAL])
        return random.choice(styles)

# ====================== HELPERS ======================
def normalize_url(url: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    if not url:
        return ""
    try:
        url = url.strip()
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace("www.", "")
        path = parsed.path.rstrip("/")
        return f"{parsed.scheme}://{domain}{path}"
    except:
        return url.split("?")[0].split("#")[0]

def calculate_similarity(text1: str, text2: str) -> float:
    """–°—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Å—Ç—Ä–æ–∫ (0.0 - 1.0)"""
    return difflib.SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def extract_key_entities(text: str) -> Set[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_lower = text.lower()
    found = set()
    
    for entity in KEY_ENTITIES:
        if entity in text_lower:
            normalized = entity.replace("-", " ").replace("_", " ")
            found.add(normalized)
    
    return found

def clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def ai_relevance(text: str) -> float:
    lower = text.lower()
    matches = sum(1 for kw in AI_KEYWORDS if kw in lower)
    return min(matches / 3.0, 1.0)

def get_content_hash(text: str) -> str:
    """MD5 —Ö–µ—à –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    if not text:
        return ""
    normalized = re.sub(r'\s+', ' ', text.strip().lower())
    return hashlib.md5(normalized[:500].encode()).hexdigest()[:16]

# ====================== POSTED MANAGER ======================
class PostedManager:
    def __init__(self, file="posted_articles.json"):
        self.file = file
        self.lock_file = file + ".lock"
        self.data: List[dict] = []
        self.urls: Set[str] = set()
        self.titles: List[str] = []
        self.content_hashes: Set[str] = set()
        self.topic_entities: List[Set[str]] = []
        self.topics: List[str] = []
        self._lock_fd = None
        
        self._acquire_lock()
        self._load()

    def _acquire_lock(self):
        """–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
        if not HAS_FCNTL:
            if os.path.exists(self.lock_file):
                try:
                    age = datetime.now().timestamp() - os.path.getmtime(self.lock_file)
                    if age < 600:
                        logger.warning("‚ö†Ô∏è –î—Ä—É–≥–æ–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç. –í—ã—Ö–æ–¥.")
                        raise SystemExit(0)
                except OSError:
                    pass
            with open(self.lock_file, 'w') as f:
                f.write(str(os.getpid()))
            return
        
        self._lock_fd = open(self.lock_file, 'w')
        try:
            fcntl.flock(self._lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
            self._lock_fd.write(str(os.getpid()))
            self._lock_fd.flush()
        except BlockingIOError:
            logger.warning("‚ö†Ô∏è –°–∫—Ä–∏–ø—Ç —É–∂–µ –∑–∞–ø—É—â–µ–Ω. –í—ã—Ö–æ–¥.")
            raise SystemExit(0)

    def _release_lock(self):
        try:
            if HAS_FCNTL and self._lock_fd:
                fcntl.flock(self._lock_fd, fcntl.LOCK_UN)
                self._lock_fd.close()
            if os.path.exists(self.lock_file):
                os.remove(self.lock_file)
        except:
            pass

    def _load(self):
        if not os.path.exists(self.file):
            self._save()
            return
        
        try:
            with open(self.file, "r", encoding="utf-8") as f:
                self.data = json.load(f)
            self._rebuild_caches()
            logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} —Å—Ç–∞—Ç–µ–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            self.data = []

    def _rebuild_caches(self):
        """–ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤—Å–µ –∫—ç—à–∏ –∏–∑ self.data"""
        self.urls.clear()
        self.titles.clear()
        self.content_hashes.clear()
        self.topic_entities.clear()
        self.topics.clear()
        
        for item in self.data:
            url = item.get("url", "")
            if url:
                self.urls.add(normalize_url(url))
            
            title = item.get("title", "")
            if title:
                self.titles.append(title)
            else:
                self.titles.append("")
            
            saved_entities = item.get("entities", [])
            if saved_entities:
                self.topic_entities.append(set(saved_entities))
            elif title:
                self.topic_entities.append(extract_key_entities(title))
            else:
                self.topic_entities.append(set())
            
            chash = item.get("content_hash", "")
            if chash:
                self.content_hashes.add(chash)
            
            topic = item.get("topic", Topic.GENERAL)
            self.topics.append(topic)

    def _save(self):
        """–ê—Ç–æ–º–∞—Ä–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"""
        try:
            dir_name = os.path.dirname(self.file) or '.'
            fd, tmp_path = tempfile.mkstemp(suffix='.json', dir=dir_name)
            with os.fdopen(fd, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
            shutil.move(tmp_path, self.file)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

    def is_duplicate(self, url: str, title: str, summary: str = "") -> bool:
        """
        4-—É—Ä–æ–≤–Ω–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç:
        1. URL (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π)
        2. –•–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        3. –ü–æ—Ö–æ–∂–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ (fuzzy)
        4. –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        """
        
        norm_url = normalize_url(url)
        if norm_url in self.urls:
            logger.info(f"üö´ [URL] –î—É–±–ª–∏–∫–∞—Ç: {title[:50]}...")
            return True

        if summary:
            chash = get_content_hash(summary)
            if chash and chash in self.content_hashes:
                logger.info(f"üö´ [HASH] –î—É–±–ª–∏–∫–∞—Ç: {title[:50]}...")
                return True

        title_len = len(title)
        for i, existing_title in enumerate(self.titles):
            if not existing_title:
                continue
            
            if abs(len(existing_title) - title_len) > title_len * 0.6:
                continue
            
            sim = calculate_similarity(title, existing_title)
            if sim > config.similarity_threshold:
                logger.info(f"üö´ [TITLE {int(sim*100)}%] '{title[:35]}' ‚âà '{existing_title[:35]}'")
                return True

        full_text = f"{title} {summary}".strip()
        new_entities = extract_key_entities(full_text)
        
        if len(new_entities) >= 2:
            for i, existing_entities in enumerate(self.topic_entities):
                if len(existing_entities) < 2:
                    continue
                
                common = new_entities & existing_entities
                min_size = min(len(new_entities), len(existing_entities))
                overlap_ratio = len(common) / min_size if min_size > 0 else 0
                
                if len(common) >= 2 and overlap_ratio >= config.entity_overlap_threshold:
                    existing_title = self.titles[i] if i < len(self.titles) else "?"
                    logger.info(f"üö´ [TOPIC] –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {common} | '{existing_title[:35]}'")
                    return True
        
        return False

    def is_too_similar_to_recent(self, title: str, summary: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –ø–æ—Ö–æ–∂–∞ —Å—Ç–∞—Ç—å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤
        """
        if len(self.data) < 2:
            return False
        
        recent_posts = self.data[-config.recent_posts_check:]
        full_text = f"{title} {summary}".strip()
        new_entities = extract_key_entities(full_text)
        detected_topic = Topic.detect(full_text)
        
        for post in recent_posts:
            post_title = post.get("title", "")
            if post_title:
                sim = calculate_similarity(title, post_title)
                if sim > config.recent_similarity_threshold:
                    logger.info(f"üîÑ [RECENT] –°–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–π –ø–æ—Å—Ç: {post_title[:40]}")
                    return True
            
            post_topic = post.get("topic", "")
            post_entities = set(post.get("entities", []))
            
            if detected_topic == post_topic and post_entities:
                common = new_entities & post_entities
                if len(common) >= config.min_entity_distance:
                    logger.info(f"üîÑ [RECENT] –¢–∞ –∂–µ —Ç–µ–º–∞ '{detected_topic}' —Å –ø–æ—Ö–æ–∂–∏–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏: {common}")
                    return True
        
        return False
    
    def get_recent_topics_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–º–∞–º –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤"""
        if len(self.data) < 3:
            return {}
        
        recent = self.data[-10:]
        stats = {}
        for post in recent:
            topic = post.get("topic", Topic.GENERAL)
            stats[topic] = stats.get(topic, 0) + 1
        
        return stats

    def add(self, url: str, title: str, summary: str = "", topic: str = Topic.GENERAL):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—å—é –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        norm_url = normalize_url(url)
        
        if norm_url in self.urls:
            logger.debug(f"–£–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ: {title[:40]}")
            return
        
        chash = get_content_hash(summary) if summary else ""
        full_text = f"{title} {summary}".strip()
        entities = extract_key_entities(full_text)
        
        self.urls.add(norm_url)
        self.titles.append(title)
        self.topic_entities.append(entities)
        self.topics.append(topic)
        if chash:
            self.content_hashes.add(chash)
        
        self.data.append({
            "url": url,
            "norm_url": norm_url,
            "title": title[:200],
            "content_hash": chash,
            "entities": list(entities),
            "topic": topic,
            "ts": datetime.now(timezone.utc).isoformat() + "Z"
        })
        
        self._save()
        logger.info(f"üíæ [{topic.upper()}] {title[:45]}... | –°—É—â–Ω–æ—Å—Ç–∏: {entities if entities else '–Ω–µ—Ç'}")

    def cleanup(self, days: int = 30):
        """–£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π"""
        cutoff = datetime.now(timezone.utc).timestamp() - days * 86400
        old_count = len(self.data)
        
        self.data = [
            item for item in self.data
            if self._parse_ts(item.get("ts")) > cutoff
        ]
        
        removed = old_count - len(self.data)
        if removed > 0:
            self._rebuild_caches()
            self._save()
            logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–æ {removed} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π")

    def _parse_ts(self, ts: Optional[str]) -> float:
        if not ts:
            return 0
        try:
            return datetime.fromisoformat(ts.replace("Z", "+00:00")).timestamp()
        except:
            return 0

    def __del__(self):
        self._release_lock()

# ====================== RSS LOADER ======================
async def fetch_feed(session: aiohttp.ClientSession, url: str, source: str, posted: PostedManager) -> List[Article]:
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=20)) as resp:
            if resp.status != 200:
                logger.warning(f"{source}: HTTP {resp.status}")
                return []
            text = await resp.text()
    except Exception as e:
        logger.warning(f"{source}: {e}")
        return []

    try:
        feed = feedparser.parse(text)
    except:
        return []

    articles = []
    for entry in feed.entries[:25]:
        link = entry.get("link", "").strip()
        title = clean_text(entry.get("title") or "")
        summary = clean_text(entry.get("summary") or entry.get("description") or "")[:1500]

        if not link or len(title) < 15:
            continue
        
        if posted.is_duplicate(link, title, summary):
            continue

        published = datetime.now(timezone.utc)
        for df in ["published", "updated", "created"]:
            ds = entry.get(df)
            if ds:
                try:
                    parsed = feedparser._parse_date(ds)
                    if parsed:
                        published = datetime(*parsed[:6], tzinfo=timezone.utc)
                        break
                except:
                    pass

        articles.append(Article(
            title=title,
            summary=summary,
            link=link,
            source=source,
            published=published
        ))

    return articles

async def load_all_feeds(posted: PostedManager) -> List[Article]:
    logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ RSS...")
    
    conn = aiohttp.TCPConnector(limit=30)
    async with aiohttp.ClientSession(headers=HEADERS, connector=conn) as session:
        tasks = [fetch_feed(session, url, name, posted) for url, name in RSS_FEEDS]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    all_articles = []
    for i, res in enumerate(results):
        source_name = RSS_FEEDS[i][1]
        if isinstance(res, list) and res:
            all_articles.extend(res)
            logger.info(f"  ‚úì {source_name}: {len(res)} –Ω–æ–≤—ã—Ö")
        elif isinstance(res, Exception):
            logger.error(f"  ‚úó {source_name}: {res}")

    logger.info(f"üìä –í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_articles)}")
    return all_articles

# ====================== FILTER ======================
def filter_articles(articles: List[Article], posted: PostedManager) -> List[Article]:
    candidates = []
    
    recent_stats = posted.get_recent_topics_stats()
    logger.info(f"üìä –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ç–µ–º—ã: {recent_stats}")
    
    for a in articles:
        text = f"{a.title} {a.summary}".lower()
        
        if any(p in text for p in BAD_PHRASES):
            continue
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue
        if not any(kw in text for kw in AI_KEYWORDS):
            continue
        if ai_relevance(text) < 0.4:
            continue
        
        if posted.is_too_similar_to_recent(a.title, a.summary):
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (—Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ): {a.title[:40]}")
            continue
        
        candidates.append(a)

    candidates.sort(key=lambda x: x.published, reverse=True)
    
    if recent_stats:
        dominant_topic = max(recent_stats, key=recent_stats.get)
        if recent_stats[dominant_topic] >= 3:
            logger.info(f"‚öñÔ∏è –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é (–º–Ω–æ–≥–æ '{dominant_topic}' –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö)")
            
            other_topics = []
            same_topic = []
            
            for art in candidates:
                detected = Topic.detect(f"{art.title} {art.summary}")
                if detected == dominant_topic:
                    same_topic.append(art)
                else:
                    other_topics.append(art)
            
            candidates = other_topics + same_topic
    
    logger.info(f"üéØ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤: {len(candidates)} —Å—Ç–∞—Ç–µ–π")
    return candidates

# ====================== –ì–ï–ù–ï–†–ê–¢–û–† –ü–û–°–¢–û–í ======================
GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "llama3-70b-8192",
]

async def generate_summary(article: Article) -> Optional[str]:
    logger.info(f"üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {article.title[:55]}...")
    
    prompt = f"""–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –∫—Ä—É–ø–Ω–æ–≥–æ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –ò–ò –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.

–ù–û–í–û–°–¢–¨:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}
–¢–µ–∫—Å—Ç: {article.summary[:2200]}
–ò—Å—Ç–æ—á–Ω–∏–∫: {article.source}

–ó–ê–î–ê–ß–ê: –ù–∞–ø–∏—à–∏ –ø–æ—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–°–¢–†–£–ö–¢–£–†–ê (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):
1. üî• –ó–ê–ì–û–õ–û–í–û–ö ‚Äî —Ü–µ–ø–ª—è—é—â–∏–π, —Å —ç–º–æ–¥–∑–∏, –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å—É—Ç—å
2. –ß–¢–û –°–õ–£–ß–ò–õ–û–°–¨ ‚Äî 3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —Ñ–∞–∫—Ç–∞–º–∏ (–∫—Ç–æ, —á—Ç–æ, –∫–æ–≥–¥–∞, —Ü–∏—Ñ—Ä—ã)
3. –ü–û–ß–ï–ú–£ –í–ê–ñ–ù–û ‚Äî 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ –≤–ª–∏—è–Ω–∏–∏ –Ω–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—é/–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π  
4. –í–´–í–û–î ‚Äî –æ—Å—Ç—Ä—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏–ª–∏ –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –î–ª–∏–Ω–∞: 600-850 —Å–∏–º–≤–æ–ª–æ–≤ (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û)
- –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã, –Ω–∏–∫–∞–∫–æ–π –≤–æ–¥—ã
- –ö–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã

–ó–ê–ü–†–ï–©–ï–ù–û:
- –§—Ä–∞–∑—ã: "—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å", "–≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —á—Ç–æ", "–¥—Ä—É–∑—å—è"
- –®–∞–±–ª–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Ç–∏–ø–∞ "–ß—Ç–æ –¥—É–º–∞–µ—Ç–µ?"
- –ü—É—Å—Ç—ã–µ –æ–±–æ–±—â–µ–Ω–∏—è –±–µ–∑ —Ñ–∞–∫—Ç–æ–≤

–•–û–†–û–®–ò–ï –í–û–ü–†–û–°–´:
‚úì "Google —Å–Ω–æ–≤–∞ –¥–æ–≥–æ–Ω—è–µ—Ç ‚Äî –∏–ª–∏ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –æ–±–≥–æ–Ω–∏—Ç?"
‚úì "–°–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ –ø–æ—Ö–æ—Ä–æ–Ω–∏—Ç —ç—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ?"

–ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å ‚Äî –º—É—Å–æ—Ä/—Ä–µ–∫–ª–∞–º–∞/–Ω–µ –ø—Ä–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –æ—Ç–≤–µ—Ç—å: SKIP

–ü–û–°–¢:"""

    for attempt in range(3):
        try:
            await asyncio.sleep(0.5)
            
            resp = await asyncio.to_thread(
                groq_client.chat.completions.create,
                model=random.choice(GROQ_MODELS),
                temperature=0.7,
                max_tokens=1100,
                messages=[{"role": "user", "content": prompt}],
            )
            text = resp.choices[0].message.content.strip()

            if "SKIP" in text.upper()[:15]:
                logger.info("  ‚è≠Ô∏è LLM: SKIP")
                return None

            if len(text) < config.min_post_length:
                logger.warning(f"  ‚ö†Ô∏è –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤.), –ø–æ–≤—Ç–æ—Ä...")
                continue

            water = ["—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å", "–≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ", 
                    "–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º—Å—è", "–Ω–µ —Å–µ–∫—Ä–µ—Ç", "–æ—á–µ–≤–∏–¥–Ω–æ, —á—Ç–æ"]
            if any(w in text.lower() for w in water):
                logger.warning("  ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤–æ–¥–∞, –ø–æ–≤—Ç–æ—Ä...")
                continue

            topic = Topic.detect(f"{article.title} {article.summary}")
            hashtags = Topic.HASHTAGS.get(topic, Topic.HASHTAGS[Topic.GENERAL])
            
            cta = "\n\nüî• ‚Äî –æ–≥–æ–Ω—å  |  üóø ‚Äî –Ω—É —Ç–∞–∫–æ–µ  |  ‚ö° ‚Äî –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ"
            source_link = f'\n\nüîó <a href="{article.link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
            
            final = f"{text}{cta}\n\n{hashtags}{source_link}"

            if len(final) > config.caption_limit:
                excess = len(final) - config.caption_limit + 20
                text = text[:-excess]
                for p in ['. ', '! ', '? ']:
                    idx = text.rfind(p)
                    if idx > len(text) * 0.6:
                        text = text[:idx+1]
                        break
                final = f"{text}{cta}\n\n{hashtags}{source_link}"

            logger.info(f"  ‚úÖ –ì–æ—Ç–æ–≤–æ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ | –¢–µ–º–∞: {topic}")
            return final
            
        except Exception as e:
            logger.error(f"  ‚ùå Groq –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
            await asyncio.sleep(2)

    return None

# ====================== –ö–ê–†–¢–ò–ù–ö–ò (–°–¢–ò–õ–¨ –ü–û –¢–ï–ú–ï) ======================
async def generate_image(title: str, topic: str = Topic.GENERAL) -> Optional[str]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Å—Ç–∏–ª–µ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º —Ç–µ–º–µ —Å—Ç–∞—Ç—å–∏.
    –ë–µ–∑ –∫–∏–±–µ—Ä–ø–∞–Ω–∫–∞! –ß–∏—Å—Ç—ã–µ, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å—Ç–∏–ª–∏.
    """
    logger.info(f"  üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏ –¥–ª—è —Ç–µ–º—ã: {topic}")
    
    # –û—á–∏—â–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤
    clean_title = re.sub(r'[^\w\s]', '', title)[:50]
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∏–ª—å –¥–ª—è —Ç–µ–º—ã
    style = Topic.get_image_style(topic)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
    prompt = f"{style}, {clean_title}, high quality, 4k, sharp focus"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —á–µ—Ä–µ–∑ URL (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
    negative = "neon, cyberpunk, dark, dystopian, gritty, purple glow, matrix"
    
    url = (
        f"https://image.pollinations.ai/prompt/{quote(prompt)}"
        f"?width=1024&height=1024&nologo=true&seed={random.randint(1,99999)}"
    )
    
    for attempt in range(3):
        try:
            async with aiohttp.ClientSession() as sess:
                async with sess.get(url, timeout=aiohttp.ClientTimeout(total=45)) as resp:
                    if resp.status != 200:
                        logger.warning(f"  ‚ö†Ô∏è HTTP {resp.status}, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}")
                        continue
                    
                    data = await resp.read()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä (–º–∏–Ω–∏–º—É–º 10KB –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
                    if len(data) < 10000:
                        logger.warning(f"  ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª ({len(data)} bytes)")
                        continue
                    
                    fname = f"img_{random.randint(1000,9999)}.jpg"
                    with open(fname, "wb") as f:
                        f.write(data)
                    
                    logger.info(f"  ‚úÖ –ö–∞—Ä—Ç–∏–Ω–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {fname} ({len(data)//1024}KB)")
                    logger.info(f"  üéØ –°—Ç–∏–ª—å: {style[:50]}...")
                    return fname
                    
        except asyncio.TimeoutError:
            logger.warning(f"  ‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/3")
            await asyncio.sleep(2)
        except Exception as e:
            logger.warning(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({attempt+1}/3): {e}")
            await asyncio.sleep(2)
    
    logger.warning("  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É")
    return None

# ====================== –ü–£–ë–õ–ò–ö–ê–¶–ò–Ø ======================
async def post_article(article: Article, text: str, posted: PostedManager) -> bool:
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    topic = Topic.detect(f"{article.title} {article.summary}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º —Ç–µ–º—ã
    img = await generate_image(article.title, topic)
    
    try:
        if img and os.path.exists(img):
            await bot.send_photo(config.channel_id, FSInputFile(img), caption=text)
            os.remove(img)
        else:
            await bot.send_message(config.channel_id, text, disable_web_page_preview=False)
        
        posted.add(article.link, article.title, article.summary, topic)
        
        logger.info(f"‚úÖ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û [{topic.upper()}]: {article.title[:50]}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Telegram –æ—à–∏–±–∫–∞: {e}")
        if img and os.path.exists(img):
            try:
                os.remove(img)
            except:
                pass
        return False

# ====================== MAIN ======================
async def main():
    logger.info("=" * 50)
    logger.info("üöÄ –ó–ê–ü–£–°–ö AI-POSTER v2.1 (–±–µ–∑ –∫–∏–±–µ—Ä–ø–∞–Ω–∫–∞)")
    logger.info("=" * 50)
    
    posted = PostedManager(config.posted_file)
    posted.cleanup(config.retention_days)
    
    raw_articles = await load_all_feeds(posted)
    candidates = filter_articles(raw_articles, posted)
    
    if not candidates:
        logger.info("üì≠ –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return

    for article in candidates[:20]:
        if posted.is_duplicate(article.link, article.title, article.summary):
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (–¥—É–±–ª—å): {article.title[:40]}")
            continue
        
        if posted.is_too_similar_to_recent(article.title, article.summary):
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (–ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ): {article.title[:40]}")
            continue
        
        summary = await generate_summary(article)
        if not summary:
            continue
        
        if await post_article(article, summary, posted):
            logger.info("\nüèÅ –ì–æ—Ç–æ–≤–æ! –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à—ë–Ω.")
            break
        
        await asyncio.sleep(3)
    else:
        logger.info("üòî –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏")

    await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())






























































































































































































































































