import os
import json
import asyncio
import random
import re
import hashlib
import logging
import difflib
import tempfile
import shutil
import sqlite3
import time
from datetime import datetime, timezone, timedelta
from typing import List, Set, Optional, Tuple, Dict
from urllib.parse import urlparse, quote, parse_qs, urlencode
from dataclasses import dataclass, field
from collections import Counter
import math

import aiohttp
import feedparser
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq

# ====================== –õ–û–ì–ò ======================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("ai_poster.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ====================== CONFIG ======================
class Config:
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.telegram_token = os.getenv("TELEGRAM_BOT_TOKEN")
        self.channel_id = os.getenv("CHANNEL_ID")
        self.retention_days = int(os.getenv("RETENTION_DAYS", "60"))  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 60
        self.caption_limit = 1024
        self.db_file = "posted_articles.db"
        
        self.similarity_threshold = 0.72  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 0.60
        self.entity_overlap_threshold = 0.55
        self.min_post_length = 500
        self.min_summary_length = 200  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 100
        self.max_article_age_hours = 24  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 72 (3 –¥–Ω—è)
        
        # TF-IDF –ø–æ—Ä–æ–≥
        self.tfidf_similarity_threshold = 0.65
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        self.recent_posts_check = 5
        self.recent_similarity_threshold = 0.45
        self.min_entity_distance = 2
        self.diversity_window = 3  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        
        # Retry –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.groq_max_retries = 5
        self.groq_base_delay = 1.0
        self.groq_max_delay = 30.0
        
        # RSS –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.rss_timeout = 20
        self.rss_jitter = 3

        missing = []
        for var, name in [(self.groq_api_key, "GROQ_API_KEY"),
                          (self.telegram_token, "TELEGRAM_BOT_TOKEN"),
                          (self.channel_id, "CHANNEL_ID")]:
            if not var:
                missing.append(name)
        if missing:
            raise SystemExit(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(missing)}")

config = Config()

bot = Bot(token=config.telegram_token, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=config.groq_api_key)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

# ====================== RSS (–†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö) ======================
RSS_FEEDS = [
    ("https://techcrunch.com/category/artificial-intelligence/feed/", "TechCrunch"),
    ("https://venturebeat.com/category/ai/feed/", "VentureBeat"),
    ("https://www.technologyreview.com/topic/artificial-intelligence/feed", "MIT Tech Review"),
    ("https://www.theverge.com/rss/index.xml", "The Verge"),
    ("https://arstechnica.com/tag/artificial-intelligence/feed/", "Ars Technica"),
    ("https://www.wired.com/feed/tag/ai/latest/rss", "WIRED"),
    ("https://www.artificialintelligence-news.com/feed/", "AI News"),
    ("https://hai.stanford.edu/news/rss.xml", "Stanford HAI"),
    ("https://deepmind.google/blog/rss.xml", "DeepMind Blog"),
    ("https://openai.com/blog/rss/", "OpenAI Blog"),
    ("https://blog.google/technology/ai/rss/", "Google AI Blog"),
    ("https://www.marktechpost.com/feed/", "MarkTechPost"),
    ("https://syncedreview.com/feed/", "Synced AI"),
    ("https://news.ycombinator.com/rss", "Hacker News"),
    ("https://www.unite.ai/feed/", "Unite.AI"),
    ("https://analyticsindiamag.com/feed/", "AIM"),
]

# ====================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ======================
AI_KEYWORDS = [
    "ai", "artificial intelligence", "machine learning", "deep learning",
    "neural network", "llm", "large language model", "gpt", "chatgpt", "claude",
    "gemini", "grok", "llama", "mistral", "qwen", "deepseek", "midjourney",
    "dall-e", "stable diffusion", "sora", "groq", "openai", "anthropic",
    "deepmind", "hugging face", "nvidia", "agi", "transformer", "generative",
    "agents", "reasoning", "multimodal", "fine-tuning", "rlhf", "o3", "o1",
    "cursor", "copilot", "replit", "v0", "perplexity", "cohere", "01.ai"
]

EXCLUDE_KEYWORDS = [
    "stock price", "ipo", "earnings call", "quarterly results", "dividend",
    "market cap", "wall street", "ps5", "xbox", "nintendo", "game review",
    "netflix", "movie review", "box office", "trailer", "tesla stock",
    "bitcoin", "crypto", "blockchain", "nft", "ethereum", "election",
    "trump", "biden", "congress", "senate"
]

BAD_PHRASES = ["sponsored", "partner content", "advertisement", "black friday", "deal alert"]

# ====================== –†–ê–°–®–ò–†–ï–ù–ù–´–ï –°–£–©–ù–û–°–¢–ò ======================
KEY_ENTITIES = [
    # –ö–æ–º–ø–∞–Ω–∏–∏
    "openai", "google", "meta", "microsoft", "anthropic", "nvidia", "apple",
    "amazon", "deepmind", "hugging face", "stability ai", "midjourney",
    "mistral", "cohere", "perplexity", "runway", "pika", "character ai",
    "inflection", "xai", "baidu", "alibaba", "tencent", "bytedance",
    "01.ai", "moonshot ai", "zhipu ai", "ai21 labs", "adept", "adept ai",
    "elevenlabs", "heygen", "synthesia", "jasper", "copy.ai", "replika",
    
    # –ú–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
    "gpt-4", "gpt-5", "gpt-4o", "gpt-4.5", "chatgpt", "claude", "claude 3",
    "claude 3.5", "claude 3.5 sonnet", "claude 3 opus", "gemini", "gemini 2",
    "gemini 2.0", "gemini 1.5", "llama", "llama 3", "llama 3.3", "llama 3.2",
    "llama 3.1", "mistral", "mixtral", "mixtral 8x7b", "mixtral 8x22b",
    "copilot", "github copilot", "microsoft copilot", "dall-e", "dall-e 3",
    "dall-e 2", "sora", "stable diffusion", "stable diffusion 3", "flux",
    "midjourney v6", "midjourney v7", "runway gen", "runway gen-3", "firefly",
    "adobe firefly", "imagen", "imagen 3", "grok", "grok 2", "grok 3",
    "deepseek", "deepseek-v3", "deepseek-v2", "deepseek-r1", "qwen",
    "qwen 2", "qwen 2.5", "yi", "yi-34b", "command r", "command r+",
    "o3", "o3 mini", "o1", "o1 mini", "o1 preview", "gpt-4o mini",
    
    # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
    "cursor", "cursor ai", "replit", "replit agent", "v0", "v0.dev",
    "vercel v0", "bolt", "bolt.new", "lovable", "temporal", "langchain",
    "llamaindex", "crewai", "autogen", "semantic kernel", "haystack",
    "weaviate", "pinecone", "chroma", "qdrant", "milvus", "modal",
    "replicate", "together ai", "fireworks ai", "baseten", "banana dev",
    
    # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
    "linux foundation", "agentic", "ai agent", "ai agents", "agi", "asi",
    "artificial general intelligence", "regulation", "safety", "alignment",
    "ai safety", "ai alignment", "open source", "open-source", "open weights",
    "robotics", "humanoid", "boston dynamics", "figure", "figure ai",
    "optimus", "tesla bot", "unitree", "agility robotics", "digit",
    "apptronik", "1x technologies", "sanctuary ai", "covariant",
    
    # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    "transformer", "transformers", "diffusion", "diffusion model",
    "multimodal", "multimodal ai", "reasoning", "chain of thought",
    "cot", "fine-tuning", "rlhf", "inference", "training", "benchmark",
    "rag", "retrieval augmented generation", "vector database", "embedding",
    "token", "context window", "prompt engineering", "jailbreak",
    "hallucination", "ai hallucination", "synthetic data",
]

# ====================== DATACLASS ======================
@dataclass
class Article:
    title: str
    summary: str
    link: str
    source: str
    published: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

# ====================== TOPIC & HASHTAGS ======================
class Topic:
    LLM = "llm"
    IMAGE_GEN = "image_gen"
    ROBOTICS = "robotics"
    HARDWARE = "hardware"
    REGULATION = "regulation"
    RESEARCH = "research"
    AGENTS = "agents"
    CODING = "coding"
    GENERAL = "general"
    
    HASHTAGS = {
        LLM: "#ChatGPT #LLM #OpenAI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        IMAGE_GEN: "#Midjourney #DALLE #StableDiffusion #–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
        ROBOTICS: "#—Ä–æ–±–æ—Ç—ã #Humanoid #—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
        HARDWARE: "#NVIDIA #GPU #—á–∏–ø—ã #–∂–µ–ª–µ–∑–æ",
        REGULATION: "#—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ #–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å #—ç—Ç–∏–∫–∞",
        RESEARCH: "#–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è #–Ω–∞—É–∫–∞ #DeepMind",
        AGENTS: "#AIAgents #–ê–≥–µ–Ω—Ç—ã #AutonomousAI",
        CODING: "#Cursor #GitHubCopilot #AI–∫–æ–¥–∏–Ω–≥",
        GENERAL: "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #–ò–ò"
    }
    
    IMAGE_STYLES = {
        LLM: [
            "clean minimalist illustration, chat interface, soft blue and white gradient, modern UI design, professional",
            "friendly robot assistant illustration, soft colors, white background, cute character design",
            "abstract conversation bubbles, flowing shapes, light blue tones, editorial style illustration",
            "modern flat design, speech bubbles and text symbols, pastel colors, tech magazine cover",
        ],
        IMAGE_GEN: [
            "artistic watercolor illustration, creative palette, splashes of color, gallery aesthetic",
            "paintbrush and canvas artistic concept, warm colors, creative studio atmosphere",
            "abstract art composition, flowing colors, creative expression, museum quality",
            "digital art creation concept, colorful gradients, artistic tools, inspiring atmosphere",
        ],
        ROBOTICS: [
            "technical blueprint illustration, soft gray background, precise mechanical drawings, engineering style",
            "friendly humanoid robot, soft studio lighting, white background, product photography style",
            "isometric robot illustration, clean lines, soft shadows, modern industrial design",
            "robotic arm in laboratory setting, clean environment, professional photography style",
        ],
        HARDWARE: [
            "product photography of tech hardware, studio lighting, reflective surfaces, premium feel",
            "clean circuit board illustration, green and gold tones, technical precision, macro style",
            "isometric computer chip illustration, metallic textures, soft gradients, professional",
            "modern data center visualization, clean rows of servers, soft blue lighting, corporate",
        ],
        REGULATION: [
            "corporate illustration, scales of justice with tech elements, muted blue tones, professional",
            "formal document and gavel illustration, clean design, government style, serious tone",
            "handshake between human and robot, diplomatic setting, soft neutral colors, editorial",
            "policy document with AI symbols, clean infographic style, trustworthy blue palette",
        ],
        RESEARCH: [
            "scientific laboratory illustration, clean white environment, research equipment, academic",
            "brain and neural connections visualization, soft purple and blue, medical illustration style",
            "scientist working with data, modern lab setting, clean aesthetic, educational",
            "abstract knowledge graph, interconnected nodes, soft colors, scientific visualization",
        ],
        AGENTS: [
            "autonomous agent illustration, interconnected nodes, soft purple and blue, futuristic but clean",
            "ai agent workflow diagram, clean design, soft gradients, professional infographic",
            "multiple ai agents collaborating, isometric illustration, soft colors, modern tech",
            "autonomous system visualization, flowing data streams, soft blue tones, editorial",
        ],
        CODING: [
            "clean code editor interface, syntax highlighting, dark theme with soft colors, developer aesthetic",
            "ai coding assistant illustration, code snippets floating, soft blue and purple, modern",
            "programmer workspace with ai, clean desk setup, soft lighting, professional",
            "abstract code visualization, flowing lines of code, soft gradients, tech magazine",
        ],
        GENERAL: [
            "modern flat illustration, geometric shapes, pastel gradient colors, editorial magazine style",
            "clean tech illustration, simple icons, white background, professional presentation",
            "isometric technology concept, soft shadows, modern design, business friendly",
            "minimalist abstract design, flowing lines, soft blue and white, corporate clean",
        ],
    }

    @staticmethod
    def detect(text: str) -> str:
        t = text.lower()
        if any(x in t for x in ["cursor", "copilot", "replit", "v0", "bolt.new", "coding", "programming", "developer"]):
            return Topic.CODING
        if any(x in t for x in ["agent", "autonomous", "crewai", "autogen", "langchain agent"]):
            return Topic.AGENTS
        if any(x in t for x in ["gpt", "chatgpt", "claude", "gemini", "llama", "grok", "llm", "o3", "o1", "deepseek"]):
            return Topic.LLM
        if any(x in t for x in ["midjourney", "dall-e", "stable diffusion", "flux", "sora", "imagen"]):
            return Topic.IMAGE_GEN
        if any(x in t for x in ["robot", "humanoid", "boston dynamics", "optimus", "figure", "unitree", "agility"]):
            return Topic.ROBOTICS
        if any(x in t for x in ["nvidia", "h100", "h200", "blackwell", "gpu", "cuda", "chip", "hardware"]):
            return Topic.HARDWARE
        if any(x in t for x in ["regulation", "safety", "alignment", "ethics", "policy", "governance"]):
            return Topic.REGULATION
        if any(x in t for x in ["research", "paper", "study", "breakthrough", "discovery", "arxiv"]):
            return Topic.RESEARCH
        return Topic.GENERAL
    
    @staticmethod
    def get_image_style(topic: str) -> str:
        styles = Topic.IMAGE_STYLES.get(topic, Topic.IMAGE_STYLES[Topic.GENERAL])
        return random.choice(styles)

# ====================== URL –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø ======================
UTM_PARAMS = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
              'fbclid', 'gclid', 'twclid', 'li_fat_id', 'mc_cid', 'mc_eid',
              'utm_id', 'utm_source_platform', 'utm_creative_format', 'utm_marketing_tactic']

def normalize_url(url: str) -> str:
    """
    –£—Å–∏–ª–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL:
    - –£–¥–∞–ª–µ–Ω–∏–µ UTM-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    - –£–¥–∞–ª–µ–Ω–∏–µ —è–∫–æ—Ä–µ–π
    - –£–¥–∞–ª–µ–Ω–∏–µ trailing slashes
    - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ lowercase
    - –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ scheme://host/path
    """
    if not url:
        return ""
    try:
        url = url.strip().lower()
        parsed = urlparse(url)
        
        # –£–¥–∞–ª—è–µ–º UTM –∏ –¥—Ä—É–≥–∏–µ tracking –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        query_params = parse_qs(parsed.query)
        filtered_params = {k: v for k, v in query_params.items() 
                          if k.lower() not in UTM_PARAMS}
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π URL
        domain = parsed.netloc.replace("www.", "")
        path = parsed.path.rstrip("/")
        
        if filtered_params:
            new_query = urlencode(filtered_params, doseq=True)
            return f"{parsed.scheme}://{domain}{path}?{new_query}"
        else:
            return f"{parsed.scheme}://{domain}{path}"
    except Exception:
        # Fallback: –±–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        url = url.lower().split('#')[0].split('?')[0].rstrip('/')
        return url

def get_domain(url: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¥–æ–º–µ–Ω –∏–∑ URL"""
    try:
        parsed = urlparse(url)
        return parsed.netloc.lower().replace("www.", "")
    except:
        return ""

def get_title_signature(title: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∏–≥–Ω–∞—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–ø–µ—Ä–≤–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞ —Å–ª–æ–≤)"""
    words = re.findall(r'\w+', title.lower())
    half = max(1, len(words) // 2)
    return ' '.join(words[:half])

# ====================== –•–ï–®–ò–†–û–í–ê–ù–ò–ï ======================
def get_content_hash(text: str) -> str:
    """MD5 —Ö–µ—à –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    if not text:
        return ""
    normalized = re.sub(r'\s+', ' ', text.strip().lower())
    return hashlib.md5(normalized[:2000].encode()).hexdigest()

def get_summary_hash(summary: str) -> str:
    """MD5 —Ö–µ—à –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ summary"""
    if not summary:
        return ""
    # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    normalized = re.sub(r'[^\w\s]', '', summary.lower())
    normalized = re.sub(r'\s+', ' ', normalized.strip())
    return hashlib.md5(normalized.encode()).hexdigest()

# ====================== –ü–û–•–û–ñ–ï–°–¢–¨ –¢–ï–ö–°–¢–ê ======================
def calculate_similarity(text1: str, text2: str) -> float:
    """–°—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Å—Ç—Ä–æ–∫ (0.0 - 1.0) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SequenceMatcher"""
    if not text1 or not text2:
        return 0.0
    return difflib.SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def ngram_similarity(text1: str, text2: str, n: int = 3) -> float:
    """–°—Ö–æ–∂–µ—Å—Ç—å –ø–æ n-–≥—Ä–∞–º–º–∞–º"""
    if not text1 or not text2:
        return 0.0
    
    def get_ngrams(text, n):
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        words = text.split()
        return set([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])
    
    ngrams1 = get_ngrams(text1, n)
    ngrams2 = get_ngrams(text2, n)
    
    if not ngrams1 or not ngrams2:
        return 0.0
    
    intersection = ngrams1 & ngrams2
    union = ngrams1 | ngrams2
    
    return len(intersection) / len(union) if union else 0.0

# ====================== TF-IDF SIMILARITY ======================
class TFIDFCalculator:
    """–ü—Ä–æ—Å—Ç–æ–π TF-IDF –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤"""
    
    @staticmethod
    def tokenize(text: str) -> List[str]:
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
                      'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
                      'would', 'could', 'should', 'may', 'might', 'must', 'shall',
                      'can', 'need', 'dare', 'ought', 'used', 'to', 'of', 'in',
                      'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into',
                      'through', 'during', 'before', 'after', 'above', 'below',
                      'between', 'under', 'and', 'but', 'or', 'yet', 'so', 'if',
                      'because', 'although', 'though', 'while', 'where', 'when',
                      'that', 'which', 'who', 'whom', 'whose', 'what', 'this',
                      'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
                      'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her',
                      'its', 'our', 'their', 'mine', 'yours', 'hers', 'ours', 'theirs'}
        return [w for w in words if len(w) > 2 and w not in stop_words]
    
    @staticmethod
    def compute_tf(tokens: List[str]) -> Dict[str, float]:
        token_counts = Counter(tokens)
        total = len(tokens)
        return {token: count / total for token, count in token_counts.items()} if total else {}
    
    @staticmethod
    def compute_idf(documents: List[List[str]]) -> Dict[str, float]:
        idf = {}
        total_docs = len(documents)
        all_tokens = set()
        for doc in documents:
            all_tokens.update(doc)
        
        for token in all_tokens:
            doc_count = sum(1 for doc in documents if token in doc)
            idf[token] = math.log(total_docs / (doc_count + 1)) + 1
        
        return idf
    
    @classmethod
    def cosine_similarity(cls, text1: str, text2: str, context_texts: List[str] = None) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏"""
        tokens1 = cls.tokenize(text1)
        tokens2 = cls.tokenize(text2)
        
        if not tokens1 or not tokens2:
            return 0.0
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è IDF
        documents = [tokens1, tokens2]
        if context_texts:
            documents.extend([cls.tokenize(t) for t in context_texts])
        
        idf = cls.compute_idf(documents)
        
        tf1 = cls.compute_tf(tokens1)
        tf2 = cls.compute_tf(tokens2)
        
        # TF-IDF –≤–µ–∫—Ç–æ—Ä—ã
        all_terms = set(tf1.keys()) | set(tf2.keys())
        vec1 = [tf1.get(term, 0) * idf.get(term, 1) for term in all_terms]
        vec2 = [tf2.get(term, 0) * idf.get(term, 1) for term in all_terms]
        
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(a * a for a in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)

# ====================== –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–£–©–ù–û–°–¢–ï–ô ======================
def extract_key_entities(text: str) -> Set[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å fuzzy matching"""
    text_lower = text.lower()
    found = set()
    
    for entity in KEY_ENTITIES:
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if entity in text_lower:
            normalized = entity.replace("-", " ").replace("_", " ")
            found.add(normalized)
            continue
        
        # Fuzzy matching –¥–ª—è –≤–∞—Ä–∏–∞—Ü–∏–π
        entity_words = entity.split()
        if len(entity_words) == 1:
            # –î–ª—è –æ–¥–Ω–æ—Å–ª–æ–≤–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –ø—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤
            pattern = r'\b' + re.escape(entity) + r'\b'
            if re.search(pattern, text_lower):
                found.add(entity)
    
    return found

def fuzzy_entity_match(entities1: Set[str], entities2: Set[str]) -> float:
    """Fuzzy matching –º–µ–∂–¥—É –Ω–∞–±–æ—Ä–∞–º–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    if not entities1 or not entities2:
        return 0.0
    
    matches = 0
    for e1 in entities1:
        for e2 in entities2:
            # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if e1 == e2:
                matches += 1
                break
            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–æ–¥–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥—Ä—É–≥–æ–µ)
            if e1 in e2 or e2 in e1:
                matches += 0.8
                break
            # –í—ã—Å–æ–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            if calculate_similarity(e1, e2) > 0.85:
                matches += 0.7
                break
    
    return matches / max(len(entities1), len(entities2))

# ====================== SQLITE MANAGER ======================
class PostedManager:
    """SQLite-based –º–µ–Ω–µ–¥–∂–µ—Ä —Å –∞—Ç–æ–º–∞—Ä–Ω—ã–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏ –∏ advisory locks"""
    
    def __init__(self, db_file: str = "posted_articles.db"):
        self.db_file = db_file
        self._local = threading.local()
        self._init_db()
        self._acquire_lock()
    
    def _get_conn(self) -> sqlite3.Connection:
        """–ü–æ–ª—É—á–∞–µ—Ç thread-local —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"""
        if not hasattr(self._local, 'conn') or self._local.conn is None:
            self._local.conn = sqlite3.connect(self.db_file, check_same_thread=False)
            self._local.conn.row_factory = sqlite3.Row
        return self._local.conn
    
    def _init_db(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ posted_articles
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS posted_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT NOT NULL UNIQUE,
                norm_url TEXT NOT NULL,
                domain TEXT NOT NULL,
                title TEXT NOT NULL,
                title_signature TEXT NOT NULL,
                summary TEXT,
                content_hash TEXT,
                summary_hash TEXT NOT NULL,
                entities TEXT,  -- JSON —Å–ø–∏—Å–æ–∫
                topic TEXT DEFAULT 'general',
                source TEXT,
                published_date TEXT,
                posted_date TEXT DEFAULT CURRENT_TIMESTAMP,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –æ—Ç–∫–ª–æ–Ω—ë–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π (–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS rejected_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT NOT NULL,
                norm_url TEXT,
                title TEXT NOT NULL,
                summary TEXT,
                source TEXT,
                rejection_reason TEXT NOT NULL,
                duplicate_of TEXT,  -- URL –¥—É–±–ª–∏–∫–∞—Ç–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
                similarity_score REAL,
                checked_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_norm_url ON posted_articles(norm_url)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_hash ON posted_articles(content_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_summary_hash ON posted_articles(summary_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_domain ON posted_articles(domain)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_title_signature ON posted_articles(title_signature)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_posted_date ON posted_articles(posted_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic ON posted_articles(topic)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_rejected_url ON rejected_articles(url)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_rejected_reason ON rejected_articles(rejection_reason)')
        
        conn.commit()
        conn.close()
        logger.info("üìö –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def _acquire_lock(self):
        """SQLite advisory lock –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
        self.lock_conn = sqlite3.connect(self.db_file)
        try:
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å advisory lock
            self.lock_conn.execute("BEGIN IMMEDIATE")
            logger.info("üîí Advisory lock –ø–æ–ª—É—á–µ–Ω")
        except sqlite3.OperationalError:
            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å lock, –¥—Ä—É–≥–æ–π –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–±–æ—Ç–∞–µ—Ç")
            raise SystemExit(0)
    
    def _release_lock(self):
        """–û—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç advisory lock"""
        try:
            if hasattr(self, 'lock_conn') and self.lock_conn:
                self.lock_conn.close()
                logger.info("üîì Advisory lock –æ—Å–≤–æ–±–æ–∂–¥—ë–Ω")
        except:
            pass
    
    def is_duplicate(self, url: str, title: str, summary: str = "") -> Tuple[bool, str]:
        """
        –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç:
        1. URL (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π)
        2. –•–µ—à summary
        3. –•–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        4. –ü–æ—Ö–æ–∂–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ (n-gram + SequenceMatcher)
        5. –î–æ–º–µ–Ω + —Å–∏–≥–Ω–∞—Ç—É—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–ª–æ–≤–∏—Ç mirror-—Å–∞–π—Ç—ã)
        6. –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        
        Returns: (is_duplicate, reason)
        """
        conn = self._get_conn()
        cursor = conn.cursor()
        
        norm_url = normalize_url(url)
        domain = get_domain(url)
        title_sig = get_title_signature(title)
        summary_hash = get_summary_hash(summary)
        content_hash = get_content_hash(summary)
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É URL
        cursor.execute('SELECT title FROM posted_articles WHERE norm_url = ?', (norm_url,))
        if cursor.fetchone():
            return True, f"URL_DUPLICATE: {norm_url[:60]}"
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ö–µ—à—É summary
        cursor.execute('SELECT title FROM posted_articles WHERE summary_hash = ?', (summary_hash,))
        if cursor.fetchone():
            return True, f"SUMMARY_HASH_DUPLICATE"
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ö–µ—à—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if content_hash:
            cursor.execute('SELECT title FROM posted_articles WHERE content_hash = ?', (content_hash,))
            if cursor.fetchone():
                return True, f"CONTENT_HASH_DUPLICATE"
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–æ–º–µ–Ω—É + —Å–∏–≥–Ω–∞—Ç—É—Ä–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (mirror-—Å–∞–π—Ç—ã)
        cursor.execute('SELECT title FROM posted_articles WHERE domain = ? AND title_signature = ?',
                      (domain, title_sig))
        if cursor.fetchone():
            return True, f"DOMAIN_TITLE_SIGNATURE: {domain}"
        
        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        cursor.execute('SELECT title FROM posted_articles WHERE posted_date > datetime("now", "-7 days")')
        recent_titles = [row[0] for row in cursor.fetchall()]
        
        for existing_title in recent_titles:
            # SequenceMatcher
            sim = calculate_similarity(title, existing_title)
            if sim > config.similarity_threshold:
                return True, f"TITLE_SIMILARITY: {sim:.2f}"
            
            # N-gram similarity
            ngram_sim = ngram_similarity(title, existing_title)
            if ngram_sim > config.similarity_threshold:
                return True, f"TITLE_NGRAM_SIMILARITY: {ngram_sim:.2f}"
        
        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        full_text = f"{title} {summary}".strip()
        new_entities = extract_key_entities(full_text)
        
        if len(new_entities) >= 2:
            cursor.execute('SELECT title, entities FROM posted_articles WHERE posted_date > datetime("now", "-14 days")')
            for row in cursor.fetchall():
                existing_title, saved_entities_json = row
                if saved_entities_json:
                    existing_entities = set(json.loads(saved_entities_json))
                else:
                    existing_entities = extract_key_entities(existing_title)
                
                if len(existing_entities) < 2:
                    continue
                
                # Fuzzy matching —Å—É—â–Ω–æ—Å—Ç–µ–π
                entity_sim = fuzzy_entity_match(new_entities, existing_entities)
                if entity_sim >= config.entity_overlap_threshold:
                    return True, f"ENTITY_OVERLAP: {entity_sim:.2f}"
                
                # –ü—Ä—è–º–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
                common = new_entities & existing_entities
                min_size = min(len(new_entities), len(existing_entities))
                overlap_ratio = len(common) / min_size if min_size > 0 else 0
                
                if len(common) >= 2 and overlap_ratio >= config.entity_overlap_threshold:
                    return True, f"ENTITY_COMMON: {len(common)} entities"
        
        return False, ""
    
    def is_too_similar_to_recent(self, title: str, summary: str) -> Tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –ø–æ—Ö–æ–∂–∞ —Å—Ç–∞—Ç—å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç TF-IDF cosine similarity
        """
        conn = self._get_conn()
        cursor = conn.cursor()
        
        cursor.execute('SELECT title, summary, topic, entities FROM posted_articles '
                      'ORDER BY posted_date DESC LIMIT ?', (config.recent_posts_check,))
        recent_posts = cursor.fetchall()
        
        if len(recent_posts) < 2:
            return False, ""
        
        full_text = f"{title} {summary}".strip()
        new_entities = extract_key_entities(full_text)
        detected_topic = Topic.detect(full_text)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è TF-IDF
        context_texts = [row[1] for row in recent_posts if row[1]]
        
        for post in recent_posts:
            post_title, post_summary, post_topic, saved_entities_json = post
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            if post_title:
                sim = calculate_similarity(title, post_title)
                if sim > config.recent_similarity_threshold:
                    return True, f"RECENT_TITLE_SIM: {sim:.2f}"
            
            # TF-IDF similarity –¥–ª—è summary
            if post_summary and summary:
                tfidf_sim = TFIDFCalculator.cosine_similarity(summary, post_summary, context_texts)
                if tfidf_sim > config.tfidf_similarity_threshold:
                    return True, f"TFIDF_SIMILARITY: {tfidf_sim:.2f}"
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–º—ã –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π
            post_entities = set(json.loads(saved_entities_json)) if saved_entities_json else set()
            
            if detected_topic == post_topic and post_entities:
                common = new_entities & post_entities
                if len(common) >= config.min_entity_distance:
                    return True, f"RECENT_TOPIC_ENTITIES: {detected_topic}"
        
        return False, ""
    
    def check_diversity_requirement(self, proposed_topic: str) -> Tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è:
        –µ—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –ø–æ—Å—Ç–∞ –ø—Ä–æ –æ–¥–Ω—É —Ç–µ–º—É, —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥—Ä—É–≥–æ–π
        """
        conn = self._get_conn()
        cursor = conn.cursor()
        
        cursor.execute('SELECT topic FROM posted_articles '
                      'ORDER BY posted_date DESC LIMIT ?', (config.diversity_window,))
        recent_topics = [row[0] for row in cursor.fetchall()]
        
        if len(recent_topics) < config.diversity_window:
            return True, ""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –Ω–∞ –æ–¥–Ω—É —Ç–µ–º—É
        topic_counts = Counter(recent_topics)
        dominant_topic, count = topic_counts.most_common(1)[0]
        
        if count >= config.diversity_window and proposed_topic == dominant_topic:
            return False, f"DIVERSITY_REQUIRED: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {config.diversity_window} –ø–æ—Å—Ç–æ–≤ –ø—Ä–æ {dominant_topic}"
        
        return True, ""
    
    def llm_duplicate_check(self, article: Article, recent_posts: List[dict]) -> Tuple[bool, str]:
        """
        LLM-–ø—Ä–æ–≤–µ—Ä–∫–∞: –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç–∞—Ç—å—é + –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –ø–æ—Å—Ç–∞ –≤ LLM
        —Å –≤–æ–ø—Ä–æ—Å–æ–º "–≠—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç? YES/NO"
        """
        if not recent_posts:
            return False, ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = "–ù–ï–î–ê–í–ù–ò–ï –ü–û–°–¢–´:\n\n"
        for i, post in enumerate(recent_posts[:3], 1):
            context += f"{i}. {post.get('title', '')}\n"
            context += f"   –°—É—â–Ω–æ—Å—Ç–∏: {', '.join(post.get('entities', []))}\n\n"
        
        prompt = f"""–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –ò–ò. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö.

{context}

–ù–û–í–ê–Ø –°–¢–ê–¢–¨–Ø:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}
–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {article.summary[:500]}
–ò—Å—Ç–æ—á–Ω–∏–∫: {article.source}

–ü–†–ê–í–ò–õ–ê:
- –î—É–±–ª–∏–∫–∞—Ç = —Å—Ç–∞—Ç—å—è –ø—Ä–æ –¢–û–¢ –ñ–ï –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å–æ–±—ã—Ç–∏–µ/–∞–Ω–æ–Ω—Å/—Ä–µ–ª–∏–∑
- –†–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø—Ä–æ –æ–¥–Ω–æ —Å–æ–±—ã—Ç–∏–µ = –¥—É–±–ª–∏–∫–∞—Ç
- –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, –Ω–æ —Ç–∞ –∂–µ —Å—É—Ç—å = –¥—É–±–ª–∏–∫–∞—Ç
- –î—Ä—É–≥–æ–π –∞—Å–ø–µ–∫—Ç —Ç–æ–π –∂–µ —Ç–µ–º—ã = –ù–ï –¥—É–±–ª–∏–∫–∞—Ç
- –†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏/–ø—Ä–æ–¥—É–∫—Ç—ã –æ–¥–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ = –ù–ï –¥—É–±–ª–∏–∫–∞—Ç

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û: YES (–µ—Å–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç) –∏–ª–∏ NO (–µ—Å–ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å)

–û—Ç–≤–µ—Ç:"""
        
        try:
            resp = groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                temperature=0.1,
                max_tokens=10,
                messages=[{"role": "user", "content": prompt}],
            )
            answer = resp.choices[0].message.content.strip().upper()
            
            if "YES" in answer:
                return True, "LLM_DUPLICATE_CHECK"
            return False, ""
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LLM duplicate check failed: {e}")
            return False, ""  # –ü—Ä–∏ –æ—à–∏–±–∫–µ LLM –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    
    def log_rejected(self, article: Article, reason: str, duplicate_of: str = None, 
                     similarity_score: float = None):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –æ—Ç–∫–ª–æ–Ω—ë–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É"""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT INTO rejected_articles 
                (url, norm_url, title, summary, source, rejection_reason, duplicate_of, similarity_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article.link,
                normalize_url(article.link),
                article.title[:200],
                article.summary[:1000] if article.summary else None,
                article.source,
                reason,
                duplicate_of,
                similarity_score
            ))
            conn.commit()
            logger.info(f"üìù –ó–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–æ rejected: {reason[:50]}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è rejected: {e}")
    
    def add(self, article: Article, topic: str = Topic.GENERAL):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—å—é –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        norm_url = normalize_url(article.link)
        domain = get_domain(article.link)
        title_sig = get_title_signature(article.title)
        summary_hash = get_summary_hash(article.summary)
        content_hash = get_content_hash(article.summary)
        full_text = f"{article.title} {article.summary}".strip()
        entities = list(extract_key_entities(full_text))
        
        try:
            cursor.execute('''
                INSERT INTO posted_articles 
                (url, norm_url, domain, title, title_signature, summary, content_hash, summary_hash, 
                 entities, topic, source, published_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article.link,
                norm_url,
                domain,
                article.title[:200],
                title_sig,
                article.summary[:2000] if article.summary else None,
                content_hash,
                summary_hash,
                json.dumps(entities),
                topic,
                article.source,
                article.published.isoformat() if article.published else None
            ))
            conn.commit()
            logger.info(f"üíæ [{topic.upper()}] {article.title[:45]}... | –°—É—â–Ω–æ—Å—Ç–∏: {entities if entities else '–Ω–µ—Ç'}")
        except sqlite3.IntegrityError:
            logger.warning(f"‚ö†Ô∏è IntegrityError (–¥—É–±–ª–∏–∫–∞—Ç URL): {article.title[:40]}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
    
    def get_recent_posts(self, limit: int = 5) -> List[dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤"""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT title, summary, topic, entities, url 
            FROM posted_articles 
            ORDER BY posted_date DESC 
            LIMIT ?
        ''', (limit,))
        
        posts = []
        for row in cursor.fetchall():
            posts.append({
                'title': row[0],
                'summary': row[1],
                'topic': row[2],
                'entities': json.loads(row[3]) if row[3] else [],
                'url': row[4]
            })
        return posts
    
    def get_recent_topics_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–º–∞–º –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤"""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT topic, COUNT(*) as count 
            FROM posted_articles 
            WHERE posted_date > datetime("now", "-7 days")
            GROUP BY topic
        ''')
        
        return {row[0]: row[1] for row in cursor.fetchall()}
    
    def cleanup(self, days: int = 60):
        """–£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π"""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        cursor.execute('''
            DELETE FROM posted_articles 
            WHERE posted_date < datetime("now", "-? days")
        ''', (days,))
        
        deleted = cursor.rowcount
        conn.commit()
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ rejected —Ç–æ–∂–µ
        cursor.execute('''
            DELETE FROM rejected_articles 
            WHERE checked_at < datetime("now", "-30 days")
        ''')
        rejected_deleted = cursor.rowcount
        conn.commit()
        
        if deleted > 0 or rejected_deleted > 0:
            logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–æ {deleted} posted, {rejected_deleted} rejected")
        
        # VACUUM –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if deleted > 100:
            cursor.execute('VACUUM')
            conn.commit()
    
    def get_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM posted_articles')
        total_posted = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM rejected_articles')
        total_rejected = cursor.fetchone()[0]
        
        cursor.execute('''
            SELECT rejection_reason, COUNT(*) as count 
            FROM rejected_articles 
            GROUP BY rejection_reason
            ORDER BY count DESC
        ''')
        rejection_reasons = {row[0]: row[1] for row in cursor.fetchall()}
        
        return {
            'total_posted': total_posted,
            'total_rejected': total_rejected,
            'rejection_reasons': rejection_reasons
        }
    
    def __del__(self):
        self._release_lock()

import threading  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç –¥–ª—è thread-local storage

# ====================== HELPERS ======================
def clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def ai_relevance(text: str) -> float:
    lower = text.lower()
    matches = sum(1 for kw in AI_KEYWORDS if kw in lower)
    return min(matches / 3.0, 1.0)

# ====================== RSS LOADER ======================
async def fetch_feed(session: aiohttp.ClientSession, url: str, source: str, 
                     posted: PostedManager) -> List[Article]:
    # –î–æ–±–∞–≤–ª—è–µ–º jitter –∫ timeout
    timeout = config.rss_timeout + random.uniform(0, config.rss_jitter)
    
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=timeout)) as resp:
            if resp.status != 200:
                logger.warning(f"{source}: HTTP {resp.status}")
                return []
            text = await resp.text()
    except asyncio.TimeoutError:
        logger.warning(f"{source}: Timeout after {timeout:.1f}s")
        return []
    except Exception as e:
        logger.warning(f"{source}: {e}")
        return []

    try:
        feed = feedparser.parse(text)
    except Exception as e:
        logger.warning(f"{source}: Parse error {e}")
        return []

    articles = []
    cutoff_time = datetime.now(timezone.utc) - timedelta(hours=config.max_article_age_hours)
    
    for entry in feed.entries[:25]:
        link = entry.get("link", "").strip()
        title = clean_text(entry.get("title") or "")
        summary = clean_text(entry.get("summary") or entry.get("description") or "")[:1500]

        if not link or len(title) < 15:
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã summary
        if len(summary) < config.min_summary_length:
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (–∫–æ—Ä–æ—Ç–∫–∏–π summary {len(summary)}): {title[:40]}")
            continue
        
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        published = datetime.now(timezone.utc)
        date_found = False
        for df in ["published", "updated", "created", "pubDate"]:
            ds = entry.get(df)
            if ds:
                try:
                    parsed = feedparser._parse_date(ds)
                    if parsed:
                        published = datetime(*parsed[:6], tzinfo=timezone.utc)
                        date_found = True
                        break
                except:
                    pass
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
        if date_found and published < cutoff_time:
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (—É—Å—Ç–∞—Ä–µ–ª–æ {published}): {title[:40]}")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç
        is_dup, reason = posted.is_duplicate(link, title, summary)
        if is_dup:
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (–¥—É–±–ª–∏–∫–∞—Ç: {reason}): {title[:40]}")
            continue

        articles.append(Article(
            title=title,
            summary=summary,
            link=link,
            source=source,
            published=published
        ))

    return articles

async def load_all_feeds(posted: PostedManager) -> List[Article]:
    logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ RSS...")
    
    conn = aiohttp.TCPConnector(limit=30)
    async with aiohttp.ClientSession(headers=HEADERS, connector=conn) as session:
        tasks = [fetch_feed(session, url, name, posted) for url, name in RSS_FEEDS]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    all_articles = []
    for i, res in enumerate(results):
        source_name = RSS_FEEDS[i][1]
        if isinstance(res, list) and res:
            all_articles.extend(res)
            logger.info(f"  ‚úì {source_name}: {len(res)} –Ω–æ–≤—ã—Ö")
        elif isinstance(res, Exception):
            logger.error(f"  ‚úó {source_name}: {res}")

    logger.info(f"üìä –í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_articles)}")
    return all_articles

# ====================== FILTER ======================
def filter_articles(articles: List[Article], posted: PostedManager) -> List[Article]:
    candidates = []
    
    recent_stats = posted.get_recent_topics_stats()
    logger.info(f"üìä –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ç–µ–º—ã: {recent_stats}")
    
    for a in articles:
        text = f"{a.title} {a.summary}".lower()
        
        if any(p in text for p in BAD_PHRASES):
            posted.log_rejected(a, "BAD_PHRASES")
            continue
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            posted.log_rejected(a, "EXCLUDE_KEYWORDS")
            continue
        if not any(kw in text for kw in AI_KEYWORDS):
            posted.log_rejected(a, "NO_AI_KEYWORDS")
            continue
        if ai_relevance(text) < 0.4:
            posted.log_rejected(a, "LOW_AI_RELEVANCE")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ö–æ–∂–µ—Å—Ç—å —Å –Ω–µ–¥–∞–≤–Ω–∏–º–∏ –ø–æ—Å—Ç–∞–º–∏
        is_similar, reason = posted.is_too_similar_to_recent(a.title, a.summary)
        if is_similar:
            posted.log_rejected(a, f"TOO_SIMILAR_RECENT: {reason}")
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (—Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ): {a.title[:40]}")
            continue
        
        candidates.append(a)

    candidates.sort(key=lambda x: x.published, reverse=True)
    
    logger.info(f"üéØ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤: {len(candidates)} —Å—Ç–∞—Ç–µ–π")
    return candidates

# ====================== –ì–ï–ù–ï–†–ê–¢–û–† –ü–û–°–¢–û–í ======================
GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "llama3-70b-8192",
]

def exponential_backoff(attempt: int) -> float:
    """–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —Å jitter"""
    delay = min(
        config.groq_base_delay * (2 ** attempt) + random.uniform(0, 1),
        config.groq_max_delay
    )
    return delay

async def generate_summary(article: Article) -> Optional[str]:
    logger.info(f"üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {article.title[:55]}...")
    
    prompt = f"""–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –∫—Ä—É–ø–Ω–æ–≥–æ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –ò–ò –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.

–ù–û–í–û–°–¢–¨:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}
–¢–µ–∫—Å—Ç: {article.summary[:2200]}
–ò—Å—Ç–æ—á–Ω–∏–∫: {article.source}

–ó–ê–î–ê–ß–ê: –ù–∞–ø–∏—à–∏ –ø–æ—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–°–¢–†–£–ö–¢–£–†–ê (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):
1. üî• –ó–ê–ì–û–õ–û–í–û–ö ‚Äî —Ü–µ–ø–ª—è—é—â–∏–π, —Å —ç–º–æ–¥–∑–∏, –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å—É—Ç—å
2. –ß–¢–û –°–õ–£–ß–ò–õ–û–°–¨ ‚Äî 3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —Ñ–∞–∫—Ç–∞–º–∏ (–∫—Ç–æ, —á—Ç–æ, –∫–æ–≥–¥–∞, —Ü–∏—Ñ—Ä—ã)
3. –ü–û–ß–ï–ú–£ –í–ê–ñ–ù–û ‚Äî 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ –≤–ª–∏—è–Ω–∏–∏ –Ω–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—é/–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π  
4. –í–´–í–û–î ‚Äî –æ—Å—Ç—Ä—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏–ª–∏ –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –î–ª–∏–Ω–∞: 600-850 —Å–∏–º–≤–æ–ª–æ–≤ (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û)
- –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã, –Ω–∏–∫–∞–∫–æ–π –≤–æ–¥—ã
- –ö–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã

–ó–ê–ü–†–ï–©–ï–ù–û:
- –§—Ä–∞–∑—ã: "—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å", "–≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —á—Ç–æ", "–¥—Ä—É–∑—å—è"
- –®–∞–±–ª–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Ç–∏–ø–∞ "–ß—Ç–æ –¥—É–º–∞–µ—Ç–µ?"
- –ü—É—Å—Ç—ã–µ –æ–±–æ–±—â–µ–Ω–∏—è –±–µ–∑ —Ñ–∞–∫—Ç–æ–≤

–•–û–†–û–®–ò–ï –í–û–ü–†–û–°–´:
‚úì "Google —Å–Ω–æ–≤–∞ –¥–æ–≥–æ–Ω—è–µ—Ç ‚Äî –∏–ª–∏ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –æ–±–≥–æ–Ω–∏—Ç?"
‚úì "–°–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ –ø–æ—Ö–æ—Ä–æ–Ω–∏—Ç —ç—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ?"

–ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å ‚Äî –º—É—Å–æ—Ä/—Ä–µ–∫–ª–∞–º–∞/–Ω–µ –ø—Ä–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –æ—Ç–≤–µ—Ç—å: SKIP

–ü–û–°–¢:"""

    for attempt in range(config.groq_max_retries):
        try:
            await asyncio.sleep(0.5)
            
            resp = await asyncio.to_thread(
                groq_client.chat.completions.create,
                model=random.choice(GROQ_MODELS),
                temperature=0.7,
                max_tokens=1100,
                messages=[{"role": "user", "content": prompt}],
            )
            text = resp.choices[0].message.content.strip()

            if "SKIP" in text.upper()[:15]:
                logger.info("  ‚è≠Ô∏è LLM: SKIP")
                return None

            if len(text) < config.min_post_length:
                logger.warning(f"  ‚ö†Ô∏è –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤.), –ø–æ–≤—Ç–æ—Ä...")
                continue

            water = ["—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å", "–≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ", 
                    "–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º—Å—è", "–Ω–µ —Å–µ–∫—Ä–µ—Ç", "–æ—á–µ–≤–∏–¥–Ω–æ, —á—Ç–æ"]
            if any(w in text.lower() for w in water):
                logger.warning("  ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤–æ–¥–∞, –ø–æ–≤—Ç–æ—Ä...")
                continue

            topic = Topic.detect(f"{article.title} {article.summary}")
            hashtags = Topic.HASHTAGS.get(topic, Topic.HASHTAGS[Topic.GENERAL])
            
            cta = "\n\nüî• ‚Äî –æ–≥–æ–Ω—å  |  üóø ‚Äî –Ω—É —Ç–∞–∫–æ–µ  |  ‚ö° ‚Äî –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ"
            source_link = f'\n\nüîó <a href="{article.link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
            
            final = f"{text}{cta}\n\n{hashtags}{source_link}"

            if len(final) > config.caption_limit:
                excess = len(final) - config.caption_limit + 20
                text = text[:-excess]
                for p in ['. ', '! ', '? ']:
                    idx = text.rfind(p)
                    if idx > len(text) * 0.6:
                        text = text[:idx+1]
                        break
                final = f"{text}{cta}\n\n{hashtags}{source_link}"

            logger.info(f"  ‚úÖ –ì–æ—Ç–æ–≤–æ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ | –¢–µ–º–∞: {topic}")
            return final
            
        except Exception as e:
            delay = exponential_backoff(attempt)
            logger.error(f"  ‚ùå Groq –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{config.groq_max_retries}): {e}")
            logger.info(f"  ‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay:.1f}s...")
            await asyncio.sleep(delay)

    return None

# ====================== –ö–ê–†–¢–ò–ù–ö–ò ======================
async def generate_image(title: str, topic: str = Topic.GENERAL) -> Optional[str]:
    logger.info(f"  üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏ –¥–ª—è —Ç–µ–º—ã: {topic}")
    
    clean_title = re.sub(r'[^\w\s]', '', title)[:50]
    style = Topic.get_image_style(topic)
    prompt = f"{style}, {clean_title}, high quality, 4k, sharp focus"
    
    url = (
        f"https://image.pollinations.ai/prompt/{quote(prompt)}"
        f"?width=1024&height=1024&nologo=true&seed={random.randint(1,99999)}"
    )
    
    for attempt in range(3):
        try:
            async with aiohttp.ClientSession() as sess:
                async with sess.get(url, timeout=aiohttp.ClientTimeout(total=45)) as resp:
                    if resp.status != 200:
                        logger.warning(f"  ‚ö†Ô∏è HTTP {resp.status}, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}")
                        continue
                    
                    data = await resp.read()
                    
                    if len(data) < 10000:
                        logger.warning(f"  ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª ({len(data)} bytes)")
                        continue
                    
                    fname = f"img_{random.randint(1000,9999)}.jpg"
                    with open(fname, "wb") as f:
                        f.write(data)
                    
                    logger.info(f"  ‚úÖ –ö–∞—Ä—Ç–∏–Ω–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {fname} ({len(data)//1024}KB)")
                    return fname
                    
        except asyncio.TimeoutError:
            logger.warning(f"  ‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/3")
            await asyncio.sleep(2)
        except Exception as e:
            logger.warning(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({attempt+1}/3): {e}")
            await asyncio.sleep(2)
    
    logger.warning("  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É")
    return None

# ====================== –ü–£–ë–õ–ò–ö–ê–¶–ò–Ø ======================
async def post_article(article: Article, text: str, posted: PostedManager) -> bool:
    topic = Topic.detect(f"{article.title} {article.summary}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    diversity_ok, diversity_reason = posted.check_diversity_requirement(topic)
    if not diversity_ok:
        posted.log_rejected(article, diversity_reason)
        logger.info(f"  ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ (—Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ): {diversity_reason}")
        return False
    
    # LLM-–ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    recent_posts = posted.get_recent_posts(3)
    is_llm_dup, llm_reason = posted.llm_duplicate_check(article, recent_posts)
    if is_llm_dup:
        posted.log_rejected(article, llm_reason)
        logger.info(f"  ‚è≠Ô∏è LLM –æ–ø—Ä–µ–¥–µ–ª–∏–ª –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç")
        return False
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img = await generate_image(article.title, topic)
    
    try:
        if img and os.path.exists(img):
            await bot.send_photo(config.channel_id, FSInputFile(img), caption=text)
            os.remove(img)
        else:
            await bot.send_message(config.channel_id, text, disable_web_page_preview=False)
        
        posted.add(article, topic)
        
        logger.info(f"‚úÖ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û [{topic.upper()}]: {article.title[:50]}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Telegram –æ—à–∏–±–∫–∞: {e}")
        if img and os.path.exists(img):
            try:
                os.remove(img)
            except:
                pass
        return False

# ====================== MAIN ======================
async def main():
    logger.info("=" * 50)
    logger.info("üöÄ –ó–ê–ü–£–°–ö AI-POSTER v3.0 (SQLite + –£—Å–∏–ª–µ–Ω–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è)")
    logger.info("=" * 50)
    
    posted = PostedManager(config.db_file)
    posted.cleanup(config.retention_days)
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = posted.get_stats()
    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î: {stats['total_posted']} posted, {stats['total_rejected']} rejected")
    
    raw_articles = await load_all_feeds(posted)
    candidates = filter_articles(raw_articles, posted)
    
    if not candidates:
        logger.info("üì≠ –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return

    for article in candidates[:20]:
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        is_dup, reason = posted.is_duplicate(article.link, article.title, article.summary)
        if is_dup:
            posted.log_rejected(article, f"FINAL_CHECK: {reason}")
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞): {article.title[:40]}")
            continue
        
        is_similar, reason = posted.is_too_similar_to_recent(article.title, article.summary)
        if is_similar:
            posted.log_rejected(article, f"FINAL_RECENT: {reason}")
            logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (–ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ): {article.title[:40]}")
            continue
        
        summary = await generate_summary(article)
        if not summary:
            posted.log_rejected(article, "LLM_GENERATION_FAILED")
            continue
        
        if await post_article(article, summary, posted):
            logger.info("\nüèÅ –ì–æ—Ç–æ–≤–æ! –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à—ë–Ω.")
            break
        
        await asyncio.sleep(3)
    else:
        logger.info("üòî –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏")

    await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())






























































































































































































































































