import os
import json
import asyncio
import random
from datetime import datetime
from typing import List, Dict, Optional

import requests
import feedparser
import urllib.parse
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from openai import OpenAI

# ============ CONFIG ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

if not all([OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    raise ValueError("‚ùå –ù–µ –≤—Å–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

POSTED_FILE = "posted_articles.json"
RETENTION_DAYS = 7

# ============ –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ï –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê (–ò–ò/–ù–ï–ô–†–û–°–ï–¢–ò) ============

AI_PRIORITY_KEYWORDS = [
    # –û–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –ò–ò
    "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å", "–∏–∏", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
    # LLM –∏ –º–æ–¥–µ–ª–∏
    "llm", "gpt", "gpt-4", "gpt-5", "chatgpt", "claude", "gemini",
    "copilot", "mistral", "llama", "qwen", "gigachat", "yandexgpt",
    "kandinsky", "—à–µ–¥–µ–≤—Ä—É–º",
    # –ö–æ–º–ø–∞–Ω–∏–∏ –ò–ò
    "openai", "anthropic", "deepmind", "—Å–±–µ—Ä", "—è–Ω–¥–µ–∫—Å", "sber",
    "yandex", "hugging face", "stability ai", "meta ai",
    # –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
    "stable diffusion", "midjourney", "dall-e", "sora", "runway",
    "–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
    # ML —Ç–µ—Ä–º–∏–Ω—ã
    "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "transformer",
    "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π",
    "–¥–æ–æ–±—É—á–µ–Ω–∏–µ", "–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", "–¥–∞—Ç–∞—Å–µ—Ç",
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ò–ò
    "—á–∞—Ç-–±–æ—Ç", "–≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
    "–Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π", "ai-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "—É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫",
    # –ù–æ–≤—ã–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    "agi", "—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "–∞–≥–µ–Ω—Ç", "–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ", "—Ç–æ–∫–µ–Ω",
    "–±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å"
]

# ============ –û–ë–´–ß–ù–´–ï –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê (–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –ø—Ä–∏–Ω—Ü–∏–ø) ============

GENERAL_KEYWORDS = [
    # –ê–Ω–æ–Ω—Å—ã —Ç–µ—Ö–Ω–∏–∫–∏
    "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª", "–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª", "–≤—ã–ø—É—Å—Ç–∏–ª", "—Ä–µ–ª–∏–∑", "–∑–∞–ø—É—Å—Ç–∏–ª",
    "–Ω–æ–≤–∏–Ω–∫–∞", "–¥–µ–±—é—Ç", "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è",
    # –ñ–µ–ª–µ–∑–æ
    "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—á–∏–ø", "cpu", "gpu", "–≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞",
    "—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "–Ω–æ—É—Ç–±—É–∫", "–≥–∞–¥–∂–µ—Ç", "—Ä–æ–±–æ—Ç", "–∫–≤–∞–Ω—Ç–æ–≤—ã–π",
    # –ö–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å (–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –ø—Ä–∏–Ω—Ü–∏–ø)
    "—É—è–∑–≤–∏–º–æ—Å—Ç—å", "–≤–∑–ª–æ–º", "—Ö–∞–∫–µ—Ä", "–∫–∏–±–µ—Ä–∞—Ç–∞–∫–∞", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
    "–ø–∞—Ç—á", "malware", "ransomware", "0-day"
]

# ============ –ò–°–ö–õ–Æ–ß–ò–¢–¨ ============

EXCLUDE_KEYWORDS = [
    "—Ç–µ–Ω–Ω–∏—Å", "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "–±–∞—Å–∫–µ—Ç–±–æ–ª", "—Å–ø–æ—Ä—Ç", "–º–∞—Ç—á",
    "–∏–≥—Ä–∞", "–≥–µ–π–º–ø–ª–µ–π", "playstation", "xbox", "steam", "nintendo",
    "–∫–∏–Ω–æ", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "–º—É–∑—ã–∫–∞", "–∫–æ–Ω—Ü–µ—Ä—Ç", "–∞–∫—Ç—ë—Ä", "–∞–∫—Ç–µ—Ä",
    "–≤—ã–±–æ—Ä—ã", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–ø–∞—Ä–ª–∞–º–µ–Ω—Ç", "–ø–æ–ª–∏—Ç–∏–∫", "–¥–µ–ø—É—Ç–∞—Ç",
    "–±–æ–ª–µ–∑–Ω—å", "–≤–∏—Ä—É—Å", "covid", "–ø–∞–Ω–¥–µ–º–∏—è", "–≥—Ä–∏–ø–ø",
    "–∫—Ä–∏–ø—Ç–æ", "bitcoin", "–±–∏—Ç–∫–æ–π–Ω", "ethereum", "nft", "–±–ª–æ–∫—á–µ–π–Ω",
    "—Å—É–¥", "—Å—É–¥–µ–±–Ω—ã–π", "–∞—Ä–µ—Å—Ç", "–ø—Ä–∏–≥–æ–≤–æ—Ä"
]

# ============ STATE ============

posted_articles: Dict[str, Optional[float]] = {}

if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        try:
            posted_data = json.load(f)
            posted_articles = {item["id"]: item.get("timestamp") for item in posted_data}
        except Exception:
            posted_articles = {}


def save_posted_articles() -> None:
    data = [{"id": id_str, "timestamp": ts} for id_str, ts in posted_articles.items()]
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def clean_old_posts() -> None:
    global posted_articles
    now = datetime.now().timestamp()
    cutoff = now - (RETENTION_DAYS * 86400)
    posted_articles = {
        id_str: ts for id_str, ts in posted_articles.items()
        if ts is None or ts > cutoff
    }
    save_posted_articles()


def save_posted(article_id: str) -> None:
    posted_articles[article_id] = datetime.now().timestamp()
    save_posted_articles()


# ============ HELPERS ============

def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())


# ============ PARSERS ============

def load_rss(url: str, source: str) -> List[Dict]:
    articles = []
    try:
        feed = feedparser.parse(url)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RSS {url}: {e}")
        return articles

    for entry in feed.entries[:30]:
        link = entry.get("link", "")
        if not link or link in posted_articles:
            continue
        articles.append({
            "id": link,
            "title": clean_text(entry.get("title") or ""),
            "summary": clean_text(
                entry.get("summary") or entry.get("description") or ""
            )[:700],
            "link": link,
            "source": source,
            "published_parsed": datetime.now()
        })
    return articles


def load_articles_from_sites() -> List[Dict]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –¢–û–õ–¨–ö–û —Å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Äî –ò–ò/–Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    """
    articles: List[Dict] = []

    # === –ü–†–ò–û–†–ò–¢–ï–¢: –ò–ò –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (—Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ) ===

    # Habr - –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru",
        "Habr AI"
    ))

    # Habr - –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru",
        "Habr ML"
    ))

    # Habr - –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/neural_networks/all/?fl=ru",
        "Habr Neural"
    ))

    # Habr - Data Science
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/data_science/all/?fl=ru",
        "Habr DS"
    ))

    # Habr - Natural Language Processing
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/natural_language_processing/all/?fl=ru",
        "Habr NLP"
    ))

    # Habr - Big Data
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/bigdata/all/?fl=ru",
        "Habr BigData"
    ))

    # Tproger - –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    articles.extend(load_rss(
        "https://tproger.ru/feed/",
        "Tproger"
    ))

    # VC.ru - –≤—Å—ë (—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)
    articles.extend(load_rss(
        "https://vc.ru/rss/all",
        "VC.ru"
    ))

    # –¢–ê–°–° - –ù–∞—É–∫–∞
    articles.extend(load_rss(
        "https://tass.ru/rss/v2.xml?sections=nauka",
        "–¢–ê–°–° –ù–∞—É–∫–∞"
    ))

    # –†–ò–ê –ù–∞—É–∫–∞
    articles.extend(load_rss(
        "https://ria.ru/export/rss2/science/index.xml",
        "–†–ò–ê –ù–∞—É–∫–∞"
    ))

    # === –û–ë–©–ò–ï –¢–ï–•–ù–û–õ–û–ì–ò–ò (—Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ) ===

    # 3DNews - —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    articles.extend(load_rss(
        "https://3dnews.ru/news/rss/",
        "3DNews"
    ))

    # iXBT - –∂–µ–ª–µ–∑–æ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    articles.extend(load_rss(
        "https://www.ixbt.com/export/news.rss",
        "iXBT"
    ))

    # ServerNews
    articles.extend(load_rss(
        "https://servernews.ru/rss",
        "ServerNews"
    ))

    # –•–∞–π—Ç–µ–∫ (hightech.fm)
    articles.extend(load_rss(
        "https://hightech.fm/feed",
        "–•–∞–π—Ç–µ–∫"
    ))

    # === –û–°–¢–ê–¢–û–ß–ù–´–ô –ü–†–ò–ù–¶–ò–ü: –ö–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å ===

    # Xakep
    articles.extend(load_rss(
        "https://xakep.ru/feed/",
        "Xakep"
    ))

    return articles


# ============ –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –° –ü–†–ò–û–†–ò–¢–ï–¢–û–ú –ò–ò ============

def filter_articles(articles: List[Dict]) -> List[Dict]:
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ –ò–ò/–Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–Ω–∞—á–∞–ª–∞ –ò–ò-—Å—Ç–∞—Ç—å–∏, –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ.
    """
    ai_articles = []
    general_articles = []

    for e in articles:
        text = f"{e['title']} {e['summary']}".lower()

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å–∫–ª—é—á—ë–Ω–Ω—ã–µ —Ç–µ–º—ã
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ò–ò/–Ω–µ–π—Ä–æ—Å–µ—Ç–∏)
        if any(kw in text for kw in AI_PRIORITY_KEYWORDS):
            ai_articles.append(e)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—ã—á–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        elif any(kw in text for kw in GENERAL_KEYWORDS):
            general_articles.append(e)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
    ai_articles.sort(key=lambda x: x["published_parsed"], reverse=True)
    general_articles.sort(key=lambda x: x["published_parsed"], reverse=True)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º: —Å–Ω–∞—á–∞–ª–∞ –ò–ò —Å—Ç–∞—Ç—å–∏, –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
    return ai_articles + general_articles


# ============ OPENAI TEXT ============

def short_summary(title: str, summary: str, link: str) -> Optional[str]:
    news_text = f"{title}. {summary}"
    prompt = (
        "–í–æ—Ç —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –ò–ò/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –°–¥–µ–ª–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π –æ–±–∑–æ—Ä –¥–ª—è Telegram –Ω–∞ —Ä—É—Å—Å–∫–æ–º:\n"
        f"{news_text}\n\n"
        "- –û–±—ä—ë–º: 380‚Äì450 —Å–∏–º–≤–æ–ª–æ–≤.\n"
        "- –§–æ–∫—É—Å: –ß—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏, –∫–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –ø–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ.\n"
        "- –°—Ç–∏–ª—å: –ñ–∏–≤–æ–π, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π. –ò—Å–ø–æ–ª—å–∑—É–π 1-2 —ç–º–æ–¥–∑–∏ –ø–æ —Ç–µ–º–µ (ü§ñüß†üí°üöÄ).\n"
        "- –§–æ—Ä–º–∞—Ç: 2-3 –∫–ª—é—á–µ–≤—ã–µ —Ñ–∏—à–∫–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏–ª–∏ –º–æ–¥–µ–ª–∏.\n"
        "- –í –∫–æ–Ω—Ü–µ: 2-3 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ö–µ—à—Ç–µ–≥–∞ (#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–ò–ò).\n"
        "- –Ø–∑—ã–∫: –¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π!\n"
        "- –ó–∞–ø—Ä–µ—â–µ–Ω–æ: –í—ã–¥—É–º—ã–≤–∞—Ç—å —Ñ–∞–∫—Ç—ã, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª–∏—à–µ —Ç–∏–ø–∞ '–º–∏—Ä –Ω–µ —Å—Ç–æ–∏—Ç –Ω–∞ –º–µ—Å—Ç–µ'.\n"
        "- –°—Å—ã–ª–∫—É –∏ –ø–æ–¥–ø–∏—Å–∏ –≤ —Ç–µ–∫—Å—Ç –Ω–µ –≤–∫–ª—é—á–∞–π."
    )

    try:
        res = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=400,
        )
        core = res.choices[0].message.content.strip()

        src = f"\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {link}"
        ps = "\n\nPSüí• –ö—Ç–æ –∑–∞ –∫–ª—é—á–∞–º–∏ üëâ https://t.me/+EdEfIkn83Wg3ZTE6"
        return core + src + ps
    except Exception as e:
        print(f"‚ùå OpenAI: {e}")
        return None


# ============ –ö–ê–†–¢–ò–ù–ö–ò ============

def generate_image(title: str) -> Optional[str]:
    seed = random.randint(0, 10**6)
    prompt = (
        f"Futuristic AI technology illustration: {title[:80]}, "
        "neural networks, digital brain, glowing circuits, "
        "clean minimal design, soft blue lighting, 4k, no text."
    )
    try:
        encoded = urllib.parse.quote(prompt)
        url = f"https://image.pollinations.ai/prompt/{encoded}?seed={seed}&width=1024&height=1024"
        resp = requests.get(url, timeout=60)
        if resp.status_code == 200:
            fname = f"img_{seed}.jpg"
            with open(fname, "wb") as f:
                f.write(resp.content)
            return fname
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
    return None


# ============ –ê–í–¢–û–ü–û–°–¢ ============

async def autopost():
    clean_old_posts()
    articles = load_articles_from_sites()
    candidates = filter_articles(articles)

    if not candidates:
        print("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ –ò–ò/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.")
        return

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    ai_count = sum(1 for a in candidates if any(
        kw in f"{a['title']} {a['summary']}".lower()
        for kw in AI_PRIORITY_KEYWORDS
    ))
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(candidates)} —Å—Ç–∞—Ç–µ–π ({ai_count} –ø—Ä–æ –ò–ò)")

    for art in candidates[:5]:
        print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞: {art['title'][:60]}...")
        post_text = short_summary(art["title"], art["summary"], art["link"])

        if post_text:
            img = generate_image(art["title"])
            try:
                if img:
                    await bot.send_photo(
                        CHANNEL_ID,
                        photo=FSInputFile(img),
                        caption=post_text
                    )
                    os.remove(img)
                else:
                    await bot.send_message(CHANNEL_ID, text=post_text)

                save_posted(art["id"])
                print(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {art['source']}")
                break
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
                if img and os.path.exists(img):
                    os.remove(img)


async def main():
    try:
        await autopost()
    finally:
        await bot.session.close()


if __name__ == "__main__":
    asyncio.run(main())




































































































