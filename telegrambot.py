import os
import json
import asyncio
import random
import re
import hashlib
import logging
import difflib
import tempfile
import shutil
import sqlite3
import time
from datetime import datetime, timezone, timedelta
from typing import List, Set, Optional, Tuple, Dict
from urllib.parse import urlparse, quote, parse_qs, urlencode
from dataclasses import dataclass, field
from collections import Counter
import math

import aiohttp
import feedparser
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq

# ====================== –õ–û–ì–ò ======================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("ai_poster.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ====================== CONFIG ======================
class Config:
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.telegram_token = os.getenv("TELEGRAM_BOT_TOKEN")
        self.channel_id = os.getenv("CHANNEL_ID")
        self.retention_days = int(os.getenv("RETENTION_DAYS", "60"))  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 60
        self.caption_limit = 1024
        self.db_file = "posted_articles.db"
        
        self.similarity_threshold = 0.72  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 0.60
        self.entity_overlap_threshold = 0.55
        self.min_post_length = 500
        self.min_summary_length = 200  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 100
        self.max_article_age_hours = 24  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 72 (3 –¥–Ω—è)
        
        # TF-IDF –ø–æ—Ä–æ–≥
        self.tfidf_similarity_threshold = 0.65
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        self.recent_posts_check = 5
        self.recent_similarity_threshold = 0.45
        self.min_entity_distance = 2
        self.diversity_window = 3  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        
        # Retry –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.groq_max_retries = 5
        self.groq_base_delay = 1.0
        self.groq_max_delay = 30.0
        
        # RSS –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.rss_timeout = 20
        self.rss_jitter = 3

        missing = []
        for var, name in [(self.groq_api_key, "GROQ_API_KEY"),
                          (self.telegram_token, "TELEGRAM_BOT_TOKEN"),
                          (self.channel_id, "CHANNEL_ID")]:
            if not var:
                missing.append(name)
        if missing:
            raise SystemExit(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(missing)}")

config = Config()

bot = Bot(token=config.telegram_token, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=config.groq_api_key)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

# ====================== RSS (–†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö) ======================
RSS_FEEDS = [
    ("https://techcrunch.com/category/artificial-intelligence/feed/", "TechCrunch"),
    ("https://venturebeat.com/category/ai/feed/", "VentureBeat"),
    ("https://www.technologyreview.com/topic/artificial-intelligence/feed", "MIT Tech Review"),
    ("https://www.theverge.com/rss/index.xml", "The Verge"),
    ("https://arstechnica.com/tag/artificial-intelligence/feed/", "Ars Technica"),
    ("https://www.wired.com/feed/tag/ai/latest/rss", "WIRED"),
    ("https://www.artificialintelligence-news.com/feed/", "AI News"),
    ("https://hai.stanford.edu/news/rss.xml", "Stanford HAI"),
    ("https://deepmind.google/blog/rss.xml", "DeepMind Blog"),
    ("https://openai.com/blog/rss/", "OpenAI Blog"),
    ("https://blog.google/technology/ai/rss/", "Google AI Blog"),
    ("https://www.marktechpost.com/feed/", "MarkTechPost"),
    ("https://syncedreview.com/feed/", "Synced AI"),
    ("https://news.ycombinator.com/rss", "Hacker News"),
    ("https://www.unite.ai/feed/", "Unite.AI"),
    ("https://analyticsindiamag.com/feed/", "AIM"),
]

# ====================== –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ======================
AI_KEYWORDS = [
    "ai", "artificial intelligence", "machine learning", "deep learning",
    "neural network", "llm", "large language model", "gpt", "chatgpt", "claude",
    "gemini", "grok", "llama", "mistral", "qwen", "deepseek", "midjourney",
    "dall-e", "stable diffusion", "sora", "groq", "openai", "anthropic",
    "deepmind", "hugging face", "nvidia", "agi", "transformer", "generative",
    "agents", "reasoning", "multimodal", "fine-tuning", "rlhf", "o3", "o1",
    "cursor", "copilot", "replit", "v0", "perplexity", "cohere", "01.ai"
]

EXCLUDE_KEYWORDS = [
    "stock price", "ipo", "earnings call", "quarterly results", "dividend",
    "market cap", "wall street", "ps5", "xbox", "nintendo", "game review",
    "netflix", "movie review", "box office", "trailer", "tesla stock",
    "bitcoin", "crypto", "blockchain", "nft", "ethereum", "election",
    "trump", "biden", "congress", "senate"
]

BAD_PHRASES = ["sponsored", "partner content", "advertisement", "black friday", "deal alert"]

# ====================== –†–ê–°–®–ò–†–ï–ù–ù–´–ï –°–£–©–ù–û–°–¢–ò ======================
KEY_ENTITIES = [
    # –ö–æ–º–ø–∞–Ω–∏–∏
    "openai", "google", "meta", "microsoft", "anthropic", "nvidia", "apple",
    "amazon", "deepmind", "hugging face", "stability ai", "midjourney",
    "mistral", "cohere", "perplexity", "runway", "pika", "character ai",
    "inflection", "xai", "baidu", "alibaba", "tencent", "bytedance",
    "01.ai", "moonshot ai", "zhipu ai", "ai21 labs", "adept", "adept ai",
    "elevenlabs", "heygen", "synthesia", "jasper", "copy.ai", "replika",
    
    # –ú–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
    "gpt-4", "gpt-5", "gpt-4o", "gpt-4.5", "chatgpt", "claude", "claude 3",
    "claude 3.5", "claude 3.5 sonnet", "claude 3 opus", "gemini", "gemini 2",
    "gemini 2.0", "gemini 1.5", "llama", "llama 3", "llama 3.3", "llama 3.2",
    "llama 3.1", "mistral", "mixtral", "mixtral 8x7b", "mixtral 8x22b",
    "copilot", "github copilot", "microsoft copilot", "dall-e", "dall-e 3",
    "dall-e 2", "sora", "stable diffusion", "stable diffusion 3", "flux",
    "midjourney v6", "midjourney v7", "runway gen", "runway gen-3", "firefly",
    "adobe firefly", "imagen", "imagen 3", "grok", "grok 2", "grok 3",
    "deepseek", "deepseek-v3", "deepseek-v2", "deepseek-r1", "qwen",
    "qwen 2", "qwen 2.5", "yi", "yi-34b", "command r", "command r+",
    "o3", "o3 mini", "o1", "o1 mini", "o1 preview", "gpt-4o mini",
    
    # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
    "cursor", "cursor ai", "replit", "replit agent", "v0", "v0.dev",
    "vercel v0", "bolt", "bolt.new", "lovable", "temporal", "langchain",
    "llamaindex", "crewai", "autogen", "semantic kernel", "haystack",
    "weaviate", "pinecone", "chroma", "qdrant", "milvus", "modal",
    "replicate", "together ai", "fireworks ai", "baseten", "banana dev",
    
    # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
    "linux foundation", "agentic", "ai agent", "ai agents", "agi", "asi",
    "artificial general intelligence", "regulation", "safety", "alignment",
    "ai safety", "ai alignment", "open source", "open-source", "open weights",
    "robotics", "humanoid", "boston dynamics", "figure", "figure ai",
    "optimus", "tesla bot", "unitree", "agility robotics", "digit",
    "apptronik", "1x technologies", "sanctuary ai", "covariant",
    
    # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    "transformer", "transformers", "diffusion", "diffusion model",
    "multimodal", "multimodal ai", "reasoning", "chain of thought",
    "cot", "fine-tuning", "rlhf", "inference", "training", "benchmark",
    "rag", "retrieval augmented generation", "vector database", "embedding",
    "token", "context window", "prompt engineering", "jailbreak",
    "hallucination", "ai hallucination", "synthetic data",
]

# ====================== DATACLASS ======================
@dataclass
class Article:
    title: str
    summary: str
    link: str
    source: str
    published: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

# ====================== TOPIC & HASHTAGS ======================
class Topic:
    LLM = "llm"
    IMAGE_GEN = "image_gen"
    ROBOTICS = "robotics"
    HARDWARE = "hardware"
    REGULATION = "regulation"
    RESEARCH = "research"
    AGENTS = "agents"
    CODING = "coding"
    GENERAL = "general"
    
    HASHTAGS = {
        LLM: "#ChatGPT #LLM #OpenAI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        IMAGE_GEN: "#Midjourney #StableDiffusion #ImageGen #–ò–ò–ê—Ä—Ç",
        ROBOTICS: "#–ò–ò #—Ä–æ–±–æ—Ç—ã #—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞ #–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è",
        HARDWARE: "#NVIDIA #—á–∏–ø—ã #GPU #–∂–µ–ª–µ–∑–æ",
        REGULATION: "#—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ #–∑–∞–∫–æ–Ω—ã #—ç—Ç–∏–∫–∞ #–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
        RESEARCH: "#–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è #–Ω–∞—É–∫–∞ #ML #DeepLearning",
        AGENTS: "#–∞–≥–µ–Ω—Ç—ã #–∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å #AutoGPT #AI",
        CODING: "#–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ #–∫–æ–¥ #—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ #DevTools",
        GENERAL: "#–ò–ò #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ #AI"
    }
    
    IMAGE_STYLES = {
        LLM: "futuristic digital brain, circuit patterns, neural network visualization, blue purple gradient",
        IMAGE_GEN: "creative art studio, digital canvas, vibrant colors, artistic AI generation",
        ROBOTICS: "sleek humanoid robot, high-tech laboratory, metallic surfaces, dramatic lighting",
        HARDWARE: "advanced computer chips, circuit boards, neon lights, technological precision",
        REGULATION: "digital scales of justice, government building, legal documents, professional",
        RESEARCH: "scientific laboratory, data visualization, graphs and charts, academic",
        AGENTS: "autonomous systems, interconnected nodes, workflow automation, modern tech",
        CODING: "code editor interface, programming environment, dark theme, developer workspace",
        GENERAL: "abstract technology, digital innovation, modern tech aesthetic, clean design"
    }
    
    @staticmethod
    def detect(text: str) -> str:
        text_lower = text.lower()
        
        llm_terms = ["gpt", "claude", "gemini", "llm", "chatbot", "language model", 
                     "chatgpt", "llama", "mistral", "deepseek", "qwen", "reasoning"]
        if any(term in text_lower for term in llm_terms):
            return Topic.LLM
        
        image_terms = ["dall-e", "midjourney", "stable diffusion", "image generation",
                      "text-to-image", "imagen", "firefly", "flux", "sora", "video generation"]
        if any(term in text_lower for term in image_terms):
            return Topic.IMAGE_GEN
        
        robot_terms = ["robot", "humanoid", "automation", "robotic", "boston dynamics",
                      "figure ai", "optimus", "tesla bot"]
        if any(term in text_lower for term in robot_terms):
            return Topic.ROBOTICS
        
        hw_terms = ["nvidia", "chip", "gpu", "hardware", "semiconductor", "processor",
                   "tpu", "asic", "groq chip"]
        if any(term in text_lower for term in hw_terms):
            return Topic.HARDWARE
        
        reg_terms = ["regulation", "policy", "law", "government", "ethical", "ban",
                    "restriction", "compliance", "legal"]
        if any(term in text_lower for term in reg_terms):
            return Topic.REGULATION
        
        research_terms = ["research", "paper", "study", "breakthrough", "discovery",
                         "scientific", "experiment", "arxiv"]
        if any(term in text_lower for term in research_terms):
            return Topic.RESEARCH
        
        agent_terms = ["agent", "autonomous", "autogpt", "workflow", "automation tool",
                      "ai assistant", "personal ai"]
        if any(term in text_lower for term in agent_terms):
            return Topic.AGENTS
        
        code_terms = ["coding", "copilot", "cursor", "programming", "developer",
                     "ide", "code generation", "replit", "v0"]
        if any(term in text_lower for term in code_terms):
            return Topic.CODING
        
        return Topic.GENERAL
    
    @staticmethod
    def get_image_style(topic: str) -> str:
        return Topic.IMAGE_STYLES.get(topic, Topic.IMAGE_STYLES[Topic.GENERAL])

# ====================== GROQ –ú–û–î–ï–õ–ò ======================
GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "llama-3.1-70b-versatile",
]

# ====================== UTILITY FUNCTIONS ======================
def normalize_url(url: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    parsed = urlparse(url.lower())
    
    # –£–±–∏—Ä–∞–µ–º query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞
    if parsed.query:
        query_params = parse_qs(parsed.query)
        tracking_params = {'utm_source', 'utm_medium', 'utm_campaign', 'utm_content', 
                          'fbclid', 'gclid', 'ref', 'source'}
        clean_params = {k: v for k, v in query_params.items() if k not in tracking_params}
        clean_query = urlencode(clean_params, doseq=True) if clean_params else ''
    else:
        clean_query = ''
    
    # –£–±–∏—Ä–∞–µ–º trailing slash
    path = parsed.path.rstrip('/')
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π URL
    norm = f"{parsed.netloc}{path}"
    if clean_query:
        norm += f"?{clean_query}"
    
    return norm

def get_domain(url: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL"""
    parsed = urlparse(url.lower())
    domain = parsed.netloc
    # –£–±–∏—Ä–∞–µ–º www.
    if domain.startswith('www.'):
        domain = domain[4:]
    return domain

def get_title_signature(title: str) -> str:
    """–°–æ–∑–¥–∞—ë—Ç —Å–∏–≥–Ω–∞—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–ø–µ—Ä–≤—ã–µ 4 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤–∞)"""
    words = re.findall(r'\b[a-zA-Z]{4,}\b', title.lower())
    return ' '.join(words[:4]) if words else title.lower()[:30]

def get_summary_hash(summary: str) -> str:
    """–•–µ—à summary –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    clean = re.sub(r'\s+', ' ', summary.lower().strip())
    return hashlib.md5(clean.encode('utf-8')).hexdigest()

def get_content_hash(text: str, length: int = 200) -> Optional[str]:
    """–•–µ—à –ø–µ—Ä–≤—ã—Ö N —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    if not text or len(text) < 50:
        return None
    clean = re.sub(r'\s+', ' ', text.lower().strip())[:length]
    return hashlib.md5(clean.encode('utf-8')).hexdigest()

def calculate_similarity(str1: str, str2: str) -> float:
    """SequenceMatcher similarity"""
    return difflib.SequenceMatcher(None, str1.lower(), str2.lower()).ratio()

def ngram_similarity(str1: str, str2: str, n: int = 3) -> float:
    """N-gram similarity –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
    def get_ngrams(text: str, n: int) -> Set[str]:
        words = text.lower().split()
        return set(' '.join(words[i:i+n]) for i in range(len(words) - n + 1))
    
    if len(str1.split()) < n or len(str2.split()) < n:
        return 0.0
    
    ngrams1 = get_ngrams(str1, n)
    ngrams2 = get_ngrams(str2, n)
    
    if not ngrams1 or not ngrams2:
        return 0.0
    
    intersection = len(ngrams1 & ngrams2)
    union = len(ngrams1 | ngrams2)
    
    return intersection / union if union > 0 else 0.0

def extract_key_entities(text: str) -> Set[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_lower = text.lower()
    found = set()
    
    for entity in KEY_ENTITIES:
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ
        if entity in text_lower:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ —á–∞—Å—Ç—å –¥—Ä—É–≥–æ–≥–æ —Å–ª–æ–≤–∞
            pattern = r'\b' + re.escape(entity) + r'\b'
            if re.search(pattern, text_lower):
                found.add(entity)
    
    return found

def fuzzy_entity_match(entities1: Set[str], entities2: Set[str], threshold: float = 0.85) -> float:
    """Fuzzy matching —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π (GPT-4 vs GPT4)"""
    if not entities1 or not entities2:
        return 0.0
    
    matches = 0
    for e1 in entities1:
        for e2 in entities2:
            sim = calculate_similarity(e1, e2)
            if sim >= threshold:
                matches += 1
                break
    
    max_size = max(len(entities1), len(entities2))
    return matches / max_size if max_size > 0 else 0.0

def tfidf_cosine_similarity(docs: List[str]) -> List[List[float]]:
    """TF-IDF –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
    if len(docs) < 2:
        return [[1.0]]
    
    # Tokenize
    all_words = set()
    tokenized_docs = []
    for doc in docs:
        words = re.findall(r'\b\w+\b', doc.lower())
        tokenized_docs.append(words)
        all_words.update(words)
    
    # Term frequency
    tf_docs = []
    for words in tokenized_docs:
        word_count = Counter(words)
        total = len(words)
        tf = {word: count / total for word, count in word_count.items()}
        tf_docs.append(tf)
    
    # Inverse document frequency
    idf = {}
    num_docs = len(docs)
    for word in all_words:
        doc_count = sum(1 for tf in tf_docs if word in tf)
        idf[word] = math.log(num_docs / doc_count) if doc_count > 0 else 0
    
    # TF-IDF vectors
    tfidf_vectors = []
    for tf in tf_docs:
        vector = {word: tf.get(word, 0) * idf.get(word, 0) for word in all_words}
        tfidf_vectors.append(vector)
    
    # Cosine similarity
    similarity_matrix = []
    for i, vec1 in enumerate(tfidf_vectors):
        row = []
        for j, vec2 in enumerate(tfidf_vectors):
            if i == j:
                row.append(1.0)
            else:
                dot_product = sum(vec1.get(word, 0) * vec2.get(word, 0) for word in all_words)
                mag1 = math.sqrt(sum(v**2 for v in vec1.values()))
                mag2 = math.sqrt(sum(v**2 for v in vec2.values()))
                cos_sim = dot_product / (mag1 * mag2) if mag1 > 0 and mag2 > 0 else 0.0
                row.append(cos_sim)
        similarity_matrix.append(row)
    
    return similarity_matrix

import threading

# ====================== POSTED MANAGER (SQLite) ======================
class PostedManager:
    """SQLite-based –º–µ–Ω–µ–¥–∂–µ—Ä —Å –∞—Ç–æ–º–∞—Ä–Ω—ã–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏ –∏ advisory locks"""
    
    def __init__(self, db_file: str = "posted_articles.db"):
        self.db_file = db_file
        self._lock = threading.Lock()
        self._conn = None
        self._init_db()
    
    def _get_conn(self) -> sqlite3.Connection:
        """–ü–æ–ª—É—á–∞–µ—Ç –µ–¥–∏–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        if self._conn is None:
            self._conn = sqlite3.connect(self.db_file, timeout=30.0, check_same_thread=False)
            self._conn.row_factory = sqlite3.Row
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
            self._conn.execute('PRAGMA journal_mode=WAL')
            self._conn.execute('PRAGMA busy_timeout=30000')
        return self._conn
    
    def _init_db(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ posted_articles
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS posted_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT NOT NULL UNIQUE,
                norm_url TEXT NOT NULL,
                domain TEXT NOT NULL,
                title TEXT NOT NULL,
                title_signature TEXT NOT NULL,
                summary TEXT,
                content_hash TEXT,
                summary_hash TEXT NOT NULL,
                entities TEXT,  -- JSON —Å–ø–∏—Å–æ–∫
                topic TEXT DEFAULT 'general',
                source TEXT,
                published_date TEXT,
                posted_date TEXT DEFAULT CURRENT_TIMESTAMP,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –æ—Ç–∫–ª–æ–Ω—ë–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π (–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS rejected_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT NOT NULL,
                norm_url TEXT,
                title TEXT NOT NULL,
                summary TEXT,
                source TEXT,
                rejection_reason TEXT NOT NULL,
                duplicate_of TEXT,  -- URL –¥—É–±–ª–∏–∫–∞—Ç–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
                similarity_score REAL,
                checked_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_norm_url ON posted_articles(norm_url)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_hash ON posted_articles(content_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_summary_hash ON posted_articles(summary_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_domain ON posted_articles(domain)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_title_signature ON posted_articles(title_signature)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_posted_date ON posted_articles(posted_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic ON posted_articles(topic)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_rejected_url ON rejected_articles(url)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_rejected_reason ON rejected_articles(rejection_reason)')
        
        conn.commit()
        logger.info("üìö –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def is_duplicate(self, url: str, title: str, summary: str = "") -> Tuple[bool, str]:
        """
        –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç:
        1. URL (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π)
        2. –•–µ—à summary
        3. –•–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        4. –ü–æ—Ö–æ–∂–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ (n-gram + SequenceMatcher)
        5. –î–æ–º–µ–Ω + —Å–∏–≥–Ω–∞—Ç—É—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–ª–æ–≤–∏—Ç mirror-—Å–∞–π—Ç—ã)
        6. –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        
        Returns: (is_duplicate, reason)
        """
        with self._lock:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            norm_url = normalize_url(url)
            domain = get_domain(url)
            title_sig = get_title_signature(title)
            summary_hash = get_summary_hash(summary)
            content_hash = get_content_hash(summary)
            
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É URL
            cursor.execute('SELECT title FROM posted_articles WHERE norm_url = ?', (norm_url,))
            if cursor.fetchone():
                return True, f"URL_DUPLICATE: {norm_url[:60]}"
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ö–µ—à—É summary
            cursor.execute('SELECT title FROM posted_articles WHERE summary_hash = ?', (summary_hash,))
            if cursor.fetchone():
                return True, f"SUMMARY_HASH_DUPLICATE"
            
            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ö–µ—à—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if content_hash:
                cursor.execute('SELECT title FROM posted_articles WHERE content_hash = ?', (content_hash,))
                if cursor.fetchone():
                    return True, f"CONTENT_HASH_DUPLICATE"
            
            # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–æ–º–µ–Ω—É + —Å–∏–≥–Ω–∞—Ç—É—Ä–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (mirror-—Å–∞–π—Ç—ã)
            cursor.execute('SELECT title FROM posted_articles WHERE domain = ? AND title_signature = ?',
                          (domain, title_sig))
            if cursor.fetchone():
                return True, f"DOMAIN_TITLE_SIGNATURE: {domain}"
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            cursor.execute('SELECT title FROM posted_articles WHERE posted_date > datetime("now", "-7 days")')
            recent_titles = [row[0] for row in cursor.fetchall()]
            
            for existing_title in recent_titles:
                # SequenceMatcher
                sim = calculate_similarity(title, existing_title)
                if sim > config.similarity_threshold:
                    return True, f"TITLE_SIMILARITY: {sim:.2f}"
                
                # N-gram similarity
                ngram_sim = ngram_similarity(title, existing_title)
                if ngram_sim > config.similarity_threshold:
                    return True, f"TITLE_NGRAM_SIMILARITY: {ngram_sim:.2f}"
            
            # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
            full_text = f"{title} {summary}".strip()
            new_entities = extract_key_entities(full_text)
            
            if len(new_entities) >= 2:
                cursor.execute('SELECT title, entities FROM posted_articles WHERE posted_date > datetime("now", "-14 days")')
                for row in cursor.fetchall():
                    existing_title, saved_entities_json = row
                    if saved_entities_json:
                        existing_entities = set(json.loads(saved_entities_json))
                    else:
                        existing_entities = extract_key_entities(existing_title)
                    
                    if len(existing_entities) < 2:
                        continue
                    
                    # Fuzzy matching —Å—É—â–Ω–æ—Å—Ç–µ–π
                    entity_sim = fuzzy_entity_match(new_entities, existing_entities)
                    if entity_sim >= config.entity_overlap_threshold:
                        return True, f"ENTITY_OVERLAP: {entity_sim:.2f}"
                    
                    # –ü—Ä—è–º–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
                    common = new_entities & existing_entities
                    min_size = min(len(new_entities), len(existing_entities))
                    overlap_ratio = len(common) / min_size if min_size > 0 else 0
                    
                    if len(common) >= 2 and overlap_ratio >= config.entity_overlap_threshold:
                        return True, f"ENTITY_COMMON: {len(common)} entities"
            
            return False, ""
    
    def is_too_similar_to_recent(self, title: str, summary: str) -> Tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –ø–æ—Ö–æ–∂–∞ —Å—Ç–∞—Ç—å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç TF-IDF cosine similarity
        """
        with self._lock:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            cursor.execute('SELECT title, summary, topic, entities FROM posted_articles '
                          'ORDER BY posted_date DESC LIMIT ?', (config.recent_posts_check,))
            recent_posts = cursor.fetchall()
            
            if len(recent_posts) < 2:
                return False, ""
            
            full_text = f"{title} {summary}".strip()
            new_entities = extract_key_entities(full_text)
            detected_topic = Topic.detect(full_text)
            
            # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è TF-IDF
            docs = [full_text]
            for row in recent_posts:
                docs.append(f"{row[0]} {row[1]}")
            
            # –í—ã—á–∏—Å–ª—è–µ–º TF-IDF similarity
            similarity_matrix = tfidf_cosine_similarity(docs)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ö–æ–∂–µ—Å—Ç—å —Å –∫–∞–∂–¥—ã–º –Ω–µ–¥–∞–≤–Ω–∏–º –ø–æ—Å—Ç–æ–º
            for idx, row in enumerate(recent_posts, start=1):
                sim_score = similarity_matrix[0][idx]
                
                if sim_score > config.tfidf_similarity_threshold:
                    return True, f"TFIDF_SIMILARITY: {sim_score:.2f} with '{row[0][:40]}...'"
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º
                if row[3]:  # entities
                    existing_entities = set(json.loads(row[3]))
                    common = new_entities & existing_entities
                    
                    if len(common) >= 3:
                        return True, f"TOO_MANY_COMMON_ENTITIES: {len(common)}"
            
            return False, ""
    
    def check_diversity_requirement(self, topic: str) -> Tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        –ù–µ –ø—É–±–ª–∏–∫—É–µ–º –ø–æ–¥—Ä—è–¥ –ø–æ—Å—Ç—ã –∏–∑ –æ–¥–Ω–æ–π —Ç–µ–º—ã
        """
        with self._lock:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            cursor.execute(
                'SELECT topic FROM posted_articles ORDER BY posted_date DESC LIMIT ?',
                (config.diversity_window,)
            )
            recent_topics = [row[0] for row in cursor.fetchall()]
            
            if not recent_topics:
                return True, ""
            
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ—Å—Ç —Ç–∞–∫–æ–π –∂–µ —Ç–µ–º—ã ‚Äî –æ—Ç–∫–ª–æ–Ω—è–µ–º
            if recent_topics[0] == topic:
                return False, f"DIVERSITY: –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ—Å—Ç –±—ã–ª {topic}"
            
            # –ï—Å–ª–∏ 2 –∏–∑ 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö ‚Äî —Ç–∞–∫–∞—è –∂–µ —Ç–µ–º–∞, –æ—Ç–∫–ª–æ–Ω—è–µ–º
            if len(recent_topics) >= config.diversity_window:
                same_topic_count = sum(1 for t in recent_topics[:config.diversity_window] if t == topic)
                if same_topic_count >= 2:
                    return False, f"DIVERSITY: {same_topic_count}/{config.diversity_window} –ø–æ—Å–ª–µ–¥–Ω–∏—Ö ‚Äî {topic}"
            
            return True, ""
    
    def llm_duplicate_check(self, article: Article, recent_posts: List[dict]) -> Tuple[bool, str]:
        """
        LLM-–ø—Ä–æ–≤–µ—Ä–∫–∞: –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç/–ø–æ—Ö–æ–∂–∞—è –Ω–æ–≤–æ—Å—Ç—å
        """
        if not recent_posts:
            return False, ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        context = "–ù–ï–î–ê–í–ù–ò–ï –ü–û–°–¢–´:\n"
        for i, post in enumerate(recent_posts[:3], 1):
            context += f"{i}. {post['title']}\n"
        
        new_article_text = f"–ù–û–í–ê–Ø –°–¢–ê–¢–¨–Ø:\n{article.title}\n{article.summary[:300]}"
        
        prompt = f"""{context}

{new_article_text}

–Ø–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º –∏–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ–π –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ—Å—Ç—ã?
–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û: YES –∏–ª–∏ NO"""
        
        try:
            resp = groq_client.chat.completions.create(
                model=GROQ_MODELS[0],
                temperature=0.3,
                max_tokens=10,
                messages=[{"role": "user", "content": prompt}],
            )
            answer = resp.choices[0].message.content.strip().upper()
            
            if "YES" in answer:
                return True, "LLM_DUPLICATE_DETECTION"
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LLM duplicate check failed: {e}")
        
        return False, ""
    
    def add(self, article: Article, topic: str = Topic.GENERAL):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—å—é –≤ –±–∞–∑—É"""
        with self._lock:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            norm_url = normalize_url(article.link)
            domain = get_domain(article.link)
            title_sig = get_title_signature(article.title)
            summary_hash = get_summary_hash(article.summary)
            content_hash = get_content_hash(article.summary)
            
            full_text = f"{article.title} {article.summary}".strip()
            entities = list(extract_key_entities(full_text))
            
            cursor.execute('''
                INSERT OR IGNORE INTO posted_articles 
                (url, norm_url, domain, title, title_signature, summary, 
                 content_hash, summary_hash, entities, topic, source, published_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article.link, norm_url, domain, article.title, title_sig, article.summary,
                content_hash, summary_hash, json.dumps(entities), topic, article.source,
                article.published.isoformat()
            ))
            
            conn.commit()
    
    def log_rejected(self, article: Article, reason: str, duplicate_of: str = None, similarity: float = None):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –æ—Ç–∫–ª–æ–Ω—ë–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é"""
        with self._lock:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            norm_url = normalize_url(article.link)
            
            cursor.execute('''
                INSERT INTO rejected_articles 
                (url, norm_url, title, summary, source, rejection_reason, duplicate_of, similarity_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article.link, norm_url, article.title, article.summary[:500],
                article.source, reason, duplicate_of, similarity
            ))
            
            conn.commit()
    
    def get_recent_posts(self, limit: int = 5) -> List[dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø–æ—Å—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        with self._lock:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT title, summary, topic, entities, url
                FROM posted_articles
                ORDER BY posted_date DESC
                LIMIT ?
            ''', (limit,))
            
            posts = []
            for row in cursor.fetchall():
                posts.append({
                    'title': row[0],
                    'summary': row[1],
                    'topic': row[2],
                    'entities': json.loads(row[3]) if row[3] else [],
                    'url': row[4]
                })
            return posts
    
    def get_recent_topics_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–º–∞–º –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤"""
        with self._lock:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT topic, COUNT(*) as count 
                FROM posted_articles 
                WHERE posted_date > datetime("now", "-7 days")
                GROUP BY topic
            ''')
            
            return {row[0]: row[1] for row in cursor.fetchall()}
    
    def cleanup(self, days: int = 60):
        """–£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π"""
        with self._lock:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            cursor.execute(f'''
                DELETE FROM posted_articles 
                WHERE posted_date < datetime('now', '-{days} days')
            ''')
            
            deleted = cursor.rowcount
            conn.commit()
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ rejected —Ç–æ–∂–µ
            cursor.execute('''
                DELETE FROM rejected_articles 
                WHERE checked_at < datetime("now", "-30 days")
            ''')
            rejected_deleted = cursor.rowcount
            conn.commit()
            
            if deleted > 0 or rejected_deleted > 0:
                logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–æ {deleted} posted, {rejected_deleted} rejected")
            
            # VACUUM –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if deleted > 100:
                cursor.execute('VACUUM')
                conn.commit()
    
    def get_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        with self._lock:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            cursor.execute('SELECT COUNT(*) FROM posted_articles')
            total_posted = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(*) FROM rejected_articles')
            total_rejected = cursor.fetchone()[0]
            
            cursor.execute('''
                SELECT rejection_reason, COUNT(*) as count 
                FROM rejected_articles 
                GROUP BY rejection_reason
                ORDER BY count DESC
            ''')
            rejection_reasons = {row[0]: row[1] for row in cursor.fetchall()}
            
            return {
                'total_posted': total_posted,
                'total_rejected': total_rejected,
                'rejection_reasons': rejection_reasons
            }
    
    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        if self._conn:
            self._conn.close()
            self._conn = None
            logger.info("üîí –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–∞")

# ====================== RSS LOADING ======================
async def fetch_feed(url: str, source: str, posted: PostedManager) -> List[Article]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω RSS feed"""
    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞: {source}")
    
    try:
        jitter = random.uniform(0, config.rss_jitter)
        await asyncio.sleep(jitter)
        
        async with aiohttp.ClientSession() as sess:
            async with sess.get(url, headers=HEADERS, timeout=aiohttp.ClientTimeout(total=config.rss_timeout)) as resp:
                if resp.status != 200:
                    logger.warning(f"  ‚ö†Ô∏è HTTP {resp.status}")
                    return []
                
                content = await resp.text()
        
        feed = await asyncio.to_thread(feedparser.parse, content)
        
        if not feed.entries:
            logger.warning(f"  ‚ö†Ô∏è –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π")
            return []
        
        articles = []
        for entry in feed.entries[:15]:
            link = entry.get('link', '')
            title = entry.get('title', '').strip()
            summary = entry.get('summary', entry.get('description', '')).strip()
            
            if not link or not title:
                continue
            
            # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏ –∏–∑ summary
            summary = re.sub(r'<[^>]+>', '', summary)
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            pub_date = entry.get('published_parsed') or entry.get('updated_parsed')
            if pub_date:
                published = datetime(*pub_date[:6], tzinfo=timezone.utc)
            else:
                published = datetime.now(timezone.utc)
            
            articles.append(Article(
                title=title,
                summary=summary,
                link=link,
                source=source,
                published=published
            ))
        
        logger.info(f"  ‚úÖ {len(articles)} —Å—Ç–∞—Ç–µ–π")
        return articles
        
    except asyncio.TimeoutError:
        logger.warning(f"  ‚è±Ô∏è –¢–∞–π–º–∞—É—Ç")
        return []
    except Exception as e:
        logger.warning(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
        return []

async def load_all_feeds(posted: PostedManager) -> List[Article]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ RSS feeds –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
    tasks = [fetch_feed(url, source, posted) for url, source in RSS_FEEDS]
    results = await asyncio.gather(*tasks)
    
    all_articles = []
    for feed_articles in results:
        all_articles.extend(feed_articles)
    
    logger.info(f"üì¶ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(all_articles)} —Å—Ç–∞—Ç–µ–π")
    return all_articles

# ====================== –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ======================
def is_relevant(article: Article) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç—å–∏"""
    text_lower = f"{article.title} {article.summary}".lower()
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ –ø–ª–æ—Ö–∏–º —Ñ—Ä–∞–∑–∞–º
    if any(bad in text_lower for bad in BAD_PHRASES):
        return False
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    if any(ex in text_lower for ex in EXCLUDE_KEYWORDS):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ AI –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    has_ai_keyword = any(kw in text_lower for kw in AI_KEYWORDS)
    if not has_ai_keyword:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
    age_hours = (datetime.now(timezone.utc) - article.published).total_seconds() / 3600
    if age_hours > config.max_article_age_hours:
        return False
    
    return True

def filter_articles(articles: List[Article], posted: PostedManager) -> List[Article]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏"""
    logger.info("üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–µ–π...")
    
    candidates = []
    for article in articles:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        if not is_relevant(article):
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        is_dup, reason = posted.is_duplicate(article.link, article.title, article.summary)
        if is_dup:
            posted.log_rejected(article, reason)
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç—å —Å –Ω–µ–¥–∞–≤–Ω–∏–º–∏
        is_similar, reason = posted.is_too_similar_to_recent(article.title, article.summary)
        if is_similar:
            posted.log_rejected(article, reason)
            continue
        
        candidates.append(article)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–≤–µ–∂–µ—Å—Ç–∏ –∏ –Ω–∞–ª–∏—á–∏—é —Å—É—â–Ω–æ—Å—Ç–µ–π
    def score_article(art: Article) -> float:
        text = f"{art.title} {art.summary}".lower()
        entities = extract_key_entities(text)
        entity_score = len(entities) * 0.5
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–≤–µ–∂–µ—Å—Ç—å
        age_hours = (datetime.now(timezone.utc) - art.published).total_seconds() / 3600
        freshness_score = max(0, 24 - age_hours) / 24
        
        return entity_score + freshness_score
    
    candidates.sort(key=score_article, reverse=True)
    
    logger.info(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
    return candidates

# ====================== EXPONENTIAL BACKOFF ======================
def exponential_backoff(attempt: int) -> float:
    """–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —Å jitter"""
    delay = min(config.groq_base_delay * (2 ** attempt), config.groq_max_delay)
    jitter = random.uniform(0, delay * 0.1)
    return delay + jitter

# ====================== –ì–ï–ù–ï–†–ê–¶–ò–Ø –°–ê–ú–ú–ê–†–ò ======================
async def generate_summary(article: Article) -> Optional[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ—Å—Ç —á–µ—Ä–µ–∑ Groq"""
    logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞: {article.title[:60]}...")
    
    prompt = f"""–ü—Ä–µ–≤—Ä–∞—Ç–∏—Ç–µ —ç—Ç—É AI-–Ω–æ–≤–æ—Å—Ç—å –≤ –≤–∏—Ä—É—Å–Ω—ã–π –ø–æ—Å—Ç –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.

–ù–û–í–û–°–¢–¨:
{article.title}
{article.summary[:800]}

–°–¢–†–£–ö–¢–£–†–ê –ü–û–°–¢–ê:
1. –í–∑—Ä—ã–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (5-8 —Å–ª–æ–≤) ‚Äî –Ω–∏–∫–∞–∫–æ–π –≤–æ–¥—ã, —Å—Ä–∞–∑—É –∫ –¥–µ–ª—É
2. –ì–ª–∞–≤–Ω–∞—è —Å—É—Ç—å –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º (—á—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å?)
3. –ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–µ–∑ –∫–ª–∏—à–µ)
4. –ö—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

–í–ê–ñ–ù–û:
√ó –ù–∏–∫–∞–∫–∏—Ö –±–∞–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π –≤—Ä–æ–¥–µ "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å", "—Å—Ç–æ–∏—Ç —Å–∫–∞–∑–∞—Ç—å", "–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ"
√ó –ë–µ–∑ –æ—á–µ–≤–∏–¥–Ω—ã—Ö –≤–µ—â–µ–π —Ç–∏–ø–∞ "–ò–ò —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è", "–∫–æ–º–ø–∞–Ω–∏–∏ —Å–æ—Ä–µ–≤–Ω—É—é—Ç—Å—è"
√ó –¢–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞, —Ü–∏—Ñ—Ä—ã, —Ñ–∞–∫—Ç—ã, –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è
√ó –≠–º–æ–¥–∑–∏ –≤ –º–µ—Ä—É (1-2 –º–∞–∫—Å–∏–º—É–º –≤ –Ω–∞—á–∞–ª–µ)
√ó –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –±–µ–∑ —Ö–∞–π–ø–∞ —Ä–∞–¥–∏ —Ö–∞–π–ø–∞

–•–û–†–û–®–ò–ï –ü–†–ò–ú–ï–†–´ –ü–û–î–ê–ß–ò:
‚úì "DeepMind –æ–±—É—á–∏–ª–∞ –ò–ò –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ–≥–æ–¥—É —Ç–æ—á–Ω–µ–µ –º–µ—Ç–µ–æ—Ä–æ–ª–æ–≥–æ–≤"
‚úì "–ù–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ–±—Ö–æ–¥–∏—Ç GPT-4 –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –ø—Ä–∏ 10x –º–µ–Ω—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö"
‚úì "Google —Å–Ω–æ–≤–∞ –¥–æ–≥–æ–Ω—è–µ—Ç ‚Äî –∏–ª–∏ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –æ–±–≥–æ–Ω–∏—Ç?"
‚úì "–°–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ –ø–æ—Ö–æ—Ä–æ–Ω–∏—Ç —ç—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ?"

–ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å ‚Äî –º—É—Å–æ—Ä/—Ä–µ–∫–ª–∞–º–∞/–Ω–µ –ø—Ä–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –æ—Ç–≤–µ—Ç—å: SKIP

–ü–û–°–¢:"""

    for attempt in range(config.groq_max_retries):
        try:
            await asyncio.sleep(0.5)
            
            resp = await asyncio.to_thread(
                groq_client.chat.completions.create,
                model=random.choice(GROQ_MODELS),
                temperature=0.7,
                max_tokens=1100,
                messages=[{"role": "user", "content": prompt}],
            )
            text = resp.choices[0].message.content.strip()

            if "SKIP" in text.upper()[:15]:
                logger.info("  ‚è≠Ô∏è LLM: SKIP")
                return None

            if len(text) < config.min_post_length:
                logger.warning(f"  ‚ö†Ô∏è –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤.), –ø–æ–≤—Ç–æ—Ä...")
                continue

            water = ["—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å", "–≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ", 
                    "–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º—Å—è", "–Ω–µ —Å–µ–∫—Ä–µ—Ç", "–æ—á–µ–≤–∏–¥–Ω–æ, —á—Ç–æ"]
            if any(w in text.lower() for w in water):
                logger.warning("  ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤–æ–¥–∞, –ø–æ–≤—Ç–æ—Ä...")
                continue

            topic = Topic.detect(f"{article.title} {article.summary}")
            hashtags = Topic.HASHTAGS.get(topic, Topic.HASHTAGS[Topic.GENERAL])
            
            cta = "\n\nüî• ‚Äî –æ–≥–æ–Ω—å  |  üóø ‚Äî –Ω—É —Ç–∞–∫–æ–µ  |  ‚ö° ‚Äî –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ"
            source_link = f'\n\nüîó <a href="{article.link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
            
            final = f"{text}{cta}\n\n{hashtags}{source_link}"

            if len(final) > config.caption_limit:
                excess = len(final) - config.caption_limit + 20
                text = text[:-excess]
                for p in ['. ', '! ', '? ']:
                    idx = text.rfind(p)
                    if idx > len(text) * 0.6:
                        text = text[:idx+1]
                        break
                final = f"{text}{cta}\n\n{hashtags}{source_link}"

            logger.info(f"  ‚úÖ –ì–æ—Ç–æ–≤–æ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ | –¢–µ–º–∞: {topic}")
            return final
            
        except Exception as e:
            delay = exponential_backoff(attempt)
            logger.error(f"  ‚ùå Groq –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{config.groq_max_retries}): {e}")
            logger.info(f"  ‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay:.1f}s...")
            await asyncio.sleep(delay)

    return None

# ====================== –ö–ê–†–¢–ò–ù–ö–ò ======================
async def generate_image(title: str, topic: str = Topic.GENERAL) -> Optional[str]:
    logger.info(f"  üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏ –¥–ª—è —Ç–µ–º—ã: {topic}")
    
    clean_title = re.sub(r'[^\w\s]', '', title)[:50]
    style = Topic.get_image_style(topic)
    prompt = f"{style}, {clean_title}, high quality, 4k, sharp focus"
    
    url = (
        f"https://image.pollinations.ai/prompt/{quote(prompt)}"
        f"?width=1024&height=1024&nologo=true&seed={random.randint(1,99999)}"
    )
    
    for attempt in range(3):
        try:
            async with aiohttp.ClientSession() as sess:
                async with sess.get(url, timeout=aiohttp.ClientTimeout(total=45)) as resp:
                    if resp.status != 200:
                        logger.warning(f"  ‚ö†Ô∏è HTTP {resp.status}, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}")
                        continue
                    
                    data = await resp.read()
                    
                    if len(data) < 10000:
                        logger.warning(f"  ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª ({len(data)} bytes)")
                        continue
                    
                    fname = f"img_{random.randint(1000,9999)}.jpg"
                    with open(fname, "wb") as f:
                        f.write(data)
                    
                    logger.info(f"  ‚úÖ –ö–∞—Ä—Ç–∏–Ω–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {fname} ({len(data)//1024}KB)")
                    return fname
                    
        except asyncio.TimeoutError:
            logger.warning(f"  ‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/3")
            await asyncio.sleep(2)
        except Exception as e:
            logger.warning(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({attempt+1}/3): {e}")
            await asyncio.sleep(2)
    
    logger.warning("  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É")
    return None

# ====================== –ü–£–ë–õ–ò–ö–ê–¶–ò–Ø ======================
async def post_article(article: Article, text: str, posted: PostedManager) -> bool:
    topic = Topic.detect(f"{article.title} {article.summary}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    diversity_ok, diversity_reason = posted.check_diversity_requirement(topic)
    if not diversity_ok:
        posted.log_rejected(article, diversity_reason)
        logger.info(f"  ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ (—Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ): {diversity_reason}")
        return False
    
    # LLM-–ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    recent_posts = posted.get_recent_posts(3)
    is_llm_dup, llm_reason = posted.llm_duplicate_check(article, recent_posts)
    if is_llm_dup:
        posted.log_rejected(article, llm_reason)
        logger.info(f"  ‚è≠Ô∏è LLM –æ–ø—Ä–µ–¥–µ–ª–∏–ª –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç")
        return False
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img = await generate_image(article.title, topic)
    
    try:
        if img and os.path.exists(img):
            await bot.send_photo(config.channel_id, FSInputFile(img), caption=text)
            os.remove(img)
        else:
            await bot.send_message(config.channel_id, text, disable_web_page_preview=False)
        
        posted.add(article, topic)
        
        logger.info(f"‚úÖ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–û [{topic.upper()}]: {article.title[:50]}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Telegram –æ—à–∏–±–∫–∞: {e}")
        if img and os.path.exists(img):
            try:
                os.remove(img)
            except:
                pass
        return False

# ====================== MAIN ======================
async def main():
    logger.info("=" * 50)
    logger.info("üöÄ –ó–ê–ü–£–°–ö AI-POSTER v3.0 (SQLite + –£—Å–∏–ª–µ–Ω–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è)")
    logger.info("=" * 50)
    
    posted = PostedManager(config.db_file)
    
    try:
        posted.cleanup(config.retention_days)
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = posted.get_stats()
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î: {stats['total_posted']} posted, {stats['total_rejected']} rejected")
        
        raw_articles = await load_all_feeds(posted)
        candidates = filter_articles(raw_articles, posted)
        
        if not candidates:
            logger.info("üì≠ –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            return

        for article in candidates[:20]:
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            is_dup, reason = posted.is_duplicate(article.link, article.title, article.summary)
            if is_dup:
                posted.log_rejected(article, f"FINAL_CHECK: {reason}")
                logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞): {article.title[:40]}")
                continue
            
            is_similar, reason = posted.is_too_similar_to_recent(article.title, article.summary)
            if is_similar:
                posted.log_rejected(article, f"FINAL_RECENT: {reason}")
                logger.debug(f"  –ü—Ä–æ–ø—É—Å–∫ (–ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ): {article.title[:40]}")
                continue
            
            summary = await generate_summary(article)
            if not summary:
                posted.log_rejected(article, "LLM_GENERATION_FAILED")
                continue
            
            if await post_article(article, summary, posted):
                logger.info("\nüèÅ –ì–æ—Ç–æ–≤–æ! –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à—ë–Ω.")
                break
            
            await asyncio.sleep(3)
        else:
            logger.info("üòî –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏")
    
    finally:
        posted.close()
        await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())































































































































































































































































