import os
import json
import asyncio
import random
from datetime import datetime
from typing import List, Dict, Optional

import requests
import feedparser
import urllib.parse
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from openai import OpenAI

# ============ CONFIG ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

if not all([OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    raise ValueError("‚ùå –ù–µ –≤—Å–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    )
}

POSTED_FILE = "posted_articles.json"
RETENTION_DAYS = 7

# ============ –ü–†–ò–û–†–ò–¢–ï–¢: –ò–ò –ò –ù–ï–ô–†–û–°–ï–¢–ò ============

AI_KEYWORDS = [
    # –û–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –ò–ò
    "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å", "–∏–∏", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
    "neural network", "artificial intelligence",
    # LLM –∏ –º–æ–¥–µ–ª–∏
    "llm", "gpt", "gpt-4", "gpt-5", "gpt-4o", "chatgpt", "claude", "gemini",
    "copilot", "mistral", "llama", "qwen", "gigachat", "yandexgpt",
    "kandinsky", "—à–µ–¥–µ–≤—Ä—É–º", "deepseek", "grok",
    # –ö–æ–º–ø–∞–Ω–∏–∏ –ò–ò
    "openai", "anthropic", "deepmind", "—Å–±–µ—Ä", "—è–Ω–¥–µ–∫—Å",
    "hugging face", "stability ai", "meta ai", "google ai",
    # –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
    "stable diffusion", "midjourney", "dall-e", "sora", "runway",
    "–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
    "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ", "text-to-image", "text-to-video",
    # ML —Ç–µ—Ä–º–∏–Ω—ã
    "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "transformer",
    "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π",
    "–¥–æ–æ–±—É—á–µ–Ω–∏–µ", "–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", "–¥–∞—Ç–∞—Å–µ—Ç", "fine-tuning",
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ò–ò
    "—á–∞—Ç-–±–æ—Ç", "–≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
    "–Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π", "ai-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "—É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫",
    "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ", "–æ–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–∞", "nlp",
    # –ù–æ–≤—ã–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    "agi", "—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "–∞–≥–µ–Ω—Ç", "ai-–∞–≥–µ–Ω—Ç", "–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ",
    "—Ç–æ–∫–µ–Ω", "–±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "reasoning",
    # –¢—Ä–µ–Ω–¥—ã
    "–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "rlhf", "–ø—Ä–æ–º–ø—Ç", "prompt"
]

# ============ –ò–ù–¢–ï–†–ï–°–ù–´–ï –¢–ï–•–ù–û–õ–û–ì–ò–ò ============

TECH_KEYWORDS = [
    # –ê–Ω–æ–Ω—Å—ã –∏ –Ω–æ–≤–∏–Ω–∫–∏
    "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª", "–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª", "–≤—ã–ø—É—Å—Ç–∏–ª", "—Ä–µ–ª–∏–∑", "–∑–∞–ø—É—Å—Ç–∏–ª",
    "–Ω–æ–≤–∏–Ω–∫–∞", "–¥–µ–±—é—Ç", "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è", "–ø–æ–∫–∞–∑–∞–ª", "unveiled",
    # –ì–∞–¥–∂–µ—Ç—ã –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    "—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "–Ω–æ—É—Ç–±—É–∫", "–≥–∞–¥–∂–µ—Ç", "–¥–µ–≤–∞–π—Å", "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ",
    "–Ω–æ—Å–∏–º–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞", "—É–º–Ω—ã–µ —á–∞—Å—ã", "–Ω–∞—É—à–Ω–∏–∫–∏",
    # –†–æ–±–æ—Ç—ã –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è
    "—Ä–æ–±–æ—Ç", "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞", "–¥—Ä–æ–Ω", "–±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç",
    "–∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π", "boston dynamics", "tesla bot",
    # –ü–µ—Ä–µ–¥–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    "–∫–≤–∞–Ω—Ç–æ–≤—ã–π", "–∫–≤–∞–Ω—Ç–æ–≤—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—á–∏–ø",
    "gpu", "–≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞", "nvidia", "amd", "intel", "apple",
    # –ö–æ—Å–º–æ—Å –∏ –Ω–∞—É–∫–∞
    "spacex", "starship", "–∫–æ—Å–º–æ—Å", "—Ä–∞–∫–µ—Ç–∞", "—Å–ø—É—Ç–Ω–∏–∫",
    "starlink", "nasa", "—Ä–æ—Å–∫–æ—Å–º–æ—Å",
    # VR/AR
    "–≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å", "–¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å",
    "vr", "ar", "meta quest", "apple vision", "–æ—á–∫–∏",
    # –≠–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª–∏
    "—ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª—å", "tesla", "—ç–ª–µ–∫—Ç—Ä–æ–∫–∞—Ä", "–±–∞—Ç–∞—Ä–µ—è",
    "–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä", "–∑–∞—Ä—è–¥–∫–∞", "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç",
    # –ë—É–¥—É—â–µ–µ
    "–ø—Ä–æ—Ä—ã–≤", "—Ä–µ–≤–æ–ª—é—Ü–∏—è", "–∏–Ω–Ω–æ–≤–∞—Ü–∏—è", "–±—É–¥—É—â–µ–µ", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è"
]

# ============ –ò–°–ö–õ–Æ–ß–ò–¢–¨ ============

EXCLUDE_KEYWORDS = [
    # –°–ø–æ—Ä—Ç
    "—Ç–µ–Ω–Ω–∏—Å", "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "–±–∞—Å–∫–µ—Ç–±–æ–ª", "—Å–ø–æ—Ä—Ç", "–º–∞—Ç—á",
    "–æ–ª–∏–º–ø–∏–∞–¥–∞", "—á–µ–º–ø–∏–æ–Ω–∞—Ç", "—Ç—É—Ä–Ω–∏—Ä", "—Å–±–æ—Ä–Ω–∞—è",
    # –ò–≥—Ä—ã
    "–∏–≥—Ä–∞", "–≥–µ–π–º–ø–ª–µ–π", "playstation", "xbox", "steam", "nintendo",
    "–≤–∏–¥–µ–æ–∏–≥—Ä–∞", "–∫–æ–Ω—Å–æ–ª—å", "gaming",
    # –†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è
    "–∫–∏–Ω–æ", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "–º—É–∑—ã–∫–∞", "–∫–æ–Ω—Ü–µ—Ä—Ç", "–∞–∫—Ç—ë—Ä", "–∞–∫—Ç–µ—Ä",
    "–ø—Ä–µ–º—å–µ—Ä–∞", "—Ç—Ä–µ–π–ª–µ—Ä", "netflix", "–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä",
    # –ü–æ–ª–∏—Ç–∏–∫–∞
    "–≤—ã–±–æ—Ä—ã", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–ø–∞—Ä–ª–∞–º–µ–Ω—Ç", "–ø–æ–ª–∏—Ç–∏–∫", "–¥–µ–ø—É—Ç–∞—Ç",
    "—Å–∞–Ω–∫—Ü–∏–∏", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–º–∏–Ω–∏—Å—Ç—Ä",
    # –ú–µ–¥–∏—Ü–∏–Ω–∞ (–∫—Ä–æ–º–µ –ò–ò –≤ –º–µ–¥–∏—Ü–∏–Ω–µ)
    "–±–æ–ª–µ–∑–Ω—å", "covid", "–ø–∞–Ω–¥–µ–º–∏—è", "–≥—Ä–∏–ø–ø", "–≤–∞–∫—Ü–∏–Ω–∞",
    # –ö—Ä–∏–ø—Ç–æ
    "–∫—Ä–∏–ø—Ç–æ", "bitcoin", "–±–∏—Ç–∫–æ–π–Ω", "ethereum", "nft", "–±–ª–æ–∫—á–µ–π–Ω",
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "–º–∞–π–Ω–∏–Ω–≥",
    # –ö—Ä–∏–º–∏–Ω–∞–ª
    "—Å—É–¥", "—Å—É–¥–µ–±–Ω—ã–π", "–∞—Ä–µ—Å—Ç", "–ø—Ä–∏–≥–æ–≤–æ—Ä", "—Ç—é—Ä—å–º–∞"
]

# ============ STATE ============

posted_articles: Dict[str, Optional[float]] = {}

if os.path.exists(POSTED_FILE):
    with open(POSTED_FILE, "r", encoding="utf-8") as f:
        try:
            posted_data = json.load(f)
            posted_articles = {item["id"]: item.get("timestamp") for item in posted_data}
        except Exception:
            posted_articles = {}


def save_posted_articles() -> None:
    data = [{"id": id_str, "timestamp": ts} for id_str, ts in posted_articles.items()]
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def clean_old_posts() -> None:
    global posted_articles
    now = datetime.now().timestamp()
    cutoff = now - (RETENTION_DAYS * 86400)
    posted_articles = {
        id_str: ts for id_str, ts in posted_articles.items()
        if ts is None or ts > cutoff
    }
    save_posted_articles()


def save_posted(article_id: str) -> None:
    posted_articles[article_id] = datetime.now().timestamp()
    save_posted_articles()


# ============ HELPERS ============

def clean_text(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").split())


# ============ PARSERS ============

def load_rss(url: str, source: str) -> List[Dict]:
    articles = []
    try:
        feed = feedparser.parse(url)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RSS {url}: {e}")
        return articles

    for entry in feed.entries[:30]:
        link = entry.get("link", "")
        if not link or link in posted_articles:
            continue
        articles.append({
            "id": link,
            "title": clean_text(entry.get("title") or ""),
            "summary": clean_text(
                entry.get("summary") or entry.get("description") or ""
            )[:700],
            "link": link,
            "source": source,
            "published_parsed": datetime.now()
        })
    return articles


def load_articles_from_sites() -> List[Dict]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ —Å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Äî –ò–ò/–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.
    """
    articles: List[Dict] = []

    # === –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ò–ò –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ ===

    # Habr - –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru",
        "Habr AI"
    ))

    # Habr - –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru",
        "Habr ML"
    ))

    # Habr - –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/neural_networks/all/?fl=ru",
        "Habr Neural"
    ))

    # Habr - Data Science
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/data_science/all/?fl=ru",
        "Habr DS"
    ))

    # Habr - NLP
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/natural_language_processing/all/?fl=ru",
        "Habr NLP"
    ))

    # Habr - –†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞
    articles.extend(load_rss(
        "https://habr.com/ru/rss/hub/robotics/all/?fl=ru",
        "Habr Robotics"
    ))

    # === –ü–†–ò–û–†–ò–¢–ï–¢ 2: –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ ===

    # –†–ë–ö –¢—Ä–µ–Ω–¥—ã (—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏)
    articles.extend(load_rss(
        "https://rssexport.rbc.ru/rbcnews/v2/trends/full.rss",
        "–†–ë–ö –¢—Ä–µ–Ω–¥—ã"
    ))

    # Tproger
    articles.extend(load_rss(
        "https://tproger.ru/feed/",
        "Tproger"
    ))

    # VC.ru
    articles.extend(load_rss(
        "https://vc.ru/rss/all",
        "VC.ru"
    ))

    # –•–∞–π—Ç–µ–∫
    articles.extend(load_rss(
        "https://hightech.fm/feed",
        "–•–∞–π—Ç–µ–∫"
    ))

    # === –ü–†–ò–û–†–ò–¢–ï–¢ 3: –û–±—â–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ ===

    # 3DNews
    articles.extend(load_rss(
        "https://3dnews.ru/news/rss/",
        "3DNews"
    ))

    # iXBT
    articles.extend(load_rss(
        "https://www.ixbt.com/export/news.rss",
        "iXBT"
    ))

    # ServerNews
    articles.extend(load_rss(
        "https://servernews.ru/rss",
        "ServerNews"
    ))

    # === –ù–ê–£–ö–ê ===

    # –¢–ê–°–° –ù–∞—É–∫–∞
    articles.extend(load_rss(
        "https://tass.ru/rss/v2.xml?sections=nauka",
        "–¢–ê–°–° –ù–∞—É–∫–∞"
    ))

    # –†–ò–ê –ù–∞—É–∫–∞
    articles.extend(load_rss(
        "https://ria.ru/export/rss2/science/index.xml",
        "–†–ò–ê –ù–∞—É–∫–∞"
    ))

    return articles


# ============ –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ============

def filter_articles(articles: List[Dict]) -> List[Dict]:
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ò–ò ‚Üí —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.
    """
    ai_articles = []
    tech_articles = []

    for e in articles:
        text = f"{e['title']} {e['summary']}".lower()

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å–∫–ª—é—á—ë–Ω–Ω—ã–µ —Ç–µ–º—ã
        if any(kw in text for kw in EXCLUDE_KEYWORDS):
            continue

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –ò–ò/–Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        if any(kw in text for kw in AI_KEYWORDS):
            ai_articles.append(e)
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        elif any(kw in text for kw in TECH_KEYWORDS):
            tech_articles.append(e)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
    ai_articles.sort(key=lambda x: x["published_parsed"], reverse=True)
    tech_articles.sort(key=lambda x: x["published_parsed"], reverse=True)

    # –ò–ò —Å—Ç–∞—Ç—å–∏ –ø–µ—Ä–≤—ã–º–∏
    return ai_articles + tech_articles


# ============ OPENAI TEXT ============

def short_summary(title: str, summary: str, link: str) -> Optional[str]:
    news_text = f"{title}. {summary}"
    prompt = (
        "–í–æ—Ç —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –ò–ò –∏–ª–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –°–¥–µ–ª–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π –æ–±–∑–æ—Ä –¥–ª—è Telegram –Ω–∞ —Ä—É—Å—Å–∫–æ–º:\n"
        f"{news_text}\n\n"
        "- –û–±—ä—ë–º: 380‚Äì450 —Å–∏–º–≤–æ–ª–æ–≤.\n"
        "- –§–æ–∫—É—Å: –ß—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏, –∫–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –ø–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ/–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ.\n"
        "- –°—Ç–∏–ª—å: –ñ–∏–≤–æ–π, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π. –ò—Å–ø–æ–ª—å–∑—É–π 1-2 —ç–º–æ–¥–∑–∏ –ø–æ —Ç–µ–º–µ (ü§ñüß†üí°üöÄ‚ú®).\n"
        "- –§–æ—Ä–º–∞—Ç: 2-3 –∫–ª—é—á–µ–≤—ã–µ —Ñ–∏—à–∫–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –º–æ–¥–µ–ª–∏ –∏–ª–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.\n"
        "- –í –∫–æ–Ω—Ü–µ: 2-3 —Ö–µ—à—Ç–µ–≥–∞ (#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–≥–∞–¥–∂–µ—Ç—ã #–±—É–¥—É—â–µ–µ).\n"
        "- –Ø–∑—ã–∫: –¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π!\n"
        "- –ó–∞–ø—Ä–µ—â–µ–Ω–æ: –í—ã–¥—É–º—ã–≤–∞—Ç—å, –∫–ª–∏—à–µ —Ç–∏–ø–∞ '–º–∏—Ä –Ω–µ —Å—Ç–æ–∏—Ç –Ω–∞ –º–µ—Å—Ç–µ'.\n"
        "- –°—Å—ã–ª–∫—É –∏ –ø–æ–¥–ø–∏—Å–∏ –Ω–µ –≤–∫–ª—é—á–∞–π."
    )

    try:
        res = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=400,
        )
        core = res.choices[0].message.content.strip()

        src = f"\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {link}"
        ps = "\n\nPSüí• –ö—Ç–æ –∑–∞ –∫–ª—é—á–∞–º–∏ üëâ https://t.me/+EdEfIkn83Wg3ZTE6"
        return core + src + ps
    except Exception as e:
        print(f"‚ùå OpenAI: {e}")
        return None


# ============ –ö–ê–†–¢–ò–ù–ö–ò ============

def generate_image(title: str) -> Optional[str]:
    seed = random.randint(0, 10**6)
    prompt = (
        f"Futuristic technology illustration: {title[:80]}, "
        "AI, neural networks, innovation, modern design, "
        "soft neon lighting, 4k, no text, clean."
    )
    try:
        encoded = urllib.parse.quote(prompt)
        url = f"https://image.pollinations.ai/prompt/{encoded}?seed={seed}&width=1024&height=1024"
        resp = requests.get(url, timeout=60)
        if resp.status_code == 200:
            fname = f"img_{seed}.jpg"
            with open(fname, "wb") as f:
                f.write(resp.content)
            return fname
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
    return None


# ============ –ê–í–¢–û–ü–û–°–¢ ============

async def autopost():
    clean_old_posts()
    articles = load_articles_from_sites()
    candidates = filter_articles(articles)

    if not candidates:
        print("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø—Ä–æ –ò–ò/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.")
        return

    ai_count = sum(1 for a in candidates if any(
        kw in f"{a['title']} {a['summary']}".lower()
        for kw in AI_KEYWORDS
    ))
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(candidates)} —Å—Ç–∞—Ç–µ–π ({ai_count} –ø—Ä–æ –ò–ò)")

    for art in candidates[:5]:
        print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞: {art['title'][:60]}...")
        post_text = short_summary(art["title"], art["summary"], art["link"])

        if post_text:
            img = generate_image(art["title"])
            try:
                if img:
                    await bot.send_photo(
                        CHANNEL_ID,
                        photo=FSInputFile(img),
                        caption=post_text
                    )
                    os.remove(img)
                else:
                    await bot.send_message(CHANNEL_ID, text=post_text)

                save_posted(art["id"])
                print(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {art['source']}")
                break
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
                if img and os.path.exists(img):
                    os.remove(img)


async def main():
    try:
        await autopost()
    finally:
        await bot.session.close()


if __name__ == "__main__":
    asyncio.run(main())




































































































